# Cap√≠tulo 03 ‚Äî Aten√ß√£o (Self-Attention)

Se o cap√≠tulo anterior respondeu **como texto vira n√∫meros**,  
este cap√≠tulo responde a pergunta que mudou completamente a hist√≥ria dos modelos de linguagem:

**Como o modelo decide quais partes do contexto s√£o importantes?**

Aqui nasce o mecanismo que tornou poss√≠veis os Transformers modernos e, consequentemente, os LLMs atuais.

---

## üéØ Objetivo do Cap√≠tulo

Ao final deste cap√≠tulo, voc√™ deve ser capaz de:

- entender por que modelos precisam de mecanismos para lidar com contexto global
- explicar intuitivamente o que √© aten√ß√£o
- compreender como vetores de contexto s√£o calculados
- entender o papel das matrizes Query, Key e Value (Q, K, V)
- explicar o que √© m√°scara causal e por que ela √© essencial para modelos autoregressivos
- compreender por que multi-head attention melhora a capacidade do modelo
- visualizar como o m√≥dulo de aten√ß√£o se encaixa dentro de um Transformer

Este cap√≠tulo marca a transi√ß√£o entre **prepara√ß√£o de dados** e **arquitetura neural real**.

---

## üß≠ Como este cap√≠tulo est√° organizado

O conte√∫do segue uma progress√£o do intuitivo para o t√©cnico:

### 1. O problema do contexto
- Por que linguagem n√£o pode ser processada palavra por palavra
- Depend√™ncias de longo alcance

### 2. Intui√ß√£o da aten√ß√£o
- Aten√ß√£o como pondera√ß√£o de import√¢ncia
- Vetores de contexto como m√©dias ponderadas

### 3. Self-Attention b√°sico
- C√°lculo de pesos de aten√ß√£o
- Normaliza√ß√£o via softmax
- Constru√ß√£o do vetor de contexto

### 4. Query, Key e Value
- Proje√ß√µes trein√°veis
- Representa√ß√£o flex√≠vel do contexto

### 5. Self-attention com pesos trein√°veis
- Aprendizado autom√°tico de rela√ß√µes lingu√≠sticas
- Constru√ß√£o do mecanismo usado em Transformers

### 6. M√°scara causal
- Restri√ß√£o de acesso ao futuro
- Base do treinamento autoregressivo

### 7. Multi-head attention
- Aten√ß√£o paralela
- Captura de m√∫ltiplos tipos de rela√ß√µes sem√¢nticas

### 8. Aten√ß√£o dentro do Transformer
- Como o m√≥dulo de aten√ß√£o se conecta ao restante da arquitetura GPT-like

Cada se√ß√£o √© acompanhada por infogr√°ficos e implementa√ß√µes progressivas em c√≥digo.

---

## üß™ Parte pr√°tica (notebook)

Este cap√≠tulo inclui um notebook execut√°vel no Google Colab que implementa, passo a passo:

- mecanismo de aten√ß√£o simplificado
- c√°lculo de pesos de aten√ß√£o
- implementa√ß√£o de Q, K e V
- aplica√ß√£o de m√°scara causal
- implementa√ß√£o conceitual de multi-head attention

O foco √© **entendimento profundo do mecanismo**, n√£o otimiza√ß√£o ou performance.

---

## üìä Infogr√°ficos

Este cap√≠tulo utiliza infogr√°ficos para ilustrar:

- por que contexto global √© necess√°rio
- funcionamento intuitivo da aten√ß√£o
- c√°lculo de vetores de contexto
- proje√ß√µes Q, K e V
- m√°scara causal
- multi-head attention
- posi√ß√£o do m√≥dulo de aten√ß√£o dentro do Transformer

Os detalhes e prompts dos infogr√°ficos est√£o documentados em:

- `infograficos/README.md`

---

## ‚ñ∂Ô∏è Como executar

1. Leia o di√°rio do cap√≠tulo:
   - `diario.md`

2. Execute o notebook:
   - `notebook.ipynb`

3. Ou abra diretamente no Google Colab:
   - veja os links em `links.md`

---


## üé• Material complementar

Este cap√≠tulo √© fortemente baseado nas explica√ß√µes do pr√≥prio autor do livro, que aprofunda visualmente o mecanismo de aten√ß√£o:

- V√≠deo: *Self-Attention Explained* ‚Äî Sebastian Raschka

O v√≠deo refor√ßa a intui√ß√£o e complementa a implementa√ß√£o apresentada neste cap√≠tulo.

---

## üìò Refer√™ncia

Este cap√≠tulo √© baseado no Cap√≠tulo 3 do livro  
**Build a Large Language Model (From Scratch)** ‚Äî Sebastian Raschka.

O conte√∫do foi adaptado para:

- abordagem did√°tica em portugu√™s
- execu√ß√£o pr√°tica no Google Colab
- foco em constru√ß√£o conceitual progressiva
- visualiza√ß√£o detalhada dos mecanismos internos do Transformer

---

## ‚ö†Ô∏è Observa√ß√£o importante

Self-attention √© o n√∫cleo matem√°tico e arquitetural dos LLMs modernos.

Este cap√≠tulo √© naturalmente mais t√©cnico e exige leitura cuidadosa.

Entretanto, dominar aten√ß√£o permite compreender:

- Transformers
- GPT
- BERT
- Modelos multimodais
- Arquiteturas modernas de IA generativa

Este √© o ponto onde a ‚Äúcaixa preta‚Äù come√ßa a se abrir.


