# Infogr√°ficos ‚Äî Cap√≠tulo 03 (Self-Attention)

Este diret√≥rio cont√©m os infogr√°ficos que ilustram os conceitos fundamentais do mecanismo de aten√ß√£o em Transformers.

O objetivo dos infogr√°ficos deste cap√≠tulo √© transformar conceitos matem√°ticos complexos em representa√ß√µes visuais intuitivas, ajudando o leitor a construir modelos mentais claros sobre:

- como contexto √© processado
- como aten√ß√£o calcula relev√¢ncia
- como surgem Q, K e V
- como funciona m√°scara causal
- como multi-head attention amplia a capacidade do modelo
- como self-attention se encaixa na arquitetura Transformer

---

## üéØ Estrat√©gia pedag√≥gica

Os infogr√°ficos seguem a progress√£o:

Problema ‚Üí Intui√ß√£o ‚Üí Matem√°tica ‚Üí Arquitetura


---

## üìä Lista de infogr√°ficos


01-contexto-importa.png
02-self-attention-intuicao.png
03-weighted-context.png
04-qkv-projecoes.png
05-self-attention-treinavel.png
06-causal-mask.png
07-dropout-attention.png
08-multi-head.png
09-self-attention-no-transformer.png





