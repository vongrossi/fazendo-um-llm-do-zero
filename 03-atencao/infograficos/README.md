# Infogr√°ficos ‚Äî Cap√≠tulo 03: Aten√ß√£o (Self-Attention)

Este diret√≥rio cont√©m os infogr√°ficos utilizados no Cap√≠tulo 03 da s√©rie
**Fazendo um LLM do Zero**.

Este cap√≠tulo foca no mecanismo mais revolucion√°rio dos Transformers:
a capacidade de processar contexto e relacionar palavras distantes.

Os infogr√°ficos aqui t√™m como objetivo:
- visualizar como o contexto √© constru√≠do
- desmistificar as matrizes Query, Key e Value
- explicar o funcionamento da m√°scara causal
- mostrar onde a aten√ß√£o se encaixa na arquitetura maior

---

## üìä Lista de Infogr√°ficos do Cap√≠tulo

### 01 ‚Äî Contexto importa
**Arquivo:** `01-contexto-importa.svg`  
**Se√ß√£o do di√°rio:** *O problema do contexto*  

Ilustra por que a tradu√ß√£o palavra por palavra falha e como o significado
depende inteiramente do contexto da frase.

---

### 02 ‚Äî Intui√ß√£o da aten√ß√£o
**Arquivo:** `02-self-attention-intuicao.svg`  
**Se√ß√£o do di√°rio:** *A ideia intuitiva da aten√ß√£o*  

Mostra visualmente como cada palavra "olha" para outras palavras da frase
para decidir sua import√¢ncia (pesos de aten√ß√£o).

---

### 03 ‚Äî Weighted context vector
**Arquivo:** `03-weighted-context.svg`  
**Se√ß√£o do di√°rio:** *Construindo vetores de contexto*  

Explica matematicamente como o vetor de contexto √© calculado como uma
soma ponderada dos vetores de entrada.

---

### 04 ‚Äî Proje√ß√µes Q, K e V
**Arquivo:** `04-qkv-projecoes.svg`  
**Se√ß√£o do di√°rio:** *Introduzindo Query, Key e Value*  

Apresenta a analogia de banco de dados (Query, Key, Value) aplicada
ao processamento de linguagem natural.

---

### 05 ‚Äî Self-attention trein√°vel
**Arquivo:** `05-self-attention-treinavel.svg`  
**Se√ß√£o do di√°rio:** *Self-Attention com pesos trein√°veis*  

Mostra como as matrizes de peso ($W_q, W_k, W_v$) transformam embeddings
em proje√ß√µes que o modelo pode aprender.

---

### 06 ‚Äî M√°scara causal
**Arquivo:** `06-causal-mask.svg`  
**Se√ß√£o do di√°rio:** *M√°scara causal: impedindo o modelo de ver o futuro*  

Visualiza a matriz triangular que bloqueia o acesso a tokens futuros,
essencial para o treinamento autoregressivo (GPT).

---

### 07 ‚Äî Dropout na aten√ß√£o
**Arquivo:** `07-dropout-attention.svg`  
**Se√ß√£o do di√°rio:** *Dropout na aten√ß√£o*  

Ilustra como o dropout zera aleatoriamente alguns pesos de aten√ß√£o
durante o treino para prevenir overfitting.

---

### 08 ‚Äî Multi-head attention
**Arquivo:** `08-multi-head.svg`  
**Se√ß√£o do di√°rio:** *Multi-Head Attention*  

Mostra como dividir o processamento em m√∫ltiplas "cabe√ßas" permite
que o modelo capture diferentes tipos de rela√ß√µes simultaneamente.

---

### 09 ‚Äî Self-attention no Transformer
**Arquivo:** `09-self-attention-no-transformer.svg`  
**Se√ß√£o do di√°rio:** *Onde a aten√ß√£o entra no Transformer*  

Situa o bloco de aten√ß√£o dentro da arquitetura completa, mostrando
sua conex√£o com as camadas de entrada e sa√≠da.

---

## üé® Diretrizes visuais

Todos os infogr√°ficos devem:
- manter consist√™ncia visual entre si
- usar cores suaves e profissionais
- evitar excesso de texto
- priorizar leitura r√°pida em artigos t√©cnicos (Medium / Dev.to)