
# Fazendo um LLM do Zero ðŸ§ ðŸ¤–

![Educational Project](https://img.shields.io/badge/Purpose-Educational-blue)
![Google Colab](https://img.shields.io/badge/Google%20Colab-Ready-orange)
![Language](https://img.shields.io/badge/Language-PortuguÃªs%20(BR)-green)
![Status](https://img.shields.io/badge/Status-Em%20EvoluÃ§Ã£o-yellow)
![Open Source Learning](https://img.shields.io/badge/Open%20Source-Learning-purple)

![Chapters](https://img.shields.io/badge/Chapters-8-blue)
![Medium Series](https://img.shields.io/badge/Series-Medium%20Articles-black)
![Educational Series](https://img.shields.io/badge/Type-Educational%20Series-brightgreen)
![Hands-On Learning](https://img.shields.io/badge/Approach-Hands--On%20Learning-orange)

---

## ðŸŽ¯ Sobre Este Projeto

Este repositÃ³rio documenta, passo a passo, a construÃ§Ã£o de um **Large Language Model (LLM) do zero**, com foco em **entendimento fundamental** â€” e nÃ£o apenas no uso de APIs prontas.

O objetivo aqui **nÃ£o Ã© criar um concorrente do ChatGPT**, mas compreender profundamente:

- como texto vira nÃºmero  
- como mecanismos de atenÃ§Ã£o operam  
- como modelos GPT sÃ£o estruturados  
- como o treinamento molda comportamento  
- como modelos aprendem a seguir instruÃ§Ãµes humanas  
- e por que entender esses fundamentos muda completamente a forma como usamos IA  

---

## ðŸ’¡ Como Este Projeto Surgiu

A ideia deste projeto nasceu de uma curiosidade pessoal que sempre me acompanhou:

> Eu sempre tive curiosidade de entender como as coisas realmente funcionam por dentro.

Ao estudar inteligÃªncia artificial e Large Language Models, percebi que muitas vezes aprendemos a usar ferramentas extremamente poderosas sem compreender seus fundamentos.

Buscar respostas mais profundas me levou a perceber algo importante:

ðŸ‘‰ Para entender sistemas complexos, aprender fazendo Ã© frequentemente o caminho mais eficaz.

Este repositÃ³rio nasceu como uma forma de:

- reforÃ§ar meus estudos  
- registrar o processo de aprendizado  
- compartilhar conhecimento  
- tornar conceitos complexos mais acessÃ­veis  

---

## ðŸ§  Filosofia Educacional

Este projeto segue alguns princÃ­pios fundamentais:

- aprendizado baseado em construÃ§Ã£o prÃ¡tica  
- progressÃ£o conceitual gradual  
- explicaÃ§Ãµes visuais e didÃ¡ticas  
- execuÃ§Ã£o acessÃ­vel via Google Colab  
- transparÃªncia sobre limitaÃ§Ãµes dos modelos didÃ¡ticos  

O foco principal Ã© construir **modelo mental e compreensÃ£o estrutural**, nÃ£o escalar para bilhÃµes de parÃ¢metros.

---

## ðŸ“– Leitura Recomendada (Artigos)

Para uma experiÃªncia de aprendizado completa, acompanhe a sÃ©rie de artigos que explicam a teoria por trÃ¡s de cada capÃ­tulo:

| CapÃ­tulo | Tema | Dev.to | Medium |
| :--- | :--- | :---: | :---: |
| **00** | Antes da InteligÃªncia: A Oficina | [ðŸ”—](https://dev.to/vongrossi/fazendo-um-llm-do-zero-00-antes-da-inteligencia-a-oficina-4n6f) | [ðŸ”—](https://medium.com/@angelovongrossi/fazendo-um-llm-do-zero-00-antes-da-inteligÃªncia-a-oficina-ï¸-84ee8926863e) |
| **01** | A MÃ¡gica Ã© EstatÃ­stica | [ðŸ”—](https://dev.to/vongrossi/fazendo-um-llm-do-zero-01-a-magica-e-apenas-estatistica-bem-feita-7k) | [ðŸ”—](https://medium.com/@angelovongrossi/fazendo-um-llm-do-zero-01-a-mÃ¡gica-Ã©-apenas-estatÃ­stica-bem-feita-0843ae21714a) |
| **02** | Texto Vira NÃºmero | [ðŸ”—](https://dev.to/vongrossi/fazendo-um-llm-do-zero-02-como-transformar-palavras-em-numeros-sem-perder-a-alma-1mm3) | [ðŸ”—](https://medium.com/@angelovongrossi/fazendo-um-llm-do-zero-02-como-transformar-palavras-em-nÃºmeros-sem-perder-a-alma-fc8970ca2dd5) |
| **03** | AtenÃ§Ã£o Ã© Tudo | [ðŸ”—](https://dev.to/vongrossi/fazendo-um-llm-do-zero-03-atencao-e-tudo-o-que-voce-precisa-3lk5) | [ðŸ”—](https://medium.com/@angelovongrossi/fazendo-um-llm-do-zero-03-atenÃ§Ã£o-Ã©-tudo-o-que-vocÃª-precisa-7890b732cc3d) |
| **04** | Arquitetura GPT | [ðŸ”—](https://dev.to/vongrossi/fazendo-um-llm-do-zero-sessao-04-a-arquitetura-da-mente-construindo-o-corpo-do-gpt-5725) | [ðŸ”—](https://medium.com/@angelovongrossi/fazendo-um-llm-do-zero-sessÃ£o-04-a-arquitetura-da-mente-construindo-o-corpo-do-gpt-ï¸-ddd1d8dba397) |
| **05** | PrÃ©-Treinamento | [ðŸ”—](https://dev.to/vongrossi/fazendo-um-llm-do-zero-sessao-05-ensinando-o-modelo-a-falar-a-escola-da-probabilidade-1b68) | [ðŸ”—](https://medium.com/@angelovongrossi/fazendo-um-llm-do-zero-sessÃ£o-05-ensinando-o-modelo-a-falar-a-escola-da-probabilidade-ï¸-d0da53964678) |
| **06** | Fine-Tuning | [ðŸ”—](https://dev.to/vongrossi/fazendo-um-llm-do-zero-sessao-06-dando-uma-profissao-ao-modelo-fine-tuning-19kc) | [ðŸ”—](https://medium.com/@angelovongrossi/fazendo-um-llm-do-zero-sessÃ£o-06-dando-uma-profissÃ£o-ao-modelo-fine-tuning-ï¸-f842094b9185) |
| **07** | Instruction Tuning | [ðŸ”—](https://dev.to/vongrossi/fazendo-um-llm-do-zero-sessao-07-de-gerador-de-texto-a-assistente-instruction-tuning-26cg) | [ðŸ”—](https://medium.com/@angelovongrossi/fazendo-um-llm-do-zero-sessÃ£o-07-de-gerador-de-texto-a-assistente-instruction-tuning-1a0a77d9994a) |
| **08** | O Fim do ComeÃ§o | [ðŸ”—](https://dev.to/vongrossi/fazendo-um-llm-do-zero-sessao-08-o-fim-do-comeco-por-que-agora-tudo-faz-sentido-5f7p) | [ðŸ”—](https://medium.com/@angelovongrossi/fazendo-um-llm-do-zero-sessÃ£o-08-o-fim-do-comeÃ§o-por-que-agora-tudo-faz-sentido-763e75dede24) |

---

## ðŸ“š Estrutura da Jornada

Cada pasta representa um capÃ­tulo progressivo de aprendizado:

```text
00-passo-zero/         â†’ Ambiente, Google Colab, PyTorch e conceitos base
01-o-que-e-um-llm/     â†’ O que Ã© um LLM de verdade
02-texto-vira-numero/  â†’ TokenizaÃ§Ã£o e embeddings
03-atencao/            â†’ Self-attention e multi-head attention
04-gpt-do-zero/        â†’ Construindo um GPT do zero
05-pre-treinamento/    â†’ PrÃ©-treinamento e geraÃ§Ã£o de texto
06-fine-tuning/        â†’ Ajustando comportamento do modelo
07-instruction-tuning/ â†’ Modelos que seguem instruÃ§Ãµes
08-extras/             â†’ ReflexÃµes, experimentos e ideias futuras
````

---

## ðŸ§­ Caminho Conceitual da SÃ©rie

Esta sÃ©rie acompanha a evoluÃ§Ã£o fundamental dos modelos modernos:

```
Texto
â†’ Tokens
â†’ Embeddings
â†’ AtenÃ§Ã£o
â†’ Transformers
â†’ GPT
â†’ PrÃ©-Treinamento
â†’ Fine-Tuning
â†’ Instruction Tuning
```

---

## â˜ï¸ Por que Google Colab?

O Google Colab Ã© a base prÃ¡tica deste projeto porque:

* elimina necessidade de setup local
* oferece CPU/GPU sob demanda
* garante reprodutibilidade
* permite execuÃ§Ã£o com um Ãºnico clique

Isso permite focar no aprendizado conceitual e nÃ£o em infraestrutura.

---

## ðŸ“¦ O que vocÃª encontrarÃ¡ em cada capÃ­tulo

Cada capÃ­tulo contÃ©m:

* ðŸ“– roteiro conceitual em Markdown
* ðŸ§ª notebook executÃ¡vel
* ðŸ“Š infogrÃ¡ficos didÃ¡ticos
* ðŸ”— links diretos para execuÃ§Ã£o no Google Colab

---

## ðŸŒŽ Acessibilidade e TraduÃ§Ã£o TÃ©cnica

Este material foi produzido em portuguÃªs brasileiro com o objetivo de ampliar o acesso ao aprendizado de IA.

âš ï¸ Este projeto **nÃ£o Ã© uma traduÃ§Ã£o do material original**, mas uma adaptaÃ§Ã£o educacional baseada no estudo dos conceitos apresentados na obra de referÃªncia.

---

## ðŸ“˜ ReferÃªncia Principal

Este projeto foi profundamente inspirado no livro:

**Build a Large Language Model (From Scratch)**
Sebastian Raschka

https://www.manning.com/books/build-a-large-language-model-from-scratch

RepositÃ³rio oficial:
[https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)



---

## ðŸš€ Continuidade do Estudo

ApÃ³s completar esta sÃ©rie, caminhos naturais incluem:

* RLHF (Reinforcement Learning with Human Feedback)
* RAG (Retrieval-Augmented Generation)
* Small Language Models (SLMs)
* Edge AI e IoT Cognitivo
* Deployment e LLMOps
* Sistemas multi-agente

---

## ðŸ¤ Comunidade e ColaboraÃ§Ã£o

Este Ã© um projeto educacional aberto.

ContribuiÃ§Ãµes sÃ£o bem-vindas para:

* melhorias didÃ¡ticas
* novos exemplos
* novos experimentos
* material visual
* expansÃ£o de conteudos

---

## ðŸ™ Agradecimentos

Este projeto sÃ³ existe graÃ§as ao trabalho da comunidade open source e educacional de Machine Learning, especialmente:

* PyTorch
* NumPy
* Matplotlib
* Google Colab
* Comunidade Open Source de IA

E, principalmente, ao trabalho educacional de Sebastian Raschka, cuja obra inspirou esta jornada.

---

## âš ï¸ Aviso Honesto

Este Ã© um projeto **educacional**.

Os modelos aqui sÃ£o pequenos e didÃ¡ticos, mas utilizam os mesmos princÃ­pios fundamentais empregados em grandes modelos de produÃ§Ã£o.

O objetivo Ã© desenvolver compreensÃ£o estrutural e pensamento crÃ­tico sobre IA.

---

## âœ¨ ReflexÃ£o Final

> Saber usar uma ferramenta Ã© valioso.
> Entender como ela funciona Ã© que muda nossa forma de agir e pensar.

