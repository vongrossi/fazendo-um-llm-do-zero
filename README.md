# üß† Fazendo um LLM do Zero (Home-Made LLM)

Este reposit√≥rio √© o registro de estudos do projeto **"Fazer um LLM do Zero"**. Funciona como meu di√°rio de bordo t√©cnico; a ideia √© abrir a caixa preta e entender como os Large Language Models (LLMs) realmente funcionam "embaixo do cap√¥".

O modelo √© baseado na arquitetura do **GPT**, utilizando apenas **PyTorch** e o seu core.

> **"Existe uma diferen√ßa brutal entre apenas saber chamar uma API e entender exatamente o que est√° sendo processado entre o seu request e a resposta que chega. Este projeto √© sobre fechar esse buraco e entender o fluxo real dos dados."**

---

## üöÄ O Projeto & Miss√£o
Este guia √© integralmente baseado na obra de refer√™ncia da √°rea:
* **Livro:** [Build a Large Language Model (from Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch) - Sebastian Raschka.
* **Reposit√≥rio Oficial:** [rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch).

**Minha Vis√£o do projeto:** Traduzir este conhecimento complexo e extremamente importante dado o momomento que estamos para o Portugu√™s e torn√°-lo acess√≠vel. 
- **Sem Barreiras de Hardware**: Todo o c√≥digo √© projetado para rodar no **Google Colab (GPU Gratuita)**, garantindo que o hardware n√£o seja um bloqueio para o aprendizado de ponta.
- **Componentes por M√≥dulo**: Constru√ß√£o passo a passo, do tratamento de dados ao ajuste fino (fine-tuning).

---

## üó∫Ô∏è Roadmap de Desenvolvimento (Baseado no Livro)

Seguiremos a estrutura de cap√≠tulos para garantir uma base s√≥lida:

| Cap√≠tulo | Status | Descri√ß√£o | Guia / Blog |
| :--- | :--- | :--- | :--- |
| **01. Intro** | ‚úÖ | Anatomia dos LLMs e pipeline de treinamento. | [Post #1](docs/post_01.md) |
| **02. Dados** | ‚è≥ | Tokeniza√ß√£o (BPE), Data Loaders e Embeddings. | [Em breve] |
| **03. Aten√ß√£o** | ‚è≥ | Mecanismos de Attention (Self, Causal, Multi-head). | [Em breve] |
| **04. Arquitetura** | ‚è≥ | Implementando o GPT: blocos Transformer e Camadas. | [Em breve] |
| **05. Pr√©-treino** | ‚è≥ | Otimiza√ß√£o, perda (loss) e carregamento de pesos. | [Em breve] |
| **06. Tuning I** | ‚è≥ | Fine-tuning para Classifica√ß√£o (Spam/Sentimentos). | [Em breve] |
| **07. Tuning II** | ‚è≥ | Fine-tuning para Instru√ß√µes (Assistente de Chat). | [Em breve] |

---

## üõ†Ô∏è Tecnologias 
* **Linguagem:** Python 3.10+
* **Framework:** PyTorch (Core/Puro)
* **Tokeniza√ß√£o:** Tiktoken (OpenAI - Byte Pair Encoding)
* **Ambiente de Execu√ß√£o:** Google Colab / Jupyter Notebooks

---

## üìÇ Organiza√ß√£o do Reposit√≥rio
* `/notebooks`: Arquivos `.ipynb` detalhados e compat√≠veis com Google Colab.
* `/src`: M√≥dulos Python reutiliz√°veis (o "engine" modular do LLM).
* `/docs`: Documenta√ß√£o t√©cnica e rascunhos para os artigos explicativos.
* `/data`: Amostras de datasets para testes (ex: TinyShakespeare).

---

## üìú Licen√ßa
Distribu√≠do sob a licen√ßa MIT. Veja `LICENSE` para mais informa√ß√µes.

---
*Criado com ‚òï por **vongrossi** como parte de um estudo profundo sobre Intelig√™ncia Artificial.*