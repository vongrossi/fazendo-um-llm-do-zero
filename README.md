
# Fazendo um LLM do Zero ðŸ§ ðŸ¤–

![Educational Project](https://img.shields.io/badge/Purpose-Educational-blue)
![Google Colab](https://img.shields.io/badge/Google%20Colab-Ready-orange)
![Language](https://img.shields.io/badge/Language-PortuguÃªs%20(BR)-green)
![Status](https://img.shields.io/badge/Status-Em%20EvoluÃ§Ã£o-yellow)
![Open Source Learning](https://img.shields.io/badge/Open%20Source-Learning-purple)

![Chapters](https://img.shields.io/badge/Chapters-8-blue)
![Medium Series](https://img.shields.io/badge/Series-Medium%20Articles-black)
![Educational Series](https://img.shields.io/badge/Type-Educational%20Series-brightgreen)
![Hands-On Learning](https://img.shields.io/badge/Approach-Hands--On%20Learning-orange)

---

## ðŸŽ¯ Sobre Este Projeto

Este repositÃ³rio documenta, passo a passo, a construÃ§Ã£o de um **Large Language Model (LLM) do zero**, com foco em **entendimento fundamental** â€” e nÃ£o apenas no uso de APIs prontas.

O objetivo aqui **nÃ£o Ã© criar um concorrente do ChatGPT**, mas compreender profundamente:

- como texto vira nÃºmero  
- como mecanismos de atenÃ§Ã£o operam  
- como modelos GPT sÃ£o estruturados  
- como o treinamento molda comportamento  
- como modelos aprendem a seguir instruÃ§Ãµes humanas  
- e por que entender esses fundamentos muda completamente a forma como usamos IA  

---

## ðŸ’¡ Como Este Projeto Surgiu

A ideia deste projeto nasceu de uma curiosidade pessoal que sempre me acompanhou:

> Eu sempre tive curiosidade de entender como as coisas realmente funcionam por dentro.

Ao estudar inteligÃªncia artificial e Large Language Models, percebi que muitas vezes aprendemos a usar ferramentas extremamente poderosas sem compreender seus fundamentos.

Buscar respostas mais profundas me levou a perceber algo importante:

ðŸ‘‰ Para entender sistemas complexos, aprender fazendo Ã© frequentemente o caminho mais eficaz.

Este repositÃ³rio nasceu como uma forma de:

- reforÃ§ar meus estudos  
- registrar o processo de aprendizado  
- compartilhar conhecimento  
- tornar conceitos complexos mais acessÃ­veis  

---

## ðŸ§  Filosofia Educacional

Este projeto segue alguns princÃ­pios fundamentais:

- aprendizado baseado em construÃ§Ã£o prÃ¡tica  
- progressÃ£o conceitual gradual  
- explicaÃ§Ãµes visuais e didÃ¡ticas  
- execuÃ§Ã£o acessÃ­vel via Google Colab  
- transparÃªncia sobre limitaÃ§Ãµes dos modelos didÃ¡ticos  

O foco principal Ã© construir **modelo mental e compreensÃ£o estrutural**, nÃ£o escalar para bilhÃµes de parÃ¢metros.

---

## ðŸ“š Estrutura da Jornada

Cada pasta representa um capÃ­tulo progressivo de aprendizado:

```text
00-passo-zero/         â†’ Ambiente, Google Colab, PyTorch e conceitos base
01-o-que-e-um-llm/     â†’ O que Ã© um LLM de verdade
02-texto-vira-numero/  â†’ TokenizaÃ§Ã£o e embeddings
03-atencao/            â†’ Self-attention e multi-head attention
04-gpt-do-zero/        â†’ Construindo um GPT do zero
05-pre-treinamento/    â†’ PrÃ©-treinamento e geraÃ§Ã£o de texto
06-fine-tuning/        â†’ Ajustando comportamento do modelo
07-instruction-tuning/ â†’ Modelos que seguem instruÃ§Ãµes
08-extras/             â†’ ReflexÃµes, experimentos e ideias futuras
````

---

## ðŸ§­ Caminho Conceitual da SÃ©rie

Esta sÃ©rie acompanha a evoluÃ§Ã£o fundamental dos modelos modernos:

```
Texto
â†’ Tokens
â†’ Embeddings
â†’ AtenÃ§Ã£o
â†’ Transformers
â†’ GPT
â†’ PrÃ©-Treinamento
â†’ Fine-Tuning
â†’ Instruction Tuning
```

---

## â˜ï¸ Por que Google Colab?

O Google Colab Ã© a base prÃ¡tica deste projeto porque:

* elimina necessidade de setup local
* oferece CPU/GPU sob demanda
* garante reprodutibilidade
* permite execuÃ§Ã£o com um Ãºnico clique

Isso permite focar no aprendizado conceitual e nÃ£o em infraestrutura.

---

## ðŸ“¦ O que vocÃª encontrarÃ¡ em cada capÃ­tulo

Cada capÃ­tulo contÃ©m:

* ðŸ“– roteiro conceitual em Markdown
* ðŸ§ª notebook executÃ¡vel
* ðŸ“Š infogrÃ¡ficos didÃ¡ticos
* ðŸ”— links diretos para execuÃ§Ã£o no Google Colab

---

## ðŸŒŽ Acessibilidade e TraduÃ§Ã£o TÃ©cnica

Este material foi produzido em portuguÃªs brasileiro com o objetivo de ampliar o acesso ao aprendizado de IA.

âš ï¸ Este projeto **nÃ£o Ã© uma traduÃ§Ã£o do material original**, mas uma adaptaÃ§Ã£o educacional baseada no estudo dos conceitos apresentados na obra de referÃªncia.

---

## ðŸ“˜ ReferÃªncia Principal

Este projeto foi profundamente inspirado no livro:

**Build a Large Language Model (From Scratch)**
Sebastian Raschka

https://www.manning.com/books/build-a-large-language-model-from-scratch

RepositÃ³rio oficial:
[https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)



---

## ðŸš€ Continuidade do Estudo

ApÃ³s completar esta sÃ©rie, caminhos naturais incluem:

* RLHF (Reinforcement Learning with Human Feedback)
* RAG (Retrieval-Augmented Generation)
* Small Language Models (SLMs)
* Edge AI e IoT Cognitivo
* Deployment e LLMOps
* Sistemas multi-agente

---

## ðŸ¤ Comunidade e ColaboraÃ§Ã£o

Este Ã© um projeto educacional aberto.

ContribuiÃ§Ãµes sÃ£o bem-vindas para:

* melhorias didÃ¡ticas
* novos exemplos
* novos experimentos
* material visual
* expansÃ£o de conteudos

---

## ðŸ™ Agradecimentos

Este projeto sÃ³ existe graÃ§as ao trabalho da comunidade open source e educacional de Machine Learning, especialmente:

* PyTorch
* NumPy
* Matplotlib
* Google Colab
* Comunidade Open Source de IA

E, principalmente, ao trabalho educacional de Sebastian Raschka, cuja obra inspirou esta jornada.

---

## âš ï¸ Aviso Honesto

Este Ã© um projeto **educacional**.

Os modelos aqui sÃ£o pequenos e didÃ¡ticos, mas utilizam os mesmos princÃ­pios fundamentais empregados em grandes modelos de produÃ§Ã£o.

O objetivo Ã© desenvolver compreensÃ£o estrutural e pensamento crÃ­tico sobre IA.

---

## âœ¨ ReflexÃ£o Final

> Saber usar uma ferramenta Ã© valioso.
> Entender como ela funciona Ã© que muda nossa forma de agir e pensar.

