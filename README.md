# Fazendo um LLM do Zero ğŸ§ ğŸ¤–

Este repositÃ³rio documenta, passo a passo, a construÃ§Ã£o de um **Large Language Model (LLM) do zero**, com foco em **entendimento fundamental** â€” e nÃ£o apenas no uso de APIs prontas.

Aqui, o objetivo **nÃ£o Ã© criar um concorrente do ChatGPT**, mas compreender profundamente **como LLMs funcionam por dentro**:

- como texto vira nÃºmero  
- como mecanismos de atenÃ§Ã£o operam  
- como um modelo do tipo GPT Ã© estruturado  
- como o treinamento molda comportamento  
- e por que tudo isso muda completamente a forma como usamos IA no dia a dia  

Todo o cÃ³digo desta sÃ©rie roda no **Google Colab**, permitindo que qualquer pessoa acompanhe o conteÃºdo **sem precisar de hardware potente**.

---

## ğŸ“š Estrutura da SÃ©rie

Cada pasta representa um capÃ­tulo da jornada de aprendizado:

```text
00-passo-zero/        â†’ Ambiente, Google Colab, PyTorch e conceitos base
01-o-que-e-um-llm/    â†’ O que Ã© um LLM de verdade
02-texto-vira-numero/ â†’ TokenizaÃ§Ã£o e embeddings
03-atencao/           â†’ Self-attention e multi-head attention
04-gpt-do-zero/       â†’ Construindo um GPT do zero
05-treinamento/       â†’ Treinamento, loss e geraÃ§Ã£o de texto
06-fine-tuning/       â†’ Ajustando comportamento do modelo
07-instruction-tuning â†’ Modelos que seguem instruÃ§Ãµes
```

---

## â˜ï¸ Por que Google Colab?

O Google Colab Ã© a base prÃ¡tica deste projeto porque:

* elimina a necessidade de setup local
* oferece CPU/GPU sob demanda
* garante reprodutibilidade dos experimentos
* permite executar os notebooks com **um Ãºnico clique**

Isso torna o aprendizado mais acessÃ­vel, focando no **entendimento dos conceitos**, e nÃ£o em infraestrutura.

---

## ğŸ“¦ O que vocÃª vai encontrar em cada capÃ­tulo

Cada capÃ­tulo da sÃ©rie contÃ©m:

* ğŸ“– um **roteiro em Markdown** (origem do post no Medium)
* ğŸ§ª um **notebook executÃ¡vel**
* ğŸ“Š **infogrÃ¡ficos didÃ¡ticos** para visualizaÃ§Ã£o dos conceitos
* ğŸ”— **links diretos para abrir o notebook no Google Colab**

---

## ğŸ“˜ ReferÃªncia

Este projeto Ã© inspirado no livro
**Build a Large Language Model (From Scratch)**, de *Sebastian Raschka*,
adaptando os conceitos para uma abordagem didÃ¡tica em **portuguÃªs** e execuÃ§Ã£o prÃ¡tica no **Google Colab**.

---

## âš ï¸ Aviso honesto

Este Ã© um projeto **educacional**.

Os modelos construÃ­dos aqui sÃ£o pequenos e didÃ¡ticos, mas utilizam **os mesmos princÃ­pios fundamentais** empregados em grandes modelos de produÃ§Ã£o.

O foco Ã© construir **modelo mental**, nÃ£o escalar para bilhÃµes de parÃ¢metros.
