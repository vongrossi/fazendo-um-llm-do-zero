{
  "cells": [
      {
       "cell_type": "markdown",
       "source": [
        "# Cap√≠tulo 08 ‚Äî Explora√ß√µes Extras e Reflex√µes\n",
        "\n",
        "Este notebook acompanha o Cap√≠tulo 08 da s√©rie **Fazendo um LLM do Zero**.\n",
        "\n",
        "Este notebook √© um laborat√≥rio aberto.\n",
        "\n",
        "üéØ **Objetivos deste notebook:**\n",
        "- Limita√ß√µes dos modelos did√°ticos\n",
        "- Impacto do tamanho do dataset\n",
        "- Compara√ß√µes entre estrat√©gias\n",
        "- Gera√ß√£o multi-turno\n",
        "- Experimentos livres\n"
       ],
       "metadata": {
        "id": "3mzhlzIZlXRr"
       },
       "id": "3mzhlzIZlXRr"
      },
      {
       "cell_type": "markdown",
       "source": [
        "## 1. Setup e Configura√ß√£o"
       ],
       "metadata": {
        "id": "setup-header"
       },
       "id": "setup-header"
      },
      {
       "cell_type": "code",
       "source": [
        "# ============================================================\n",
        "# Setup do reposit√≥rio\n",
        "# ============================================================\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/vongrossi/fazendo-um-llm-do-zero.git\"\n",
        "REPO_DIR = \"fazendo-um-llm-do-zero\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone {REPO_URL}\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print(\"Diret√≥rio atual:\", os.getcwd())\n"
       ],
       "metadata": {
        "id": "fouB7NoRlUfp"
       },
       "id": "fouB7NoRlUfp",
       "execution_count": null,
       "outputs": []
      },
      {
       "cell_type": "markdown",
       "source": [
        "### 1.1 Depend√™ncias e Imports"
       ],
       "metadata": {
        "id": "gjXJfafGlcQi"
       },
       "id": "gjXJfafGlcQi"
      },
      {
       "cell_type": "code",
       "source": [
        "!pip -q install -r 08-extras/requirements.txt\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "# Adiciona raiz ao path\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n"
       ],
       "metadata": {
        "id": "NbO_mduLlc5b"
       },
       "id": "NbO_mduLlc5b",
       "execution_count": null,
       "outputs": []
      },    {
      "cell_type": "markdown",
      "source": [
        "Carregar Modelo Instruction Tuned"
      ],
      "metadata": {
        "id": "Wc2XG9_Ilgga"
      },
      "id": "Wc2XG9_Ilgga"
    },
    {
      "cell_type": "code",
      "source": [
        "from lib.gptmini import GPTConfig, GPTMini\n",
        "\n",
        "config = GPTConfig(\n",
        "    vocab_size=2000,\n",
        "    context_size=64,\n",
        "    d_model=128,\n",
        "    n_heads=4,\n",
        "    n_layers=2\n",
        ")\n",
        "\n",
        "model = GPTMini(config).to(device)\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(\"07_instruction_gpt.pt\", map_location=device))\n",
        "    print(\"Modelo Instruction Tuned carregado\")\n",
        "except:\n",
        "    print(\"Modelo n√£o encontrado ‚Äî use checkpoints anteriores\")\n"
      ],
      "metadata": {
        "id": "J21OPWg0lhPO"
      },
      "id": "J21OPWg0lhPO",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "### 2.1 Fun√ß√µes de Tokeniza√ß√£o"
       ],
       "metadata": {
        "id": "ZVBRl9VWlk9T"
       },
       "id": "ZVBRl9VWlk9T"
      },    {
      "cell_type": "code",
      "source": [
        "chars = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,!?\\n:\")\n",
        "stoi = {c:i for i,c in enumerate(chars)}\n",
        "itos = {i:c for c,i in stoi.items()}\n",
        "\n",
        "def encode(text):\n",
        "    return [stoi[c] for c in text if c in stoi]\n",
        "\n",
        "def decode(tokens):\n",
        "    return \"\".join([itos[t] for t in tokens])\n"
      ],
      "metadata": {
        "id": "otfmFgqVllrm"
      },
      "id": "otfmFgqVllrm",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "## 3. Experimento 1 ‚Äî Impacto do Tamanho do Prompt\n",
        "\n",
        "Aqui vamos observar como o tamanho do contexto influencia a resposta do modelo."
       ],
       "metadata": {
        "id": "qvLJTz-Mlndg"
       },
       "id": "qvLJTz-Mlndg"
      },    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, prompt, max_tokens=80):\n",
        "\n",
        "    tokens = encode(prompt)\n",
        "    tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    for _ in range(max_tokens):\n",
        "        logits, _ = model(tokens)\n",
        "        next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
        "        tokens = torch.cat([tokens, next_token.unsqueeze(1)], dim=1)\n",
        "\n",
        "    return decode(tokens.squeeze().tolist())\n"
      ],
      "metadata": {
        "id": "WBBindBHlqHF"
      },
      "id": "WBBindBHlqHF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_curto = \"Explique IA:\"\n",
        "prompt_longo = \"Explique IA considerando aprendizado supervisionado e n√£o supervisionado:\"\n",
        "\n",
        "print(generate(model, prompt_curto))\n",
        "print(\"\\n---\\n\")\n",
        "print(generate(model, prompt_longo))\n"
      ],
      "metadata": {
        "id": "RvL5SiIsl0zQ"
      },
      "id": "RvL5SiIsl0zQ",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "## 4. Experimento 2 ‚Äî Limita√ß√£o de Dataset Pequeno\n",
        "\n",
        "Vamos observar comportamento fora do dom√≠nio treinado."
       ],
       "metadata": {
        "id": "lu9B14jjl3Bw"
       },
       "id": "lu9B14jjl3Bw"
      },    {
      "cell_type": "code",
      "source": [
        "print(generate(model, \"Explique f√≠sica qu√¢ntica:\"))\n"
      ],
      "metadata": {
        "id": "nEiW_h1wl5_1"
      },
      "id": "nEiW_h1wl5_1",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "## 5. Experimento 3 ‚Äî Simulando Conversa Multi-turno\n",
        "\n",
        "Aqui simulamos uma intera√ß√£o cont√≠nua com o modelo."
       ],
       "metadata": {
        "id": "3bJKZSSjl8Dy"
       },
       "id": "3bJKZSSjl8Dy"
      },    {
      "cell_type": "code",
      "source": [
        "conversation = \"\"\"\n",
        "Usu√°rio: O que √© Machine Learning?\n",
        "Assistente:\n",
        "\"\"\"\n",
        "\n",
        "print(generate(model, conversation))\n"
      ],
      "metadata": {
        "id": "SUVJIYWxmDej"
      },
      "id": "SUVJIYWxmDej",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "## 6. Experimento 4 ‚Äî Comparar Temperatura\n",
        "\n",
        "Criatividade vs Determinismo"
       ],
       "metadata": {
        "id": "hHlI9dC1mHOs"
       },
       "id": "hHlI9dC1mHOs"
      },    {
      "cell_type": "code",
      "source": [
        "def generate_temp(model, prompt, temperature=1.0):\n",
        "\n",
        "    tokens = encode(prompt)\n",
        "    tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    for _ in range(80):\n",
        "        logits, _ = model(tokens)\n",
        "\n",
        "        probs = torch.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
        "        next_token = torch.multinomial(probs, 1)\n",
        "\n",
        "        tokens = torch.cat([tokens, next_token], dim=1)\n",
        "\n",
        "    return decode(tokens.squeeze().tolist())\n"
      ],
      "metadata": {
        "id": "s3Yx5LTqmIB6"
      },
      "id": "s3Yx5LTqmIB6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_temp(model, \"Explique IA:\", temperature=0.3))\n",
        "print(\"\\n---\\n\")\n",
        "print(generate_temp(model, \"Explique IA:\", temperature=1.2))\n"
      ],
      "metadata": {
        "id": "z2LkanY7mP7h"
      },
      "id": "z2LkanY7mP7h",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "## 7. Experimento 5 ‚Äî Demonstrando Hallucination"
       ],
       "metadata": {
        "id": "OOaUZt_CmSa0"
       },
       "id": "OOaUZt_CmSa0"
      },    {
      "cell_type": "code",
      "source": [
        "print(generate(model, \"Quem descobriu a linguagem Python em 1800?\"))\n"
      ],
      "metadata": {
        "id": "d3sZOKuVmVgJ"
      },
      "id": "d3sZOKuVmVgJ",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "### 7.1 O Que Esses Experimentos Mostram\n",
        "\n",
        "Modelos pequenos:\n",
        "\n",
        "‚Ä¢ dependem fortemente do dataset  \n",
        "‚Ä¢ podem produzir respostas plaus√≠veis mas incorretas  \n",
        "‚Ä¢ possuem limita√ß√£o de contexto  \n",
        "‚Ä¢ demonstram comportamento probabil√≠stico  \n",
        "\n",
        "Essas caracter√≠sticas tamb√©m existem em modelos maiores, embora com menor frequ√™ncia.\n"
       ],
       "metadata": {
        "id": "c_az6fMjmYDw"
       },
       "id": "c_az6fMjmYDw"
      },    {
      "cell_type": "markdown",
      "source": [
        "üß™ Experimento 6 ‚Äî Prompt Engineering"
      ],
      "metadata": {
        "id": "7DrngsDpmbq-"
      },
      "id": "7DrngsDpmbq-"
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(model, \"Responda como um professor explicando IA para iniciantes:\"))\n"
      ],
      "metadata": {
        "id": "-XfDQMBomcZJ"
      },
      "id": "-XfDQMBomcZJ",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "## 9. Visualizando Distribui√ß√£o de Probabilidades"
       ],
       "metadata": {
        "id": "27eintm_mecu"
       },
       "id": "27eintm_mecu"
      },    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def show_next_token_distribution(model, prompt):\n",
        "\n",
        "    tokens = encode(prompt)\n",
        "    tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    logits, _ = model(tokens)\n",
        "    probs = torch.softmax(logits[0, -1], dim=-1).cpu()\n",
        "\n",
        "    plt.bar(range(len(probs)), probs)\n",
        "    plt.title(\"Distribui√ß√£o do Pr√≥ximo Token\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "gH4LrcGXmiUp"
      },
      "id": "gH4LrcGXmiUp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_next_token_distribution(model, \"Explique IA:\")\n"
      ],
      "metadata": {
        "id": "4JbveCWXmk4q"
      },
      "id": "4JbveCWXmk4q",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "## 10. Reflex√µes Finais\n",
        "\n",
        "Este notebook demonstrou que:\n",
        "\n",
        "‚Ä¢ Modelos did√°ticos possuem limita√ß√µes claras  \n",
        "‚Ä¢ Compreender fundamentos permite interpretar comportamento  \n",
        "‚Ä¢ Pequenas mudan√ßas em dados e prompts impactam resultados  \n",
        "‚Ä¢ LLMs s√£o sistemas probabil√≠sticos, n√£o sistemas conscientes  \n",
        "\n",
        "Este laborat√≥rio marca o encerramento da jornada pr√°tica da s√©rie.\n"
       ],
       "metadata": {
        "id": "MvxAWfy9mlku"
       },
       "id": "MvxAWfy9mlku"
      }  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}