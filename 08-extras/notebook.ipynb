{"cells": [{"cell_type": "markdown", "source": ["# Cap√≠tulo 08 ‚Äî Explora√ß√µes Extras e Reflex√µes\n", "\n", "Este notebook acompanha o Cap√≠tulo 08 da s√©rie **Fazendo um LLM do Zero**.\n", "Ele √© um laborat√≥rio aberto para experimentar limites, comparar estrat√©gias e refletir sobre o comportamento do modelo.\n", "\n", "üéØ **Objetivos deste notebook:**\n", "- Limita√ß√µes dos modelos did√°ticos\n", "- Impacto do tamanho do dataset\n", "- Compara√ß√µes entre estrat√©gias\n", "- Gera√ß√£o multi-turno\n", "- Experimentos livres\n", "\n", "**Nota importante:** este modelo usa um vocabul√°rio e dataset pequenos.\n", "Entradas fora do vocabul√°rio s√£o normalizadas (acentos removidos e caracteres desconhecidos viram espa√ßo).\n", "Isso pode simplificar a sa√≠da ‚Äî esse efeito √© esperado e faz parte do experimento.\n", "\n", "![Teoria vs Pr√°tica](./infograficos/02-teoria-vs-pratica.png)\n", "\n", "---\n", "\n", "![Caminhos Futuros](./infograficos/01-caminhos-futuros.png)\n", "\n", "---\n"], "metadata": {"id": "3mzhlzIZlXRr"}, "id": "3mzhlzIZlXRr"}, {"cell_type": "markdown", "source": ["## 1. Setup e Configura√ß√£o"], "metadata": {"id": "setup-header"}, "id": "setup-header"}, {"cell_type": "code", "source": ["# ============================================================\n", "# Setup do reposit√≥rio\n", "# ============================================================\n", "import os\n", "\n", "REPO_URL = \"https://github.com/vongrossi/fazendo-um-llm-do-zero.git\"\n", "REPO_DIR = \"fazendo-um-llm-do-zero\"\n", "\n", "if not os.path.exists(REPO_DIR):\n", "    !git clone {REPO_URL}\n", "\n", "os.chdir(REPO_DIR)\n", "print(\"Diret√≥rio atual:\", os.getcwd())\n"], "metadata": {"id": "fouB7NoRlUfp"}, "id": "fouB7NoRlUfp", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### 1.1 Depend√™ncias e Imports"], "metadata": {"id": "gjXJfafGlcQi"}, "id": "gjXJfafGlcQi"}, {"cell_type": "code", "source": ["!pip -q install -r 08-extras/requirements.txt\n", "\n", "import torch\n", "import random\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import sys\n", "\n", "# Adiciona raiz ao path\n", "sys.path.append(os.getcwd())\n", "\n", "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "print(\"Device:\", device)\n"], "metadata": {"id": "NbO_mduLlc5b"}, "id": "NbO_mduLlc5b", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Carregar Modelo Instruction Tuned"], "metadata": {"id": "Wc2XG9_Ilgga"}, "id": "Wc2XG9_Ilgga"}, {"cell_type": "code", "source": ["from lib.gptmini import GPTConfig, GPTMini\n", "\n", "# ============================================================\n", "# Carregando o modelo treinado (base para os experimentos)\n", "# ============================================================\n", "ckpt_path = None\n", "for p in [\"07_instruction_gpt.pt\", \"gpt_checkpoint.pt\",\n", "          \"../07-instruction-tuning/07_instruction_gpt.pt\", \"../gpt_checkpoint.pt\"]:\n", "    if os.path.exists(p):\n", "        ckpt_path = p\n", "        break\n", "\n", "if ckpt_path is None:\n", "    from google.colab import files\n", "    print(\"üì§ Checkpoint n√£o encontrado. Fa√ßa upload do gpt_checkpoint.pt:\")\n", "    uploaded = files.upload()\n", "    ckpt_path = next(iter(uploaded.keys()))\n", "\n", "ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n", "stoi, itos = ckpt['stoi'], ckpt['itos']\n", "config = ckpt['config']\n", "\n", "model = GPTMini(config).to(device)\n", "model.load_state_dict(ckpt['state_dict'])\n", "print(f\"Modelo carregado ‚úÖ | vocab={len(stoi)} | context={config.context_size}\")\n"], "metadata": {"id": "J21OPWg0lhPO"}, "id": "J21OPWg0lhPO", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### 2.1 Fun√ß√µes de Tokeniza√ß√£o"], "metadata": {"id": "ZVBRl9VWlk9T"}, "id": "ZVBRl9VWlk9T"}, {"cell_type": "code", "source": ["import unicodedata\n", "\n", "def normalize_text(text):\n", "    text = text.lower()\n", "    text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n", "    return text\n", "\n", "def sanitize_text(text):\n", "    text = normalize_text(text)\n", "    unk = []\n", "    out = []\n", "    for c in text:\n", "        if c in stoi:\n", "            out.append(c)\n", "        else:\n", "            out.append(' ')\n", "            unk.append(c)\n", "    # colapsa espacos\n", "    s = ' '.join(''.join(out).split())\n", "    return s, sorted(set(unk))\n", "\n", "def encode(text):\n", "    text, _ = sanitize_text(text)\n", "    unk_id = stoi.get(' ', 0)\n", "    return [stoi.get(c, unk_id) for c in text]\n", "\n", "def decode(tokens):\n", "    return \"\".join([itos[t] for t in tokens])\n"], "metadata": {"id": "otfmFgqVllrm"}, "id": "otfmFgqVllrm", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**Nota sobre vocabul√°rio e dados**\n", "Este modelo foi treinado com um vocabul√°rio pequeno e dataset did√°tico.\n", "Entradas fora do vocabul√°rio s√£o normalizadas (acentos removidos e caracteres desconhecidos viram espa√ßo).\n", "Por isso, respostas podem parecer simplificadas ‚Äî isso √© esperado e faz parte do experimento.\n"]}, {"cell_type": "markdown", "source": ["## 3. Experimento 1 ‚Äî Impacto do Tamanho da Instru√ß√£o\n", "\n", "Aqui vamos observar como o tamanho do contexto influencia a resposta do modelo."], "metadata": {"id": "qvLJTz-Mlndg"}, "id": "qvLJTz-Mlndg"}, {"cell_type": "code", "source": ["@torch.no_grad()\n", "def generate(model, input_text, max_tokens=80, show_sanitized=False):\n", "    safe_text, unk = sanitize_text(input_text)\n", "    if show_sanitized and unk:\n", "        print(f\"‚ö†Ô∏è Caracteres fora do vocab: {unk}\")\n", "        print(f\"‚û°Ô∏è Entrada normalizada: {safe_text}\")\n", "\n", "    tokens = encode(safe_text)\n", "    tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n", "\n", "    for _ in range(max_tokens):\n", "        idx_cond = tokens[:, -config.context_size:]\n", "        logits, _ = model(idx_cond)\n", "        next_token = torch.argmax(logits[:, -1, :], dim=-1)\n", "        tokens = torch.cat([tokens, next_token.unsqueeze(1)], dim=1)\n", "        if itos[next_token.item()] == '\\n':\n", "            break\n", "\n", "    return decode(tokens.squeeze().tolist())\n"], "metadata": {"id": "WBBindBHlqHF"}, "id": "WBBindBHlqHF", "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["input_curto = \"explique ia\"\n", "input_longo = \"explique ia considerando aprendizado supervisionado e nao supervisionado\"\n", "\n", "print(generate(model, input_curto, show_sanitized=True))\n", "print(\"\\n---\\n\")\n", "print(generate(model, input_longo, show_sanitized=True))\n"], "metadata": {"id": "RvL5SiIsl0zQ"}, "id": "RvL5SiIsl0zQ", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## 4. Experimento 2 ‚Äî Limita√ß√£o de Dataset Pequeno\n", "\n", "Vamos observar comportamento fora do dom√≠nio treinado."], "metadata": {"id": "lu9B14jjl3Bw"}, "id": "lu9B14jjl3Bw"}, {"cell_type": "code", "source": ["print(generate(model, \"explique fisica quantica\"))\n"], "metadata": {"id": "nEiW_h1wl5_1"}, "id": "nEiW_h1wl5_1", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## 5. Experimento 3 ‚Äî Simulando Conversa Multi-turno\n", "\n", "Aqui simulamos uma intera√ß√£o cont√≠nua com o modelo."], "metadata": {"id": "3bJKZSSjl8Dy"}, "id": "3bJKZSSjl8Dy"}, {"cell_type": "code", "source": ["conversation = \"\"\"\n", "usuario: o que e machine learning?\n", "assistente:\n", "\"\"\"\n", "\n", "print(generate(model, conversation))\n"], "metadata": {"id": "SUVJIYWxmDej"}, "id": "SUVJIYWxmDej", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## 6. Experimento 4 ‚Äî Comparar Temperatura\n", "\n", "Criatividade vs Determinismo"], "metadata": {"id": "hHlI9dC1mHOs"}, "id": "hHlI9dC1mHOs"}, {"cell_type": "code", "source": ["def generate_temp(model, input_text, temperature=1.0, max_tokens=80):\n", "    tokens = encode(input_text)\n", "    tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n", "\n", "    for _ in range(max_tokens):\n", "        idx_cond = tokens[:, -config.context_size:]\n", "        logits, _ = model(idx_cond)\n", "        probs = torch.softmax(logits[:, -1, :] / temperature, dim=-1)\n", "        next_token = torch.multinomial(probs, 1)\n", "        tokens = torch.cat([tokens, next_token], dim=1)\n", "        if itos[next_token.item()] == '\\n':\n", "            break\n", "\n", "    return decode(tokens.squeeze().tolist())\n"], "metadata": {"id": "s3Yx5LTqmIB6"}, "id": "s3Yx5LTqmIB6", "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["print(generate_temp(model, \"explique ia\", temperature=0.3))\n", "print(\"\\n---\\n\")\n", "print(generate_temp(model, \"explique ia\", temperature=1.2))\n"], "metadata": {"id": "z2LkanY7mP7h"}, "id": "z2LkanY7mP7h", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## 7. Experimento 5 ‚Äî Demonstrando Hallucination"], "metadata": {"id": "OOaUZt_CmSa0"}, "id": "OOaUZt_CmSa0"}, {"cell_type": "code", "source": ["print(generate(model, \"quem descobriu a linguagem python em 1800\"))\n"], "metadata": {"id": "d3sZOKuVmVgJ"}, "id": "d3sZOKuVmVgJ", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### 7.1 O Que Esses Experimentos Mostram\n", "\n", "Modelos pequenos:\n", "\n", "‚Ä¢ dependem fortemente do dataset  \n", "‚Ä¢ podem produzir respostas plaus√≠veis mas incorretas  \n", "‚Ä¢ possuem limita√ß√£o de contexto  \n", "‚Ä¢ demonstram comportamento probabil√≠stico  \n", "\n", "Essas caracter√≠sticas tamb√©m existem em modelos maiores, embora com menor frequ√™ncia.\n"], "metadata": {"id": "c_az6fMjmYDw"}, "id": "c_az6fMjmYDw"}, {"cell_type": "markdown", "source": ["üß™ Experimento 6 ‚Äî Engenharia de Instru√ß√£o"], "metadata": {"id": "7DrngsDpmbq-"}, "id": "7DrngsDpmbq-"}, {"cell_type": "code", "source": ["print(generate(model, \"responda como professor explicando ia para iniciantes\"))\n"], "metadata": {"id": "-XfDQMBomcZJ"}, "id": "-XfDQMBomcZJ", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## 9. Visualizando Distribui√ß√£o de Probabilidades"], "metadata": {"id": "27eintm_mecu"}, "id": "27eintm_mecu"}, {"cell_type": "code", "source": ["show_next_token_distribution(model, \"explique ia\")\n"], "metadata": {"id": "gH4LrcGXmiUp"}, "id": "gH4LrcGXmiUp", "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["show_next_token_distribution(model, \"Explique IA:\")\n"], "metadata": {"id": "4JbveCWXmk4q"}, "id": "4JbveCWXmk4q", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## 10. Reflex√µes Finais\n", "\n", "Este notebook demonstrou que:\n", "\n", "‚Ä¢ Modelos did√°ticos possuem limita√ß√µes claras  \n", "‚Ä¢ Compreender fundamentos permite interpretar comportamento  \n", "‚Ä¢ Pequenas mudan√ßas em dados e instru√ß√µes impactam resultados  \n", "‚Ä¢ LLMs s√£o sistemas probabil√≠sticos, n√£o sistemas conscientes  \n", "\n", "Este laborat√≥rio marca o encerramento da jornada pr√°tica da s√©rie.\n"], "metadata": {"id": "MvxAWfy9mlku"}, "id": "MvxAWfy9mlku"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.12.3"}, "colab": {"provenance": []}}, "nbformat": 4, "nbformat_minor": 5}