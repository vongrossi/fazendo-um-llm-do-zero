{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cap√≠tulo 06 ‚Äî Fine-Tuning: A Especializa√ß√£o do Modelo\n",
    "\n",
    "No cap√≠tulo anterior, criamos um modelo que sabe falar. Agora, vamos ensinar esse modelo a ter um **prop√≥sito**. Vamos transform√°-lo em um especialista em detectar mensagens indesejadas (Spam).\n",
    "\n",
    "--- \n",
    "### üéØ O Poder da Adapta√ß√£o\n",
    "O Fine-tuning pega o conhecimento geral do Backbone e o canaliza para uma tarefa espec√≠fica. √â a diferen√ßa entre um estudante de medicina e um neurocirurgi√£o.\n",
    "\n",
    "![Pretrain vs Finetune](./infograficos/01-pretrain-vs-finetune.png)"
   ],
   "metadata": { "id": "header" }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setup e Carregamento do C√©rebro Base\n",
    "\n",
    "Para este cap√≠tulo, precisamos dos pesos que salvamos no Cap√≠tulo 05 (`gpt_checkpoint.pt`)."
   ],
   "metadata": { "id": "setup-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lib.gptmini import GPTConfig, GPTMini\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚úÖ Especialista pronto em: {device}\")"
   ],
   "metadata": { "id": "setup-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Preparando os Dados de Miss√£o\n",
    "\n",
    "Nosso dataset agora √© supervisionado: cada texto tem uma etiqueta (0 = Normal, 1 = Spam)."
   ],
   "metadata": { "id": "data-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "spam_data = [\n",
    "    (\"ganhe 1 milhao agora clique aqui\", 1),\n",
    "    (\"oferta imperdivel ganhe desconto gratis\", 1),\n",
    "    (\"seu premio esta esperando resgate ja\", 1),\n",
    "    (\"ola tudo bem como voce esta\", 0),\n",
    "    (\"reuniao de equipe amanha as dez\", 0),\n",
    "    (\"voce vai no churrasco no domingo\", 0)\n",
    "]\n",
    "\n",
    "# Simples tokeniza√ß√£o por palavras para manter o foco no fine-tuning\n",
    "vocab = sorted(set(\" \".join([t for t, l in spam_data]).split()))\n",
    "stoi = {t:i+1 for i,t in enumerate(vocab)}\n",
    "stoi['<pad>'] = 0\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "def encode(text, max_len=10):\n",
    "    ids = [stoi.get(t, 0) for t in text.split()]\n",
    "    return ids + [0]*(max_len - len(ids))\n",
    "\n",
    "X = torch.tensor([encode(t) for t, l in spam_data]).to(device)\n",
    "Y = torch.tensor([l for t, l in spam_data]).to(device)\n",
    "\n",
    "print(f\"Dataset pronto: {len(X)} exemplos de treinamento.\")"
   ],
   "metadata": { "id": "data-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Cirurgia de Arquitetura: A Cabe√ßa de Classifica√ß√£o\n",
    "\n",
    "No treinamento original, a sa√≠da era do tamanho do vocabul√°rio. Agora, vamos reduzir essa sa√≠da para apenas **duas classes**.\n",
    "\n",
    "![Classification Head](./infograficos/02-classification-head.png)"
   ],
   "metadata": { "id": "model-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "class GPTClassifier(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        # Trocamos a cabe√ßa de sa√≠da por uma linear de classifica√ß√£o\n",
    "        self.clf_head = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Passamos pelo backbone Transformer\n",
    "        # Usamos apenas o √∫ltimo estado oculto (Pooling) para a decis√£o\n",
    "        x = self.backbone.emb(x)\n",
    "        x = self.backbone.blocks(x)\n",
    "        x = self.backbone.ln_f(x)\n",
    "        \n",
    "        last_token = x[:, -1, :]\n",
    "        logits = self.clf_head(last_token)\n",
    "        return logits\n",
    "\n",
    "config = GPTConfig(vocab_size=vocab_size, context_size=10, d_model=64, n_heads=4, n_layers=2)\n",
    "backbone = GPTMini(config).to(device)\n",
    "model = GPTClassifier(backbone).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ],
   "metadata": { "id": "model-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Estrat√©gia: Freeze vs Unfreeze\n",
    "\n",
    "Podemos treinar o modelo todo ou congelar a base e treinar apenas a pe√ßa nova.\n",
    "\n",
    "![Freeze Unfreeze](./infograficos/03-freeze-vs-unfreeze.png)"
   ],
   "metadata": { "id": "strategy-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "def set_freeze(model, freeze=True):\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = not freeze\n",
    "    print(f\"‚ùÑÔ∏è Backbone {'Congelado' if freeze else 'Aberto'} para treino.\")\n",
    "\n",
    "set_freeze(model, freeze=True)"
   ],
   "metadata": { "id": "strategy-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Treinamento da Miss√£o\n",
    "\n",
    "Diferente do pr√©-treino, aqui usamos a Loss de Classifica√ß√£o para reduzir o erro entre a predi√ß√£o bin√°ria e a etiqueta real."
   ],
   "metadata": { "id": "train-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "loss_history = []\n",
    "model.train()\n",
    "for step in range(301):\n",
    "    logits = model(X)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    if step % 100 == 0: print(f\"Step {step:03d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(loss_history, color='#34A853')\n",
    "plt.title(\"Curva de Especializa√ß√£o (Fine-tuning Loss)\")\n",
    "plt.show()"
   ],
   "metadata": { "id": "train-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Avalia√ß√£o de Sucesso\n",
    "\n",
    "Como saber se o nosso filtro de Spam √© confi√°vel? Vamos testar com frases que o modelo nunca viu."
   ],
   "metadata": { "id": "eval-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "def classify(text):\n",
    "    model.eval()\n",
    "    ids = torch.tensor([encode(text)]).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(ids)\n",
    "        pred = torch.argmax(logits, dim=-1).item()\n",
    "    return \"üö® SPAM\" if pred == 1 else \"‚úÖ NORMAL\"\n",
    "\n",
    "print(\"üîç Testando o Especialista:\")\n",
    "print(f\"'ganhe agora desconto': {classify('ganhe agora desconto')}\")\n",
    "print(f\"'ola tudo bem amigo': {classify('ola tudo bem amigo')}\")"
   ],
   "metadata": { "id": "eval-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üèÅ Conclus√£o\n",
    "\n",
    "Neste cap√≠tulo, voc√™ viu como transformar um c√©rebro digital gen√©rico em uma ferramenta √∫til de seguran√ßa. \n",
    "\n",
    "O Fine-tuning √© o que permite que modelos de linguagem sejam aplicados em trilhares de tarefas diferentes no mundo real.\n",
    "\n",
    "![Confusion Matrix](./infograficos/05-metricas-confusion-matrix.png)"
   ],
   "metadata": { "id": "footer" }
  }
 ]\n,
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.12.3" }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}