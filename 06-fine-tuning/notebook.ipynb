{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bca629b3",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 06 ‚Äî Fine-tuning para Classifica√ß√£o (GPT vira ‚Äúproduto‚Äù)\n",
    "\n",
    "Este notebook acompanha o Cap√≠tulo 06 da s√©rie **Fazendo um LLM do Zero**.\n",
    "\n",
    "Neste notebook n√≥s vamos **reaproveitar o GPTMini** (que foi pr√©-treinado/treinado no Cap√≠tulo 05) e adapt√°-lo para uma tarefa supervisionada de classifica√ß√£o.\n",
    "\n",
    "üéØ **Objetivos deste notebook:**\n",
    "- Como carregar pesos do Cap√≠tulo 05 (backbone pr√©-treinado)\n",
    "- Como adicionar uma **classification head**\n",
    "- Como comparar **pooling**: *last-token* vs *mean pooling*\n",
    "- Como comparar **estrat√©gias**: *freeze* (s√≥ head) vs *unfreeze* (fine-tuning completo)\n",
    "- Como ajustar **learning rates diferentes** para cada estrat√©gia (boa pr√°tica)\n",
    "- Como avaliar com m√©tricas (accuracy, precision, recall, F1 e confusion matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setup e Configura√ß√£o"
   ],
   "metadata": {
    "id": "setup-header"
   },
   "id": "setup-header"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b34cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Setup do reposit√≥rio\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/vongrossi/fazendo-um-llm-do-zero.git\"\n",
    "REPO_DIR = \"fazendo-um-llm-do-zero\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL}\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(\"Diret√≥rio atual:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Depend√™ncias e Imports"
   ],
   "metadata": {
    "id": "deps-header"
   },
   "id": "deps-header"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b7215",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -r 06-fine-tuning/requirements.txt\n",
    "\n",
    "# Depend√™ncias e GPU opcional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# Adiciona raiz ao path para imports locais\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698c1db",
   "metadata": {},
   "source": [
    "## 2. Importando o Backbone (GPTMini)\n",
    "\n",
    "Premissa da s√©rie: **n√£o copiar c√≥digo entre notebooks**.\n",
    "\n",
    "N√≥s reutilizamos o n√∫cleo do modelo a partir do m√≥dulo:\n",
    "\n",
    "`lib/gptmini.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1791ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.gptmini import GPTConfig, GPTMini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255cf64b",
   "metadata": {},
   "source": [
    "## 3. Carregando Pesos do Cap√≠tulo 05\n",
    "\n",
    "A ideia √© come√ßar este cap√≠tulo com um modelo que **j√° aprendeu padr√µes gerais de linguagem** no Cap√≠tulo 05.\n",
    "\n",
    "Como checkpoints podem ter nomes diferentes, vamos procurar automaticamente por alguns candidatos comuns no reposit√≥rio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb8bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "\n",
    "# Ajuste aqui se voc√™ quiser apontar para um arquivo espec√≠fico\n",
    "CHECKPOINT_CANDIDATES = [\n",
    "    \"05-pre-treinamento/gpt_checkpoint.pt\",\n",
    "    \"05-pre-treinamento/gpt_checkpoint_full.pt\",\n",
    "    \"05-pre-treinamento/05_gpt_checkpoint.pt\",\n",
    "    \"05-pre-treinamento/05_pretrain_checkpoint.pt\",\n",
    "    \"gpt_checkpoint.pt\",\n",
    "]\n",
    "\n",
    "def find_checkpoint(candidates):\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    # fallback: busca por qualquer .pt dentro do cap 05\n",
    "    pts = glob.glob(\"05-pre-treinamento/**/*.pt\", recursive=True)\n",
    "    return pts[0] if pts else None\n",
    "\n",
    "ckpt_path = find_checkpoint(CHECKPOINT_CANDIDATES)\n",
    "print(\"Checkpoint encontrado:\", ckpt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd6e566",
   "metadata": {},
   "source": [
    "### 3.1 Estrat√©gia de Vocabul√°rio\n",
    "\n",
    "Um checkpoint de LLM geralmente depende do **tamanho do vocabul√°rio** (embedding de tokens) e do `context_size`.\n",
    "\n",
    "- Se o checkpoint trouxer `config` e `stoi/itos`, n√≥s reaproveitamos.\n",
    "- Se ele n√£o trouxer, ainda d√° para reaproveitar **parte** dos pesos (por exemplo, blocos Transformer) quando as dimens√µes baterem.\n",
    "\n",
    "No nosso caso, vamos tentar o melhor caminho primeiro: **reusar o tokenizer do checkpoint**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9321af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path):\n",
    "    if path is None:\n",
    "        return None\n",
    "    obj = torch.load(path, map_location=\"cpu\")\n",
    "    # pode ser state_dict puro ou dict com metadados\n",
    "    if isinstance(obj, dict) and \"state_dict\" in obj:\n",
    "        return obj\n",
    "    if isinstance(obj, dict):\n",
    "        # heur√≠stica: parece um state_dict\n",
    "        return {\"state_dict\": obj}\n",
    "    raise ValueError(\"Formato de checkpoint n√£o reconhecido\")\n",
    "\n",
    "ckpt = load_checkpoint(ckpt_path)\n",
    "print(\"Tem checkpoint?\", ckpt is not None)\n",
    "print(\"Chaves do checkpoint:\", list(ckpt.keys())[:10] if ckpt else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ba0ba",
   "metadata": {},
   "source": [
    "## 4. Dataset Supervisionado\n",
    "\n",
    "Para um fine-tuning fazer sentido, precisamos de um dataset rotulado ‚Äúde verdade‚Äù.\n",
    "\n",
    "Vamos usar o **SMS Spam Collection** (dataset cl√°ssico, leve e √≥timo para Colab):\n",
    "- r√≥tulos: `spam` vs `ham`\n",
    "- tamanho ‚Äúm√©dio‚Äù (alguns milhares de exemplos)\n",
    "- download simples\n",
    "\n",
    "Se o download falhar, o notebook cai para um dataset toy (bem pequeno), s√≥ para demonstrar o pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "def load_sms_spam_dataset():\n",
    "    # UCI SMS Spam Collection (zip)\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "    try:\n",
    "        with urllib.request.urlopen(url, timeout=30) as resp:\n",
    "            data = resp.read()\n",
    "        z = zipfile.ZipFile(io.BytesIO(data))\n",
    "        raw = z.read(\"SMSSpamCollection\").decode(\"utf-8\", errors=\"replace\")\n",
    "        rows = []\n",
    "        for line in raw.splitlines():\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            label, text = line.split(\"\\t\", 1)\n",
    "            y = 1 if label.strip().lower() == \"spam\" else 0\n",
    "            rows.append((text.strip(), y))\n",
    "        return rows\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Falha ao baixar dataset SMS Spam Collection. Erro:\", e)\n",
    "        return None\n",
    "\n",
    "data = load_sms_spam_dataset()\n",
    "\n",
    "if data is None:\n",
    "    # fallback toy (para n√£o quebrar o notebook)\n",
    "    data = [\n",
    "        (\"ganhe dinheiro r√°pido clique aqui\", 1),\n",
    "        (\"promo√ß√£o imperd√≠vel compre agora\", 1),\n",
    "        (\"voc√™ foi selecionado para um pr√™mio\", 1),\n",
    "        (\"clique no link e resgate seu b√¥nus\", 1),\n",
    "        (\"oferta limitada aproveite j√°\", 1),\n",
    "        (\"oi tudo bem vamos marcar amanh√£\", 0),\n",
    "        (\"segue o relat√≥rio do projeto\", 0),\n",
    "        (\"podemos alinhar a reuni√£o √†s 10h\", 0),\n",
    "        (\"me chama quando puder\", 0),\n",
    "        (\"confirmado, vou te enviar ainda hoje\", 0),\n",
    "    ]\n",
    "\n",
    "random.shuffle(data)\n",
    "print(\"Total exemplos:\", len(data))\n",
    "print(\"Exemplo:\", data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f5b4cb",
   "metadata": {},
   "source": [
    "## 5. Tokeniza√ß√£o e Vocabul√°rio\n",
    "\n",
    "Para conseguir **carregar pesos** do Cap√≠tulo 05 de forma mais fiel, vamos tentar reutilizar:\n",
    "\n",
    "- `stoi` / `itos` do checkpoint (se existirem)\n",
    "- `context_size` do checkpoint (se existir)\n",
    "\n",
    "Caso n√£o exista, constru√≠mos um tokenizer simples por palavras.\n",
    "\n",
    "> Observa√ß√£o did√°tica: aqui n√£o estamos usando BPE/WordPiece.  \n",
    "> A ideia √© entender o *fine-tuning* e as decis√µes do pipeline, n√£o otimizar tokeniza√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "def simple_tokenize(text: str):\n",
    "    # tokeniza√ß√£o word-level did√°tica\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "def build_vocab_from_texts(texts, add_pad_unk=True):\n",
    "    toks = []\n",
    "    for t in texts:\n",
    "        toks.extend(simple_tokenize(t))\n",
    "    vocab = sorted(set(toks))\n",
    "    stoi = {}\n",
    "    if add_pad_unk:\n",
    "        stoi[PAD] = 0\n",
    "        stoi[UNK] = 1\n",
    "        offset = 2\n",
    "    else:\n",
    "        offset = 0\n",
    "    for i, tok in enumerate(vocab):\n",
    "        stoi[tok] = i + offset\n",
    "    itos = {i:t for t,i in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "texts = [t for t,_ in data]\n",
    "labels = [y for _,y in data]\n",
    "\n",
    "# tenta usar tokenizer do checkpoint\n",
    "ckpt_stoi = ckpt.get(\"stoi\") if ckpt else None\n",
    "ckpt_itos = ckpt.get(\"itos\") if ckpt else None\n",
    "ckpt_ctx = ckpt.get(\"context_size\") if ckpt else None\n",
    "ckpt_cfg = ckpt.get(\"config\") if ckpt else None\n",
    "\n",
    "if ckpt_stoi and ckpt_itos:\n",
    "    stoi = ckpt_stoi\n",
    "    itos = ckpt_itos\n",
    "    print(\"‚úÖ Reutilizando stoi/itos do checkpoint (cap 05)\")\n",
    "else:\n",
    "    stoi, itos = build_vocab_from_texts(texts, add_pad_unk=True)\n",
    "    print(\"‚ö†Ô∏è Criando stoi/itos novo (word-level)\")\n",
    "\n",
    "# garante PAD/UNK mesmo se vier do checkpoint\n",
    "if PAD not in stoi:\n",
    "    stoi = {PAD:0, **{k:(v+1) for k,v in stoi.items()}}\n",
    "if UNK not in stoi:\n",
    "    # coloca UNK como 1 e desloca o resto se necess√°rio\n",
    "    if 1 in stoi.values():\n",
    "        # se j√° tem algo em 1, remapeia tudo preservando ordem\n",
    "        items = sorted(stoi.items(), key=lambda kv: kv[1])\n",
    "        new = {}\n",
    "        new[PAD] = 0\n",
    "        new[UNK] = 1\n",
    "        cur = 2\n",
    "        for tok,_id in items:\n",
    "            if tok in (PAD, UNK):\n",
    "                continue\n",
    "            new[tok] = cur\n",
    "            cur += 1\n",
    "        stoi = new\n",
    "    else:\n",
    "        stoi[UNK] = 1\n",
    "\n",
    "itos = {i:t for t,i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "# context size: do checkpoint se houver; sen√£o padr√£o\n",
    "context_size = int(ckpt_ctx) if ckpt_ctx else 64\n",
    "\n",
    "pad_id = stoi[PAD]\n",
    "unk_id = stoi[UNK]\n",
    "\n",
    "print(\"vocab_size:\", vocab_size, \"| context_size:\", context_size, \"| pad_id:\", pad_id, \"| unk_id:\", unk_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b8f00",
   "metadata": {},
   "source": [
    "### 5.1 Encoding com Padding/Truncation\n",
    "\n",
    "Transformamos textos em sequ√™ncias de tamanho fixo (`context_size`) com:\n",
    "\n",
    "- truncation (corta excesso)\n",
    "- padding (com `<pad>`)\n",
    "- OOV cai em `<unk>`\n",
    "\n",
    "Isso √© suficiente para um cap√≠tulo did√°tico de fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceaedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str, context_size: int):\n",
    "    toks = simple_tokenize(text)\n",
    "    ids = [stoi.get(tok, unk_id) for tok in toks]\n",
    "    ids = ids[:context_size]\n",
    "    if len(ids) < context_size:\n",
    "        ids = ids + [pad_id] * (context_size - len(ids))\n",
    "    return ids\n",
    "\n",
    "X = torch.tensor([encode(t, context_size) for t in texts], dtype=torch.long)\n",
    "Y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "print(\"X shape:\", X.shape, \"Y shape:\", Y.shape)\n",
    "print(\"Exemplo tokens:\", texts[0])\n",
    "print(\"Exemplo ids[:20]:\", X[0][:20].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe58b6de",
   "metadata": {},
   "source": [
    "### 5.2 Split Treino/Val\n",
    "\n",
    "Usaremos uma separa√ß√£o simples 80/20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf676c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X)\n",
    "perm = torch.randperm(n)\n",
    "train_size = int(0.8 * n)\n",
    "\n",
    "train_idx = perm[:train_size]\n",
    "val_idx = perm[train_size:]\n",
    "\n",
    "X_train, Y_train = X[train_idx].to(device), Y[train_idx].to(device)\n",
    "X_val, Y_val = X[val_idx].to(device), Y[val_idx].to(device)\n",
    "\n",
    "print(\"Train:\", X_train.shape[0], \"Val:\", X_val.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb06c7",
   "metadata": {},
   "source": [
    "## 6. Backbone Pr√©-treinado + Cabe√ßa de Classifica√ß√£o\n",
    "\n",
    "Vamos instanciar o **GPTMini** e tentar carregar os pesos do Cap√≠tulo 05.\n",
    "\n",
    "Ponto importante:\n",
    "\n",
    "- se `vocab_size` e `context_size` n√£o baterem com o checkpoint, embeddings podem n√£o encaixar\n",
    "- mesmo assim, podemos reaproveitar blocos Transformer quando as dimens√µes coincidirem\n",
    "\n",
    "A ideia: **aproveitar o m√°ximo poss√≠vel sem quebrar o notebook**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a3a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config: se checkpoint trouxe config, usa como base (mas ajusta vocab/context se necess√°rio)\n",
    "if isinstance(ckpt_cfg, dict):\n",
    "    cfg_dict = dict(ckpt_cfg)\n",
    "    cfg_dict[\"vocab_size\"] = vocab_size\n",
    "    cfg_dict[\"context_size\"] = context_size\n",
    "    config = GPTConfig(**cfg_dict)\n",
    "else:\n",
    "    config = GPTConfig(\n",
    "        vocab_size=vocab_size,\n",
    "        context_size=context_size,\n",
    "        d_model=128,\n",
    "        n_heads=4,\n",
    "        n_layers=4,\n",
    "        dropout=0.1,\n",
    "    )\n",
    "\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda069c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = GPTMini(config).to(device)\n",
    "\n",
    "def load_pretrained_into_backbone(backbone, ckpt):\n",
    "    if not ckpt or \"state_dict\" not in ckpt:\n",
    "        print(\"‚ö†Ô∏è Sem checkpoint de pesos para carregar.\")\n",
    "        return\n",
    "\n",
    "    sd = ckpt[\"state_dict\"]\n",
    "    model_sd = backbone.state_dict()\n",
    "\n",
    "    filtered = {}\n",
    "    for k, v in sd.items():\n",
    "        if k in model_sd and tuple(model_sd[k].shape) == tuple(v.shape):\n",
    "            filtered[k] = v\n",
    "\n",
    "    backbone.load_state_dict(filtered, strict=False)\n",
    "    pct = 100.0 * len(filtered) / max(1, len(model_sd))\n",
    "    print(f\"‚úÖ Pesos carregados (shape-match): {len(filtered)}/{len(model_sd)} ({pct:.1f}%)\")\n",
    "    # dicas √∫teis\n",
    "    if len(filtered) < len(model_sd):\n",
    "        print(\"‚ÑπÔ∏è Nem todos os pesos encaixaram (normal se vocab/context mudarem).\")\n",
    "\n",
    "load_pretrained_into_backbone(backbone, ckpt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768dcbd3",
   "metadata": {},
   "source": [
    "### 6.1 Features do Backbone + Pooling\n",
    "\n",
    "Para classifica√ß√£o, precisamos de uma representa√ß√£o ‚Äúdo texto todo‚Äù.\n",
    "\n",
    "Vamos comparar duas estrat√©gias:\n",
    "\n",
    "1) **Last-token pooling**: usa o vetor do √∫ltimo token da sequ√™ncia  \n",
    "2) **Mean pooling**: m√©dia dos vetores dos tokens **n√£o-PAD**\n",
    "\n",
    "Mean pooling costuma ser mais robusto quando a sequ√™ncia tem muitos PADs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTMiniFeatures(nn.Module):\n",
    "    def __init__(self, gptmini: GPTMini):\n",
    "        super().__init__()\n",
    "        self.gpt = gptmini\n",
    "\n",
    "    def forward(self, idx):\n",
    "        x = self.gpt.emb(idx)\n",
    "        x = self.gpt.blocks(x)\n",
    "        x = self.gpt.ln_f(x)\n",
    "        return x  # (B, T, C)\n",
    "\n",
    "def mean_pool(feats, idx, pad_id):\n",
    "    # feats: (B,T,C), idx: (B,T)\n",
    "    mask = (idx != pad_id).float().unsqueeze(-1)  # (B,T,1)\n",
    "    summed = (feats * mask).sum(dim=1)            # (B,C)\n",
    "    denom = mask.sum(dim=1).clamp(min=1.0)        # (B,1)\n",
    "    return summed / denom\n",
    "\n",
    "class GPTClassifier(nn.Module):\n",
    "    def __init__(self, gpt_features: GPTMiniFeatures, d_model: int, num_classes=2, pooling=\"last\"):\n",
    "        super().__init__()\n",
    "        assert pooling in (\"last\", \"mean\")\n",
    "        self.gpt_features = gpt_features\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def forward(self, idx, labels=None):\n",
    "        feats = self.gpt_features(idx)  # (B,T,C)\n",
    "\n",
    "        if self.pooling == \"last\":\n",
    "            pooled = feats[:, -1, :]\n",
    "        else:\n",
    "            pooled = mean_pool(feats, idx, pad_id)\n",
    "\n",
    "        logits = self.classifier(pooled)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6d246",
   "metadata": {},
   "source": [
    "## 7. M√©tricas (Sem Sklearn)\n",
    "\n",
    "Vamos implementar m√©tricas b√°sicas de classifica√ß√£o bin√°ria:\n",
    "\n",
    "- accuracy\n",
    "- precision\n",
    "- recall\n",
    "- F1\n",
    "- confusion matrix\n",
    "\n",
    "Isso ajuda a ‚Äúler o modelo‚Äù para al√©m de simplesmente ‚Äúacertou/errou‚Äù.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb63e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def confusion_matrix_binary(y_true, y_pred):\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum().item())\n",
    "    tn = int(((y_true == 0) & (y_pred == 0)).sum().item())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum().item())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum().item())\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "@torch.no_grad()\n",
    "def metrics_from_confusion(tp, fp, fn, tn):\n",
    "    acc = (tp + tn) / max(1, (tp + tn + fp + fn))\n",
    "    prec = tp / max(1, (tp + fp))\n",
    "    rec = tp / max(1, (tp + fn))\n",
    "    f1 = (2 * prec * rec) / max(1e-12, (prec + rec))\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, X, Y):\n",
    "    model.eval()\n",
    "    logits, _ = model(X, labels=None)\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    tp, fp, fn, tn = confusion_matrix_binary(Y, preds)\n",
    "    acc, prec, rec, f1 = metrics_from_confusion(tp, fp, fn, tn)\n",
    "\n",
    "    return {\"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1, \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc867ca",
   "metadata": {},
   "source": [
    "## 8. Experimentos\n",
    "\n",
    "Vamos rodar **4 experimentos** para comparar:\n",
    "\n",
    "- Pooling: `last` vs `mean`\n",
    "- Estrat√©gia: `freeze` vs `unfreeze`\n",
    "\n",
    "E vamos usar **learning rates diferentes** (boa pr√°tica):\n",
    "\n",
    "- Freeze (s√≥ head): LR maior (ex.: 2e-3)  \n",
    "- Unfreeze (modelo todo): LR menor (ex.: 5e-4)  \n",
    "\n",
    "Motivo: quando muitos par√¢metros est√£o trein√°veis, LR alto pode destruir rapidamente o conhecimento pr√©vio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a2b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_trainable(module: nn.Module, trainable: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = trainable\n",
    "\n",
    "def train_classifier(model, X_train, Y_train, X_val, Y_val, steps=400, batch_size=32, lr=1e-3, eval_every=100):\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    train_loss_hist = []\n",
    "    val_hist = []\n",
    "\n",
    "    model.train()\n",
    "    for step in range(steps):\n",
    "        idx = torch.randint(0, X_train.size(0), (batch_size,), device=device)\n",
    "        xb = X_train[idx]\n",
    "        yb = Y_train[idx]\n",
    "\n",
    "        _, loss = model(xb, labels=yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_hist.append(loss.item())\n",
    "\n",
    "        if step % eval_every == 0:\n",
    "            stats = evaluate(model, X_val, Y_val)\n",
    "            val_hist.append((step, stats))\n",
    "            print(f\"step {step:04d} | loss {loss.item():.4f} | val_acc {stats['acc']:.3f} | f1 {stats['f1']:.3f}\")\n",
    "\n",
    "    return train_loss_hist, val_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(pooling: str, strategy: str, lr: float, steps=400):\n",
    "    seed_everything(42)\n",
    "\n",
    "    # Novo backbone para cada experimento (mesmo ponto de partida)\n",
    "    bb = GPTMini(config).to(device)\n",
    "    load_pretrained_into_backbone(bb, ckpt)\n",
    "\n",
    "    feats = GPTMiniFeatures(bb).to(device)\n",
    "    clf = GPTClassifier(feats, d_model=config.d_model, num_classes=2, pooling=pooling).to(device)\n",
    "\n",
    "    if strategy == \"freeze\":\n",
    "        set_trainable(clf.gpt_features, False)\n",
    "        set_trainable(clf.classifier, True)\n",
    "    elif strategy == \"unfreeze\":\n",
    "        set_trainable(clf.gpt_features, True)\n",
    "        set_trainable(clf.classifier, True)\n",
    "    else:\n",
    "        raise ValueError(\"strategy deve ser freeze ou unfreeze\")\n",
    "\n",
    "    trainable = sum(p.requires_grad for p in clf.parameters())\n",
    "    total = sum(1 for _ in clf.parameters())\n",
    "    print(f\"\\n=== Experimento | pooling={pooling} | strategy={strategy} | lr={lr} ===\")\n",
    "    print(f\"Par√¢metros trein√°veis: {trainable}/{total}\")\n",
    "\n",
    "    loss_hist, val_hist = train_classifier(\n",
    "        clf, X_train, Y_train, X_val, Y_val,\n",
    "        steps=steps,\n",
    "        batch_size=64 if X_train.size(0) > 256 else 16,\n",
    "        lr=lr,\n",
    "        eval_every=max(50, steps//4),\n",
    "    )\n",
    "\n",
    "    final_stats = evaluate(clf, X_val, Y_val)\n",
    "    return clf, loss_hist, val_hist, final_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4bdc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRs recomendados (did√°ticos)\n",
    "LR_FREEZE = 2e-3\n",
    "LR_UNFREEZE = 5e-4\n",
    "\n",
    "STEPS = 400 if X_train.size(0) > 200 else 300\n",
    "\n",
    "results = {}\n",
    "\n",
    "for pooling in [\"last\", \"mean\"]:\n",
    "    # freeze\n",
    "    clf_f, loss_f, val_f, stats_f = run_experiment(pooling, \"freeze\", lr=LR_FREEZE, steps=STEPS)\n",
    "    results[(pooling, \"freeze\")] = {\"model\": clf_f, \"loss\": loss_f, \"val\": val_f, \"stats\": stats_f}\n",
    "\n",
    "    # unfreeze\n",
    "    clf_u, loss_u, val_u, stats_u = run_experiment(pooling, \"unfreeze\", lr=LR_UNFREEZE, steps=STEPS)\n",
    "    results[(pooling, \"unfreeze\")] = {\"model\": clf_u, \"loss\": loss_u, \"val\": val_u, \"stats\": stats_u}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc3e0f",
   "metadata": {},
   "source": [
    "### 8.1 Comparando Resultados\n",
    "\n",
    "Vamos ver as m√©tricas finais de cada experimento e plotar as losses.\n",
    "\n",
    "Em datasets pequenos, resultados podem variar, mas a compara√ß√£o conceitual √© o mais importante:\n",
    "- *mean pooling* tende a ser melhor quando h√° muito padding\n",
    "- *freeze* tende a treinar r√°pido mas tem teto de performance\n",
    "- *unfreeze* costuma melhorar mais, mas precisa LR menor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion(stats):\n",
    "    tp, fp, fn, tn = stats[\"tp\"], stats[\"fp\"], stats[\"fn\"], stats[\"tn\"]\n",
    "    print(\"Confusion Matrix (bin√°ria)\")\n",
    "    print(f\"         Pred 0   Pred 1\")\n",
    "    print(f\"True 0 |   {tn:4d}   {fp:4d}\")\n",
    "    print(f\"True 1 |   {fn:4d}   {tp:4d}\")\n",
    "\n",
    "for key, obj in results.items():\n",
    "    pooling, strategy = key\n",
    "    stats = obj[\"stats\"]\n",
    "    print(f\"\\n=== {pooling.upper()} + {strategy.upper()} ===\")\n",
    "    print({k: round(v, 4) if isinstance(v, float) else v for k,v in stats.items()})\n",
    "    print_confusion(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc98d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot das losses\n",
    "plt.figure(figsize=(10,5))\n",
    "for key, obj in results.items():\n",
    "    pooling, strategy = key\n",
    "    plt.plot(obj[\"loss\"], label=f\"{pooling}-{strategy}\")\n",
    "plt.title(\"Training Loss ‚Äî Compara√ß√£o\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ab678",
   "metadata": {},
   "source": [
    "## 9. Infer√™ncia (Testar Textos Novos)\n",
    "\n",
    "Vamos pegar o melhor modelo (por F1 na valida√ß√£o) e testar com exemplos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca55519",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_key = max(results.keys(), key=lambda k: results[k][\"stats\"][\"f1\"])\n",
    "best_model = results[best_key][\"model\"]\n",
    "print(\"Melhor modelo (val F1):\", best_key, \"->\", results[best_key][\"stats\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b39851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(text, model):\n",
    "    model.eval()\n",
    "    x = torch.tensor([encode(text, context_size)], dtype=torch.long, device=device)\n",
    "    logits, _ = model(x, labels=None)\n",
    "    probs = F.softmax(logits, dim=-1).squeeze(0)\n",
    "    pred = int(torch.argmax(probs).item())\n",
    "    return pred, probs.detach().cpu().numpy()\n",
    "\n",
    "tests = [\n",
    "    \"Congratulations! You won a prize, click now\",\n",
    "    \"Please review the report before the meeting\",\n",
    "    \"FREE entry in a weekly draw, claim your reward\",\n",
    "    \"Can you call me tomorrow morning?\",\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    pred, probs = predict(t, best_model)\n",
    "    label = \"SPAM\" if pred == 1 else \"HAM\"\n",
    "    print(f\"\\nTexto: {t}\\nPred: {label} | probs={probs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7440c49a",
   "metadata": {},
   "source": [
    "## 10. Salvando Checkpoint do Classificador\n",
    "\n",
    "Vamos salvar:\n",
    "- pesos do classificador (backbone + head)\n",
    "- config do modelo\n",
    "- stoi/itos e context_size\n",
    "\n",
    "Isso permite abrir o notebook depois e reproduzir infer√™ncia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2623b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "ckpt_out = {\n",
    "    \"config\": config.__dict__,\n",
    "    \"state_dict\": best_model.state_dict(),\n",
    "    \"stoi\": stoi,\n",
    "    \"itos\": itos,\n",
    "    \"context_size\": context_size,\n",
    "    \"pooling\": best_key[0],\n",
    "    \"strategy\": best_key[1],\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}\n",
    "\n",
    "out_path = \"06-fine-tuning-classificacao/06_gpt_classifier.pt\"\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "torch.save(ckpt_out, out_path)\n",
    "print(\"Salvo em:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe70e0",
   "metadata": {},
   "source": [
    "## 11. Conclus√£o\n",
    "\n",
    "Voc√™ acabou de:\n",
    "\n",
    "- carregar pesos do Cap√≠tulo 05 (transfer√™ncia de conhecimento)\n",
    "- transformar um GPT em classificador (classification head)\n",
    "- comparar pooling (last vs mean)\n",
    "- comparar estrat√©gias (freeze vs unfreeze) com LRs diferentes\n",
    "- avaliar com m√©tricas e confusion matrix\n",
    "- salvar um checkpoint reproduz√≠vel\n",
    "\n",
    "Isso √© a base pr√°tica de como LLMs viram ‚Äúfeatures‚Äù para resolver problemas reais.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
