{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cap√≠tulo 06 ‚Äî Fine-Tuning: A Especializa√ß√£o do Modelo\n",
    "\n",
    "Neste cap√≠tulo, vamos realizar uma \"cirurgia neural\". Pegaremos o GPTMini que aprendeu a ler e escrever no Cap√≠tulo 05 e o ensinaremos a classificar mensagens como **Normal** ou **Spam**.\n",
    "\n",
    "--- \n",
    "### üéØ O Poder da Especializa√ß√£o\n",
    "O Fine-tuning n√£o apaga o que o modelo sabe; ele apenas direciona esse conhecimento para uma tarefa espec√≠fica.\n",
    "\n",
    "![Pretrain vs Finetune](./infograficos/01-pretrain-vs-finetune.png)"
   ],
   "metadata": { "id": "header" }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Setup do reposit√≥rio no Colab\n",
    "# ============================================================\n",
    "import os, sys\n",
    "REPO_NAME = \"fazendo-um-llm-do-zero\"\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if not os.path.exists(REPO_NAME):\n",
    "        os.system(f\"git clone https://github.com/vongrossi/{REPO_NAME}.git\")\n",
    "    if os.path.exists(REPO_NAME) and os.getcwd().split('/')[-1] != REPO_NAME:\n",
    "        os.chdir(REPO_NAME)\n",
    "if os.getcwd() not in sys.path: sys.path.append(os.getcwd())\n",
    "print(\"üìÇ Diret√≥rio atual:\", os.getcwd())"
   ],
   "metadata": { "id": "setup-repo" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 1. Setup e Conex√£o com a Intelig√™ncia Base\n",
    "# ============================================================\n",
    "import os, sys, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.gptmini import GPTConfig, GPTMini\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if not os.path.exists(\"gpt_checkpoint.pt\"):\n",
    "    from google.colab import files\n",
    "    print(\"üì§ Por favor, suba o 'gpt_checkpoint.pt' gerado no Cap√≠tulo 05:\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "try:\n",
    "    ckpt = torch.load(\"gpt_checkpoint.pt\", map_location=device, weights_only=False)\n",
    "    stoi, itos = ckpt['stoi'], ckpt['itos']\n",
    "    vocab_size = len(stoi)\n",
    "    print(f\"‚úÖ Intelig√™ncia Base Carregada! Vocabul√°rio: {vocab_size} caracteres.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRO AO CARREGAR: {e}\")"
   ],
   "metadata": { "id": "setup" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Preparando os Dados de Miss√£o\n",
    "\n",
    "Precisamos de exemplos de SPAM para que o modelo entenda o padr√£o de mensagens maliciosas."
   ],
   "metadata": { "id": "data-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "raw_data = [\n",
    "    (\"ganhe 1 milhao agora clique aqui\", 1), # Spam\n",
    "    (\"oferta imperdivel premio gratis\", 1),   # Spam\n",
    "    (\"seu premio esta esperando resgate\", 1), # Spam\n",
    "    (\"ola tudo bem como voce esta\", 0),      # Normal\n",
    "    (\"reuniao de equipe amanha as dez\", 0),   # Normal\n",
    "    (\"voce vai no churrasco no domingo\", 0)   # Normal\n",
    "]\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s.lower() if c in stoi]\n",
    "\n",
    "def build_dataset(data, max_len=32):\n",
    "    X, Y = [], []\n",
    "    for text, label in data:\n",
    "        ids = encode(text)\n",
    "        ids = ids[:max_len] + [stoi.get(' ', 0)] * (max_len - len(ids))\n",
    "        X.append(ids); Y.append(label)\n",
    "    return torch.tensor(X).to(device), torch.tensor(Y).to(device)\n",
    "\n",
    "X_train, Y_train = build_dataset(raw_data)\n",
    "print(f\"üìä Dataset Processado: {len(X_train)} exemplos prontos.\")"
   ],
   "metadata": { "id": "data-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Criando o Classificador\n",
    "\n",
    "Acoplamos a \"Cabe√ßa de Classifica√ß√£o\" ao Backbone do Transformer.\n",
    "\n",
    "![Classification Head](./infograficos/02-classification-head.png)"
   ],
   "metadata": { "id": "model-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "class GPTClassifier(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=2):\n",
    "        super().__init__(); self.backbone = backbone\n",
    "        self.clf_head = nn.Linear(backbone.config.d_model, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.emb(x)\n",
    "        x = self.backbone.blocks(x)\n",
    "        x = self.backbone.ln_f(x)\n",
    "        return self.clf_head(x[:, -1, :])\n",
    "\n",
    "backbone = GPTMini(ckpt['config']).to(device)\n",
    "backbone.load_state_dict(ckpt['state_dict'])\n",
    "model = GPTClassifier(backbone).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "print(\"üèóÔ∏è Modelo especializado pronto.\")"
   ],
   "metadata": { "id": "model-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. O Treinamento do Especialista\n",
    "\n",
    "O modelo aprende a separar as classes Spam e Normal."
   ],
   "metadata": { "id": "train-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "loss_history = []\n",
    "model.train()\n",
    "for step in range(201):\n",
    "    logits = model(X_train)\n",
    "    loss = F.cross_entropy(logits, Y_train)\n",
    "    optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "    loss_history.append(loss.item())\n",
    "    if step % 50 == 0:\n",
    "        acc = (torch.argmax(logits, dim=-1) == Y_train).float().mean()\n",
    "        print(f\"Passo {step:03d} | Erro: {loss.item():.4f} | Acur√°cia: {acc*100:.1f}%\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(loss_history, color='#34A853')\n",
    "plt.title(\"Curva de Especializa√ß√£o\")\n",
    "plt.show()"
   ],
   "metadata": { "id": "train-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Teste de Campo\n",
    "\n",
    "Vamos testar com frases in√©ditas."
   ],
   "metadata": { "id": "test-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "def classify(text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ids = encode(text)\n",
    "        ids_tensor = torch.tensor(ids[:32] + [stoi.get(' ', 0)] * (32 - len(ids))).unsqueeze(0).to(device)\n",
    "        probs = F.softmax(model(ids_tensor), dim=-1)\n",
    "        pred = torch.argmax(probs, dim=-1).item()\n",
    "        label = \"üö® SPAM\" if pred == 1 else \"‚úÖ NORMAL\"\n",
    "        return f\"{label} ({probs[0, pred].item()*100:.1f}% de confian√ßa)\"\n",
    "\n",
    "print(f\"'ganhe premio agora': {classify('ganhe premio agora')}\")\n",
    "print(f\"'ola tudo bem amigo': {classify('ola tudo bem amigo')}\")"
   ],
   "metadata": { "id": "test-code" },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.12.3" }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}