{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cap√≠tulo 06 ‚Äî Fine-Tuning: A Especializa√ß√£o do Modelo\n",
    "\n",
    "Neste cap√≠tulo, vamos realizar uma \"cirurgia neural\". Pegaremos o GPTMini que aprendeu a ler e escrever no Cap√≠tulo 05 e o ensinaremos a classificar mensagens como **Normal** ou **Spam**.\n",
    "\n",
    "--- \n",
    "### üéØ O Poder da Especializa√ß√£o\n",
    "O Fine-tuning n√£o apaga o que o modelo sabe; ele apenas direciona esse conhecimento para uma tarefa espec√≠fica. Substituiremos a \"cabe√ßa de vocabul√°rio\" por uma \"cabe√ßa de decis√£o\".\n",
    "\n",
    "![Pretrain vs Finetune](./infograficos/01-pretrain-vs-finetune.png)"
   ],
   "metadata": { "id": "header" }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Setup do reposit√≥rio no Colab\n",
    "# ============================================================\n",
    "import os, sys\n",
    "REPO_NAME = \"fazendo-um-llm-do-zero\"\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if not os.path.exists(REPO_NAME):\n",
    "        get_ipython().system(f\"git clone https://github.com/vongrossi/{REPO_NAME}.git\")\n",
    "    \n",
    "    if os.path.exists(REPO_NAME) and os.getcwd().split('/')[-1] != REPO_NAME:\n",
    "        os.chdir(REPO_NAME)\n",
    "\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "print(\"üìÇ Diret√≥rio atual:\", os.getcwd())"
   ],
   "metadata": { "id": "setup-repo" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 1. Setup e Conex√£o com a Intelig√™ncia Base\n",
    "# ============================================================\n",
    "import os, sys, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.gptmini import GPTConfig, GPTMini\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if not os.path.exists(\"gpt_checkpoint.pt\"):\n",
    "    from google.colab import files\n",
    "    print(\"üì§ O arquivo 'gpt_checkpoint.pt' n√£o foi encontrado.\")\n",
    "    print(\"Por favor, suba o checkpoint gerado no final do Cap√≠tulo 05:\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "try:\n",
    "    ckpt = torch.load(\"gpt_checkpoint.pt\", map_location=device, weights_only=False)\n",
    "    stoi, itos = ckpt['stoi'], ckpt['itos']\n",
    "    vocab_size = len(stoi)\n",
    "    print(f\"‚úÖ Intelig√™ncia Base Carregada! Vocabul√°rio: {vocab_size} caracteres.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRO AO CARREGAR: {e}\")"
   ],
   "metadata": { "id": "setup" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Preparando os Dados de Miss√£o\n",
    "\n",
    "Precisamos de exemplos de SPAM para que o modelo entenda o padr√£o de mensagens maliciosas."
   ],
   "metadata": { "id": "data-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "raw_data = [\n",
    "    (\"ganhe 1 milhao agora clique aqui\", 1), # Spam\n",
    "    (\"oferta imperdivel premio gratis\", 1),   # Spam\n",
    "    (\"seu premio esta esperando resgate\", 1), # Spam\n",
    "    (\"ola tudo bem como voce esta\", 0),      # Normal\n",
    "    (\"reuniao de equipe amanha as dez\", 0),   # Normal\n",
    "    (\"voce vai no churrasco no domingo\", 0)   # Normal\n",
    "]\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s.lower() if c in stoi]\n",
    "\n",
    "def build_dataset(data, max_len=32):\n",
    "    X, Y = [], []\n",
    "    for text, label in data:\n",
    "        ids = encode(text)\n",
    "        ids = ids[:max_len] + [stoi.get(' ', 0)] * (max_len - len(ids))\n",
    "        X.append(ids); Y.append(label)\n",
    "    return torch.tensor(X).to(device), torch.tensor(Y).to(device)\n",
    "\n",
    "X_train, Y_train = build_dataset(raw_data)\n",
    "print(f\"üìä Dataset Processado: {len(X_train)} exemplos prontos.\")"
   ],
   "metadata": { "id": "data-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Criando o Classificador\n",
    "\n",
    "Plugamos uma camada linear no topo do Backbone para tomar a decis√£o final.\n",
    "\n",
    "![Head](./infograficos/02-classification-head.png)"
   ],
   "metadata": { "id": "model-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "class GPTClassifier(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=2):\n",
    "        super().__init__(); self.backbone = backbone\n",
    "        self.clf_head = nn.Linear(backbone.config.d_model, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.emb(x)\n",
    "        x = self.backbone.blocks(x)\n",
    "        x = self.backbone.ln_f(x)\n",
    "        return self.clf_head(x[:, -1, :])\n",
    "\n",
    "backbone = GPTMini(ckpt['config']).to(device)\n",
    "backbone.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "model = GPTClassifier(backbone).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "print(\"üèóÔ∏è Modelo especializado constru√≠do.\")"
   ],
   "metadata": { "id": "model-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. O Treinamento do Especialista\n",
    "\n",
    "Damos 200 passos de ajuste fino. O modelo deve parar de chutar e come√ßar a ter certeza."
   ],
   "metadata": { "id": "train-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "loss_history = []\n",
    "model.train()\n",
    "for step in range(201):\n",
    "    logits = model(X_train)\n",
    "    loss = F.cross_entropy(logits, Y_train)\n",
    "    optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "    loss_history.append(loss.item())\n",
    "    if step % 50 == 0: \n",
    "        acc = (torch.argmax(logits, dim=-1) == Y_train).float().mean()\n",
    "        print(f\"Passo {step:03d} | Erro: {loss.item():.4f} | Acur√°cia: {acc*100:.1f}%\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(loss_history, color='#34A853')\n",
    "plt.title(\"Curva de Especializa√ß√£o (Fine-tuning Loss)\")\n",
    "plt.show()"
   ],
   "metadata": { "id": "train-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Teste de Campo: Dashboard de Confian√ßa\n",
    "\n",
    "Vamos testar com frases in√©ditas e observar como o modelo distribui sua confian√ßa entre as categorias."
   ],
   "metadata": { "id": "test-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "def analyze_text(text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ids = encode(text)\n",
    "        ids_tensor = torch.tensor(ids[:32] + [stoi.get(' ', 0)] * (32 - len(ids))).unsqueeze(0).to(device)\n",
    "        \n",
    "        logits = model(ids_tensor)\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "        \n",
    "        # Plotagem\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3), gridspec_kw={'width_ratios': [2, 1]})\n",
    "        \n",
    "        # Gr√°fico de barras\n",
    "        classes = ['NORMAL', 'SPAM']\n",
    "        colors = ['#34A853', '#EA4335']\n",
    "        ax2.barh(classes, probs, color=colors, alpha=0.8)\n",
    "        ax2.set_xlim(0, 1)\n",
    "        ax2.set_title(\"Confian√ßa do Modelo\")\n",
    "        \n",
    "        # Texto descritivo\n",
    "        ax1.axis('off')\n",
    "        label = \"üö® SPAM\" if probs[1] > probs[0] else \"‚úÖ NORMAL\"\n",
    "        ax1.text(0, 0.6, f\"Frase: '{text}'\", fontsize=12, fontweight='bold')\n",
    "        ax1.text(0, 0.3, f\"Verdito: {label}\", fontsize=14, color=colors[np.argmax(probs)])\n",
    "        ax1.text(0, 0.1, f\"Probabilidade Spam: {probs[1]*100:.1f}%\", fontsize=10, color='#5F6368')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"üîç ANALISANDO MENSAGENS IN√âDITAS:\\n\")\n",
    "analyze_text(\"ganhe premio agora mesmo gratis\")\n",
    "analyze_text(\"oi tudo bem vamos na aula\")\n",
    "analyze_text(\"clique para resgatar seu milhao\")"
   ],
   "metadata": { "id": "test-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üèÅ Conclus√£o\n",
    "\n",
    "Neste cap√≠tulo, voc√™ viu como transformar um c√©rebro digital gen√©rico em uma ferramenta √∫til de seguran√ßa. \n",
    "\n",
    "O Fine-tuning √© o que permite que modelos de linguagem sejam aplicados em trilhares de tarefas diferentes no mundo real.\n",
    "\n",
    "![Confusion Matrix](./infograficos/05-metricas-confusion-matrix.png)"
   ],
   "metadata": { "id": "footer" }
  }
 ],
 "metadata": {
  "kernelspec": { \"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\" },
  "language_info\": { \"name\": \"python\", \"version\": \"3.12.3\" }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}