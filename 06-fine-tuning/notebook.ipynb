{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cap√≠tulo 06 ‚Äî Fine-Tuning: A Especializa√ß√£o do Modelo\n",
    "\n",
    "Neste cap√≠tulo, vamos realizar uma \"cirurgia neural\". Pegaremos o GPTMini que aprendeu a ler e escrever no Cap√≠tulo 05 e o ensinaremos a classificar mensagens como **Normal** ou **Spam**.\n",
    "\n",
    "--- \n",
    "### üéØ O Poder da Especializa√ß√£o\n",
    "O Fine-tuning n√£o apaga o que o modelo sabe; ele apenas direciona esse conhecimento para uma tarefa espec√≠fica. Substituiremos a \"cabe√ßa de vocabul√°rio\" por uma \"cabe√ßa de decis√£o\".\n",
    "\n",
    "![Pretrain vs Finetune](./infograficos/01-pretrain-vs-finetune.png)"
   ],
   "metadata": { "id": "header" }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Setup do reposit√≥rio no Colab\n",
    "# ============================================================\n",
    "import os, sys\n",
    "REPO_NAME = \"fazendo-um-llm-do-zero\"\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if not os.path.exists(REPO_NAME):\n",
    "        get_ipython().system(f\"git clone https://github.com/vongrossi/{REPO_NAME}.git\")\n",
    "    \n",
    "    if os.path.exists(REPO_NAME) and os.getcwd().split('/')[-1] != REPO_NAME:\n",
    "        os.chdir(REPO_NAME)\n",
    "\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "print(\"üìÇ Diret√≥rio atual:\", os.getcwd())"
   ],
   "metadata": { "id": "setup-repo" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 1. Setup e Conex√£o com a Intelig√™ncia Base\n",
    "# ============================================================\n",
    "import os, sys, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.gptmini import GPTConfig, GPTMini\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if not os.path.exists(\"gpt_checkpoint.pt\"):\n",
    "    from google.colab import files\n",
    "    print(\"üì§ O arquivo 'gpt_checkpoint.pt' n√£o foi encontrado localmente.\")\n",
    "    print(\"Por favor, suba o checkpoint gerado no final do Cap√≠tulo 05:\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "try:\n",
    "    ckpt = torch.load(\"gpt_checkpoint.pt\", map_location=device, weights_only=False)\n",
    "    stoi, itos = ckpt['stoi'], ckpt['itos']\n",
    "    vocab_size = len(stoi)\n",
    "    print(f\"‚úÖ Intelig√™ncia Base Carregada! Vocabul√°rio: {vocab_size} caracteres.\")\n",
    "    print(f\"üîπ Configura√ß√£o original detectada: Layers={ckpt['config'].n_layers}, Heads={ckpt['config'].n_heads}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRO AO CARREGAR: {e}\")\n",
    "    print(\"Certifique-se de que voc√™ salvou o checkpoint corretamente no Cap√≠tulo 05.\")"
   ],
   "metadata": { "id": "setup" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Preparando os Dados de Miss√£o\n",
    "\n",
    "Precisamos de exemplos de SPAM para que o modelo entenda o padr√£o de mensagens maliciosas."
   ],
   "metadata": { "id": "data-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "raw_data = [\n",
    "    (\"ganhe 1 milhao agora clique aqui\", 1), # Spam\n",
    "    (\"oferta imperdivel premio gratis\", 1),   # Spam\n",
    "    (\"seu premio esta esperando resgate\", 1), # Spam\n",
    "    (\"ola tudo bem como voce esta\", 0),      # Normal\n",
    "    (\"reuniao de equipe amanha as dez\", 0),   # Normal\n",
    "    (\"voce vai no churrasco no domingo\", 0)   # Normal\n",
    "]\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s.lower() if c in stoi]\n",
    "\n",
    "def build_dataset(data, max_len=32):\n",
    "    X, Y = [], []\n",
    "    for text, label in data:\n",
    "        ids = encode(text)\n",
    "        # Padding para garantir que todas as sequ√™ncias tenham o mesmo tamanho\n",
    "        ids = ids[:max_len] + [stoi.get(' ', 0)] * (max_len - len(ids))\n",
    "        X.append(ids)\n",
    "        Y.append(label)\n",
    "    return torch.tensor(X).to(device), torch.tensor(Y).to(device)\n",
    "\n",
    "X_train, Y_train = build_dataset(raw_data)\n",
    "print(f\"üìä Dataset Processado: {len(X_train)} exemplos prontos para o treino.\")"
   ],
   "metadata": { "id": "data-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Criando o Classificador\n",
    "\n",
    "Aqui, acoplamos a \"Cabe√ßa de Classifica√ß√£o\" ao Backbone do Transformer.\n",
    "\n",
    "![Classification Head](./infograficos/02-classification-head.png)"
   ],
   "metadata": { "id": "model-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "class GPTClassifier(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        # Camada que converte os neur√¥nios do GPT em 2 op√ß√µes (Normal/Spam)\n",
    "        self.clf_head = nn.Linear(backbone.config.d_model, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone.emb(x)\n",
    "        x = self.backbone.blocks(x)\n",
    "        x = self.backbone.ln_f(x)\n",
    "        # Usamos o √∫ltimo token para representar o significado da frase inteira (Pooling)\n",
    "        last_token_feat = x[:, -1, :]\n",
    "        return self.clf_head(last_token_feat)\n",
    "\n",
    "# Inicializamos o Backbone com a intelig√™ncia do Cap 05\n",
    "backbone = GPTMini(ckpt['config']).to(device)\n",
    "backbone.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "model = GPTClassifier(backbone).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "print(\"üèóÔ∏è Modelo especializado pronto para o treinamento.\")"
   ],
   "metadata": { "id": "model-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. O Treinamento do Especialista\n",
    "\n",
    "Damos 200 passos de ajuste fino. O modelo deve parar de chutar e come√ßar a ter certeza."
   ],
   "metadata": { "id": "train-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"üöÄ Iniciando Especializa√ß√£o...\")\n",
    "loss_history = []\n",
    "model.train()\n",
    "\n",
    "for step in range(201):\n",
    "    logits = model(X_train)\n",
    "    loss = F.cross_entropy(logits, Y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    if step % 50 == 0: \n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        acc = (preds == Y_train).float().mean()\n",
    "        print(f\"Passo {step:03d} | Erro: {loss.item():.4f} | Acur√°cia: {acc.item()*100:.1f}%\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(loss_history, color='#34A853')\n",
    "plt.title(\"Curva de Aprendizado do Especialista\")\n",
    "plt.show()"
   ],
   "metadata": { "id": "train-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Teste de Campo: Identificando Spams Reais\n",
    "\n",
    "Vamos testar com frases in√©ditas para ver se ele generalizou o conceito de Spam."
   ],
   "metadata": { "id": "test-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "def classify(text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ids = encode(text)\n",
    "        # Padding manual para 32 caracteres\n",
    "        ids_tensor = torch.tensor(ids[:32] + [stoi.get(' ', 0)] * (32 - len(ids))).unsqueeze(0).to(device)\n",
    "        \n",
    "        logits = model(ids_tensor)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        pred = torch.argmax(probs, dim=-1).item()\n",
    "        \n",
    "        label = \"üö® SPAM\" if pred == 1 else \"‚úÖ NORMAL\"\n",
    "        conf = probs[0, pred].item() * 100\n",
    "        return f\"{label} ({conf:.1f}% de confian√ßa)\"\n",
    "\n",
    "print(\"üîç TESTANDO O ESPECIALISTA:\")\n",
    "print(\"-\" * 30)\n",
    "frases = [\n",
    "    \"ganhe seu premio agora mesmo gratis\",\n",
    "    \"oi amigo voce vai na aula hoje\",\n",
    "    \"clique aqui para resgatar 1 milhao\"\n",
    "]\n",
    "\n",
    "for f in frases:\n",
    "    print(f\"Frase: '{f}'\")\n",
    "    print(f\"Resultado: {classify(f)}\\n\")"
   ],
   "metadata": { "id": "test-code" },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}