# Cap√≠tulo 02 ‚Äî Texto vira n√∫mero

No cap√≠tulo anterior, entendemos **o que √© um LLM**:
um modelo que aprende a prever o pr√≥ximo token a partir de contexto.

Agora surge uma pergunta ainda mais fundamental:

**como texto ‚Äî algo simb√≥lico ‚Äî vira algo que um modelo matem√°tico consegue processar?**

Este cap√≠tulo responde exatamente isso.

Aqui entramos no mundo onde:
- palavras viram n√∫meros
- n√∫meros viram vetores
- vetores viram contexto
- e contexto vira aprendizado

---

## Por que texto n√£o pode ser usado diretamente?

Modelos de aprendizado profundo n√£o processam s√≠mbolos.
Eles processam **n√∫meros**.

Texto, √°udio, imagens e v√≠deos s√£o formas de dados **brutos**.
Para que um modelo consiga operar sobre eles, tudo precisa ser convertido para
uma **representa√ß√£o num√©rica densa**.

Isso vale para qualquer modalidade:
- texto
- √°udio
- imagem
- v√≠deo

Todos seguem o mesmo princ√≠pio:  
**dados brutos ‚Üí vetores num√©ricos**

![Texto, √°udio e v√≠deo convertidos em embeddings](./infograficos/01-texto-para-vetor.svg)

Esse processo de convers√£o √© chamado de **embedding**.

---

## Embeddings: representando significado como n√∫meros

Um embedding √© um vetor num√©rico que representa uma entidade ‚Äî uma palavra,
uma frase, um som ou uma imagem ‚Äî em um espa√ßo cont√≠nuo.

A ideia central √© simples:
> coisas semanticamente parecidas ficam **pr√≥ximas** nesse espa√ßo.

Embeddings n√£o carregam ‚Äúsignificado‚Äù no sentido humano.
Eles carregam **rela√ß√µes aprendidas a partir dos dados**.

Para visualizar isso, imagine um espa√ßo bidimensional:

![Espa√ßo vetorial e similaridade sem√¢ntica](./infograficos/02-espaco-vetorial.svg)

Palavras relacionadas a p√°ssaros aparecem pr√≥ximas.
Cidades aparecem agrupadas.
Adjetivos relacionados formam seus pr√≥prios clusters.

Esse tipo de estrutura **emerge do treinamento**.
N√£o √© programado manualmente.

---

## Tokeniza√ß√£o: quebrando texto em unidades process√°veis

Antes de gerar embeddings, precisamos resolver um problema mais b√°sico:

**como quebrar texto em partes menores que o modelo consiga manipular?**

Esse processo se chama **tokeniza√ß√£o**.

Um ponto crucial:
> **tokens n√£o s√£o palavras.**

Dependendo do tokenizer, um token pode ser:
- uma palavra inteira
- parte de uma palavra
- um caractere
- um s√≠mbolo especial

O primeiro passo da tokeniza√ß√£o √©:
1. percorrer todo o texto de treinamento
2. identificar tokens √∫nicos
3. construir um vocabul√°rio

![Tokeniza√ß√£o e constru√ß√£o do vocabul√°rio](./infograficos/03-tokenizacao-vocabulario.svg)

Cada token √∫nico recebe um **ID num√©rico**.
A partir daqui, o texto deixa de ser texto.

---

## Encode e Decode: texto ‚Üî n√∫meros

Com um vocabul√°rio em m√£os, surgem duas opera√ß√µes fundamentais:

- **encode**: texto ‚Üí token IDs  
- **decode**: token IDs ‚Üí texto

Essas duas fun√ß√µes formam a interface b√°sica entre humanos e modelos.

![Encode e Decode de tokens](./infograficos/04-encode-decode.svg)

Esse processo √© quase sempre **revers√≠vel**,
desde que os tokens estejam presentes no vocabul√°rio.

Mas isso nos leva a um problema cl√°ssico.

---

## Palavras desconhecidas e o problema do vocabul√°rio

E se o texto contiver uma palavra que **nunca apareceu no treino**?

Esse √© o problema conhecido como **OOV (out-of-vocabulary)**.

Uma solu√ß√£o ing√™nua seria usar um token especial como `<unk>`,
mas isso destr√≥i muita informa√ß√£o.

A solu√ß√£o moderna √© usar **subwords**.

---

## Byte Pair Encoding (BPE): quebrando palavras em partes

O BPE resolve o problema quebrando palavras desconhecidas em:
- partes menores
- ou at√© caracteres individuais

Isso permite que o modelo processe **qualquer palavra**, mesmo que nunca a tenha visto antes.

![BPE e tokeniza√ß√£o por subwords](./infograficos/05-bpe-subwords.svg)

Com BPE:
- o vocabul√°rio fica control√°vel
- palavras novas ainda carregam estrutura
- o modelo generaliza melhor

Esse √© um dos motivos pelos quais LLMs modernos conseguem lidar com
nomes pr√≥prios, termos t√©cnicos e neologismos.

---

## Janela deslizante: como o modelo aprende de fato

Agora entramos em um dos conceitos mais importantes de todo o livro.

LLMs **n√£o aprendem frases inteiras**.
Eles aprendem a partir de **pares deslocados de entrada e alvo**.

A t√©cnica usada para isso √© a **janela deslizante**.

![Janela deslizante e pares input‚Äìtarget](./infograficos/06-sliding-window.svg)

O processo √©:
- a entrada cont√©m um bloco de tokens
- o alvo √© o pr√≥ximo token
- o modelo **n√£o pode ver o futuro**

Durante o treinamento, aplicamos uma **m√°scara causal**:
o modelo s√≥ enxerga o que veio antes.

Essa mec√¢nica √© o cora√ß√£o do treinamento autoregressivo.

---

## O pipeline completo de entrada de um GPT-like

Agora podemos juntar todas as pe√ßas.

O caminho completo √©:

1. Texto bruto  
2. Tokeniza√ß√£o  
3. Token IDs  
4. Token embeddings  
5. Embeddings posicionais  
6. Entrada no modelo GPT-like

![Pipeline completo de entrada do GPT](./infograficos/07-gpt-input-pipeline.svg)

Cada etapa resolve um problema espec√≠fico:
- s√≠mbolos ‚Üí n√∫meros
- n√∫meros ‚Üí vetores
- vetores ‚Üí sequ√™ncia com posi√ß√£o
- sequ√™ncia ‚Üí contexto

Nada aqui √© opcional.
Esse pipeline √© a base de qualquer modelo GPT moderno.

---

## Conectando com o que vem a seguir

Depois deste cap√≠tulo, √© possivel entender algo fundamental:

> um LLM n√£o ‚Äúl√™ texto‚Äù  
> ele opera sobre **vetores em sequ√™ncia**, organizados no tempo

No pr√≥ximo cap√≠tulo, vamos finalmente abrir a caixa preta da **aten√ß√£o**:
o mecanismo que permite ao modelo decidir **o que √© importante em cada posi√ß√£o**.

Mas agora, pela primeira vez, sabemos **o que exatamente entra no modelo**.

---

## üßæ Gloss√°rio R√°pido ‚Äî Cap√≠tulo 02

**Embedding**  
Representa√ß√£o vetorial densa de uma entidade simb√≥lica.

**Tokeniza√ß√£o**  
Processo de quebrar texto em unidades menores chamadas tokens.

**Token ID**  
Identificador num√©rico √∫nico associado a um token no vocabul√°rio.

**Vocabul√°rio**  
Conjunto de todos os tokens conhecidos pelo modelo.

**BPE (Byte Pair Encoding)**  
Algoritmo de tokeniza√ß√£o baseado em subwords.

**Janela deslizante**  
T√©cnica para gerar pares input‚Äìtarget deslocados no tempo.

**M√°scara causal**  
Mecanismo que impede o modelo de acessar tokens futuros durante o treino.

---

> Este cap√≠tulo √© a funda√ß√£o matem√°tica da linguagem.  
> No pr√≥ximo, veremos como a aten√ß√£o transforma esses vetores em contexto.

---

### üöÄ Execute agora

- **Notebook:** `02-texto-vira-numero/notebook.ipynb`
- **Abrir direto no Colab:** (veja `links.md`)
