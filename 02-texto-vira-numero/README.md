# Cap√≠tulo 02 ‚Äî Texto vira n√∫mero

Se o cap√≠tulo anterior respondeu **o que √© um LLM**,  
este cap√≠tulo responde uma pergunta ainda mais fundamental:

**como texto vira algo que um modelo consegue processar?**

Aqui come√ßa, de fato, a constru√ß√£o t√©cnica de um modelo de linguagem.

---

## üéØ Objetivo do Cap√≠tulo

Ao final deste cap√≠tulo, voc√™ deve ser capaz de:

- entender por que modelos n√£o processam texto bruto
- explicar o papel dos embeddings em modelos de linguagem
- compreender o que √© tokeniza√ß√£o e por que ela √© necess√°ria
- saber como um vocabul√°rio √© constru√≠do
- entender a diferen√ßa entre texto, tokens e token IDs
- explicar como funcionam encode e decode
- compreender como modelos lidam com palavras desconhecidas (BPE)
- entender o conceito de janela deslizante e pares input‚Äìtarget
- visualizar o pipeline completo de entrada de um modelo GPT-like

Este cap√≠tulo **n√£o treina um Transformer ainda**.
Ele constr√≥i toda a **infraestrutura conceitual e pr√°tica** que torna isso poss√≠vel.

---

## üß≠ Como este cap√≠tulo est√° organizado

O conte√∫do segue uma progress√£o l√≥gica, do mais intuitivo ao mais t√©cnico:

1. **Por que texto n√£o pode ser usado diretamente**
   - Dados simb√≥licos vs dados num√©ricos
   - Texto, √°udio e v√≠deo como entrada bruta

2. **Embeddings: representando significado como n√∫meros**
   - Vetores como representa√ß√£o
   - Similaridade no espa√ßo vetorial

3. **Tokeniza√ß√£o**
   - Tokens n√£o s√£o palavras
   - Constru√ß√£o de vocabul√°rio
   - Token IDs

4. **Encode e Decode**
   - Texto ‚Üí n√∫meros
   - N√∫meros ‚Üí texto

5. **Lidando com palavras desconhecidas**
   - Problema de OOV (out-of-vocabulary)
   - Byte Pair Encoding (BPE)
   - Subwords

6. **Janela deslizante e pares input‚Äìtarget**
   - Como o modelo aprende a prever o pr√≥ximo token
   - M√°scara causal

7. **Pipeline completo de entrada do GPT**
   - Texto ‚Üí tokens ‚Üí IDs ‚Üí embeddings ‚Üí embeddings posicionais

Cada se√ß√£o √© acompanhada de **infogr√°ficos did√°ticos** e exemplos pr√°ticos.

---

## üß™ Parte pr√°tica (notebook)

Este cap√≠tulo inclui um notebook execut√°vel no Google Colab que:

- implementa um tokenizer simples
- constr√≥i um vocabul√°rio a partir de texto
- converte texto em token IDs
- cria pares input‚Äìtarget usando janela deslizante
- conecta o pipeline completo de entrada de um modelo GPT-like

O foco do notebook √© **clareza conceitual**, n√£o performance.

---

## üìä Infogr√°ficos

Este cap√≠tulo utiliza infogr√°ficos para visualizar conceitos como:

- convers√£o de texto em vetores
- espa√ßo vetorial e similaridade sem√¢ntica
- tokeniza√ß√£o e vocabul√°rio
- encode e decode
- tokeniza√ß√£o por subwords (BPE)
- janela deslizante
- pipeline de entrada de um modelo GPT-like

As descri√ß√µes dos infogr√°ficos est√£o documentadas em:

- [infograficos/README.md](infograficos/README.md)

---

## ‚ñ∂Ô∏è Como executar

1. Leia o di√°rio do cap√≠tulo:
   - [diario.md](diario.md)

2. Execute o notebook:
   - [notebook.ipynb](notebook.ipynb)

3. Ou abra diretamente no Google Colab:
   - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vongrossi/fazendo-um-llm-do-zero/blob/main/02-texto-vira-numero/notebook.ipynb)
   - ou veja mais detalhes em [links.md](links.md)

---

## üé• Material complementar

Este cap√≠tulo √© fortemente inspirado no conte√∫do do pr√≥prio autor do livro,
que aprofunda visualmente os conceitos de tokeniza√ß√£o e embeddings:


---

## üìò Refer√™ncia

Este cap√≠tulo √© baseado no Cap√≠tulo 2 do livro  
*Build a Large Language Model (From Scratch)*, de Sebastian Raschka.

O conte√∫do foi adaptado para:
- uma abordagem did√°tica em portugu√™s
- execu√ß√£o pr√°tica no Google Colab
- foco em entendimento profundo, n√£o apenas uso de bibliotecas prontas

---

## ‚ö†Ô∏è Observa√ß√£o importante

Se este cap√≠tulo parecer denso, isso √© intencional.

Tokeniza√ß√£o e embeddings s√£o conceitos que:
- aparecem em todos os LLMs
- impactam desempenho, custo e qualidade
- influenciam diretamente como o modelo aprende

Entender bem este cap√≠tulo economiza muita confus√£o nos pr√≥ximos.
