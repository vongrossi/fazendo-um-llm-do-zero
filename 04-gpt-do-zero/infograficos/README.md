# InfogrÃ¡ficos â€” CapÃ­tulo 04 (Construindo um GPT do Zero)

Este diretÃ³rio contÃ©m os infogrÃ¡ficos utilizados no CapÃ­tulo 04 da sÃ©rie:

**Fazendo um LLM do Zero**

Este capÃ­tulo marca a transiÃ§Ã£o entre compreender os componentes individuais dos Transformers e construir um modelo GPT completo.

Os infogrÃ¡ficos deste capÃ­tulo tÃªm como objetivo ajudar o leitor a visualizar:

- como um bloco Transformer Ã© estruturado
- como o fluxo de dados ocorre dentro do modelo
- como feedforward complementa a atenÃ§Ã£o
- como conexÃµes residuais e normalizaÃ§Ã£o estabilizam o treinamento
- como o pipeline completo de um GPT funciona

---

## ğŸ¯ EstratÃ©gia pedagÃ³gica dos infogrÃ¡ficos

Os infogrÃ¡ficos seguem a progressÃ£o:

Arquitetura â†’ Fluxo â†’ Componentes â†’ Pipeline completo


Essa progressÃ£o acompanha exatamente a narrativa do capÃ­tulo.

---

## ğŸ“Š Lista de InfogrÃ¡ficos

01-transformer-block.png
02-transformer-flow.png
03-feedforward.png
04-residual-norm.png
05-gpt-mini-pipeline.png


---

## ğŸ§­ DescriÃ§Ã£o pedagÃ³gica de cada infogrÃ¡fico

---

### 01 â€” Estrutura de um Bloco Transformer

ğŸ“ SeÃ§Ã£o do diÃ¡rio:
O bloco fundamental do Transformer

ğŸ¯ Objetivo didÃ¡tico:

Mostrar visualmente os quatro componentes principais de um bloco Transformer:

- Self-Attention
- Feedforward Network
- ConexÃ£o residual
- Layer Normalization

Este infogrÃ¡fico apresenta a arquitetura estrutural do bloco.

---

### 02 â€” Fluxo de dados dentro do Transformer

ğŸ“ SeÃ§Ã£o do diÃ¡rio:
Como os dados fluem dentro de um bloco Transformer

ğŸ¯ Objetivo didÃ¡tico:

Mostrar como um token percorre as etapas do bloco:

- Entrada
- AtenÃ§Ã£o
- Residual + NormalizaÃ§Ã£o
- Feedforward
- Residual + NormalizaÃ§Ã£o
- SaÃ­da

Este infogrÃ¡fico foca no fluxo computacional.

---

### 03 â€” Feedforward Network

ğŸ“ SeÃ§Ã£o do diÃ¡rio:
Feedforward Network: refinando representaÃ§Ãµes

ğŸ¯ Objetivo didÃ¡tico:

Explicar como a rede feedforward:

- aplica transformaÃ§Ã£o nÃ£o-linear
- expande e comprime dimensionalidade
- refina embeddings individualmente por posiÃ§Ã£o

---

### 04 â€” ConexÃµes Residuais e Layer Normalization

ğŸ“ SeÃ§Ã£o do diÃ¡rio:
ConexÃµes residuais e normalizaÃ§Ã£o

ğŸ¯ Objetivo didÃ¡tico:

Mostrar como:

- conexÃµes residuais preservam informaÃ§Ã£o
- normalizaÃ§Ã£o estabiliza treinamento
- ambos permitem empilhamento profundo de camadas

---

### 05 â€” Pipeline completo de um GPT didÃ¡tico

ğŸ“ SeÃ§Ã£o do diÃ¡rio:
Construindo um GPT didÃ¡tico

ğŸ¯ Objetivo didÃ¡tico:

Mostrar o fluxo completo:

Texto â†’ Tokens â†’ Embeddings â†’ Blocos Transformer â†’ SaÃ­da de probabilidades

Este infogrÃ¡fico fecha o capÃ­tulo e conecta todos os conceitos anteriores.

---

