# CapÃ­tulo 04 â€” Construindo um GPT do Zero

Nos capÃ­tulos anteriores, construÃ­mos o conhecimento necessÃ¡rio para entender os LLMs:

- aprendemos como texto vira nÃºmeros
- entendemos embeddings
- exploramos o mecanismo de self-attention

Agora chegamos ao momento em que tudo isso se conecta.

Neste capÃ­tulo, vamos montar um modelo GPT completo, peÃ§a por peÃ§a, entendendo cada bloco que compÃµe essa arquitetura.

---

## Relembrando a jornada atÃ© aqui

Antes de comeÃ§ar a construÃ§Ã£o do modelo, vale revisitar o pipeline que jÃ¡ aprendemos.

Um LLM moderno recebe:

1. Texto bruto  
2. TokenizaÃ§Ã£o  
3. Token IDs  
4. Embeddings  
5. Self-attention para construir contexto  

AtÃ© o capÃ­tulo anterior, exploramos cada etapa isoladamente.

Agora vamos conectar essas partes dentro de uma arquitetura funcional.

---

## O bloco fundamental do Transformer

O GPT Ã© construÃ­do empilhando mÃºltiplos blocos Transformer.

Cada bloco contÃ©m quatro componentes principais:

- Self-Attention  
- Feedforward Network  
- ConexÃ£o residual  
- NormalizaÃ§Ã£o  

![Bloco Transformer completo](./infograficos/01-transformer-block.svg)

Esse bloco Ã© repetido diversas vezes ao longo do modelo, permitindo que o contexto seja refinado progressivamente.

---

## Como os dados fluem dentro de um bloco Transformer

Quando um token entra em um bloco Transformer, ele passa por uma sequÃªncia de transformaÃ§Ãµes.

![Fluxo de dados dentro do Transformer](./infograficos/02-transformer-flow.svg)

O fluxo bÃ¡sico Ã©:

1. Self-attention constrÃ³i contexto  
2. ConexÃ£o residual preserva informaÃ§Ã£o original  
3. NormalizaÃ§Ã£o estabiliza valores  
4. Feedforward refina representaÃ§Ãµes  
5. Nova conexÃ£o residual mantÃ©m estabilidade  

Esse design permite que Transformers sejam extremamente profundos sem perder informaÃ§Ã£o.

---

## Feedforward Network: refinando representaÃ§Ãµes

ApÃ³s a etapa de atenÃ§Ã£o, cada posiÃ§Ã£o da sequÃªncia passa por uma rede neural feedforward.

![Feedforward network](./infograficos/03-feedforward.svg)

Essa camada:

- Ã© aplicada independentemente a cada token
- introduz nÃ£o-linearidade
- expande e comprime a dimensionalidade dos vetores
- permite refinamento semÃ¢ntico

Sem feedforward, o modelo seria limitado a combinaÃ§Ãµes lineares dos embeddings.

---

## ConexÃµes residuais e normalizaÃ§Ã£o

Treinar redes profundas costuma gerar problemas de estabilidade e perda de gradiente.

Transformers resolvem isso usando duas tÃ©cnicas fundamentais:

### ConexÃµes residuais

Permitem que a informaÃ§Ã£o original continue fluindo ao longo da rede.

### Layer Normalization

MantÃ©m distribuiÃ§Ãµes numÃ©ricas estÃ¡veis durante o treinamento.

![Residual + LayerNorm](./infograficos/04-residual-norm.svg)

Juntas, essas duas tÃ©cnicas tornam possÃ­vel empilhar dezenas de blocos Transformer.

---

## Construindo um GPT didÃ¡tico

Agora podemos juntar todas as peÃ§as.

Um GPT bÃ¡sico contÃ©m:

- embeddings de tokens  
- embeddings posicionais  
- mÃºltiplos blocos Transformer  
- camada linear de saÃ­da  

![Pipeline completo do GPT didÃ¡tico](./infograficos/05-gpt-mini-pipeline.svg)

Esse pipeline transforma uma sequÃªncia de tokens em probabilidades para o prÃ³ximo token.

---

## O papel da mÃ¡scara causal

Como vimos no capÃ­tulo anterior, modelos autoregressivos precisam prever o prÃ³ximo token sem olhar o futuro.

Durante o treinamento, aplicamos uma mÃ¡scara causal para garantir que:

- cada posiÃ§Ã£o sÃ³ enxergue tokens anteriores
- o modelo aprenda geraÃ§Ã£o sequencial realista

Essa propriedade Ã© o que permite ao GPT gerar texto de forma coerente.

---

## GeraÃ§Ã£o autoregressiva

Depois de treinado, o modelo pode gerar texto token por token.

O processo funciona assim:

1. Recebe tokens iniciais  
2. Prediz probabilidades do prÃ³ximo token  
3. Seleciona um token  
4. Adiciona Ã  sequÃªncia  
5. Repete o processo  

Esse ciclo permite gerar sequÃªncias arbitrariamente longas.

---

## Como isso se conecta com modelos reais

Modelos GPT comerciais usam exatamente a mesma arquitetura bÃ¡sica, mas com:

- muito mais parÃ¢metros  
- muito mais dados  
- tÃ©cnicas avanÃ§adas de treinamento  
- paralelizaÃ§Ã£o massiva  

O modelo didÃ¡tico que construiremos mantÃ©m os mesmos princÃ­pios fundamentais.

---

## O que construiremos no notebook

No notebook deste capÃ­tulo vamos implementar:

- Self-attention com mÃ¡scara causal  
- Feedforward network  
- TransformerBlock completo  
- GPTMini empilhando blocos  
- Loop de treinamento simples  
- GeraÃ§Ã£o autoregressiva  

O objetivo nÃ£o Ã© performance, mas compreensÃ£o estrutural.

---

## LimitaÃ§Ãµes do modelo didÃ¡tico

Nosso modelo terÃ¡ algumas simplificaÃ§Ãµes:

- dataset pequeno  
- poucas camadas  
- dimensionalidade reduzida  
- treinamento curto  

Essas simplificaÃ§Ãµes tornam o modelo viÃ¡vel para execuÃ§Ã£o em CPU e ambiente educacional.

---

## Preparando o prÃ³ximo passo

Depois de construir o GPT, surge uma nova pergunta:

> Como treinamos modelos dessa escala de forma eficiente?

Esse serÃ¡ o foco do prÃ³ximo capÃ­tulo, onde exploraremos:

- funÃ§Ã£o de perda  
- otimizaÃ§Ã£o  
- estratÃ©gias de treinamento  
- geraÃ§Ã£o controlada  

---

## ğŸ§¾ GlossÃ¡rio RÃ¡pido â€” CapÃ­tulo 04

**Transformer Block**  
Unidade bÃ¡sica composta por self-attention, feedforward, residual e normalizaÃ§Ã£o.

**Feedforward Network**  
Rede neural aplicada separadamente a cada posiÃ§Ã£o da sequÃªncia.

**Layer Normalization**  
TÃ©cnica que estabiliza distribuiÃ§Ãµes numÃ©ricas durante o treinamento.

**Residual Connection**  
Atalho que permite que entradas sejam somadas Ã s saÃ­das de camadas profundas.

**Autoregressive Generation**  
Processo de gerar tokens sequencialmente com base nos tokens anteriores.

---

> Neste capÃ­tulo, saÃ­mos do territÃ³rio conceitual e entramos na construÃ§Ã£o real de modelos generativos.

---

### ğŸš€ Execute agora

- **Notebook:** `04-gpt-do-zero/notebook.ipynb`
- **Abrir direto no Colab:** (veja `links.md`)
