# Cap√≠tulo 04 ‚Äî Construindo um GPT do Zero

Nos cap√≠tulos anteriores, constru√≠mos o conhecimento necess√°rio para entender os LLMs:

- aprendemos como texto vira n√∫meros
- entendemos embeddings
- exploramos o mecanismo de self-attention

Agora chegamos ao momento em que tudo isso se conecta.

Neste cap√≠tulo, vamos montar um modelo GPT completo, pe√ßa por pe√ßa, entendendo cada bloco que comp√µe essa arquitetura.

---
---

## Arquivos do Cap√≠tulo
- [README.md](README.md)
- [notebook.ipynb](notebook.ipynb)
- [links.md](links.md)
- [infograficos/README.md](infograficos/README.md)


## Relembrando a jornada at√© aqui

Antes de come√ßar a constru√ß√£o do modelo, vale revisitar o pipeline que j√° aprendemos.

Um LLM moderno recebe:

1. Texto bruto  
2. Tokeniza√ß√£o  
3. Token IDs  
4. Embeddings  
5. Self-attention para construir contexto  

At√© o cap√≠tulo anterior, exploramos cada etapa isoladamente.

Agora vamos conectar essas partes dentro de uma arquitetura funcional.

---

## O bloco fundamental do Transformer

O GPT √© constru√≠do empilhando m√∫ltiplos blocos Transformer.

Cada bloco cont√©m quatro componentes principais:

- Self-Attention  
- Feedforward Network  
- Conex√£o residual  
- Normaliza√ß√£o  

![Bloco Transformer completo](./infograficos/01-transformer-block.png)

Esse bloco √© repetido diversas vezes ao longo do modelo, permitindo que o contexto seja refinado progressivamente.

---

## Como os dados fluem dentro de um bloco Transformer

Quando um token entra em um bloco Transformer, ele passa por uma sequ√™ncia de transforma√ß√µes.

![Fluxo de dados dentro do Transformer](./infograficos/02-transformer-flow.png)

O fluxo b√°sico √©:

1. Self-attention constr√≥i contexto  
2. Conex√£o residual preserva informa√ß√£o original  
3. Normaliza√ß√£o estabiliza valores  
4. Feedforward refina representa√ß√µes  
5. Nova conex√£o residual mant√©m estabilidade  

Esse design permite que Transformers sejam extremamente profundos sem perder informa√ß√£o.

---

## Feedforward Network: refinando representa√ß√µes

Ap√≥s a etapa de aten√ß√£o, cada posi√ß√£o da sequ√™ncia passa por uma rede neural feedforward.

![Feedforward network](./infograficos/03-feedforward.png)

Essa camada:

- √© aplicada independentemente a cada token
- introduz n√£o-linearidade
- expande e comprime a dimensionalidade dos vetores
- permite refinamento sem√¢ntico

Sem feedforward, o modelo seria limitado a combina√ß√µes lineares dos embeddings.

---

## Conex√µes residuais e normaliza√ß√£o

Treinar redes profundas costuma gerar problemas de estabilidade e perda de gradiente.

Transformers resolvem isso usando duas t√©cnicas fundamentais:

### Conex√µes residuais

Permitem que a informa√ß√£o original continue fluindo ao longo da rede.

### Layer Normalization

Mant√©m distribui√ß√µes num√©ricas est√°veis durante o treinamento.

![Residual + LayerNorm](./infograficos/04-residual-norm.png)

Juntas, essas duas t√©cnicas tornam poss√≠vel empilhar dezenas de blocos Transformer.

---

## Construindo um GPT did√°tico

Agora podemos juntar todas as pe√ßas.

Um GPT b√°sico cont√©m:

- embeddings de tokens  
- embeddings posicionais  
- m√∫ltiplos blocos Transformer  
- camada linear de sa√≠da  

![Pipeline completo do GPT did√°tico](./infograficos/05-gpt-mini-pipeline.png)

Esse pipeline transforma uma sequ√™ncia de tokens em probabilidades para o pr√≥ximo token.

---

## O papel da m√°scara causal

Como vimos no cap√≠tulo anterior, modelos autoregressivos precisam prever o pr√≥ximo token sem olhar o futuro.

Durante o treinamento, aplicamos uma m√°scara causal para garantir que:

- cada posi√ß√£o s√≥ enxergue tokens anteriores
- o modelo aprenda gera√ß√£o sequencial realista

Essa propriedade √© o que permite ao GPT gerar texto de forma coerente.

---

## Gera√ß√£o autoregressiva

Depois de treinado, o modelo pode gerar texto token por token.

O processo funciona assim:

1. Recebe tokens iniciais  
2. Prediz probabilidades do pr√≥ximo token  
3. Seleciona um token  
4. Adiciona √† sequ√™ncia  
5. Repete o processo  

Esse ciclo permite gerar sequ√™ncias arbitrariamente longas.

---

## Como isso se conecta com modelos reais

Modelos GPT comerciais usam exatamente a mesma arquitetura b√°sica, mas com:

- muito mais par√¢metros  
- muito mais dados  
- t√©cnicas avan√ßadas de treinamento  
- paraleliza√ß√£o massiva  

O modelo did√°tico que construiremos mant√©m os mesmos princ√≠pios fundamentais.

---

## O que construiremos no notebook

No notebook deste cap√≠tulo vamos implementar:

- Self-attention com m√°scara causal  
- Feedforward network  
- TransformerBlock completo  
- GPTMini empilhando blocos  
- Loop de treinamento simples  
- Gera√ß√£o autoregressiva  

O objetivo n√£o √© performance, mas compreens√£o estrutural.

---

## Limita√ß√µes do modelo did√°tico

Nosso modelo ter√° algumas simplifica√ß√µes:

- dataset pequeno  
- poucas camadas  
- dimensionalidade reduzida  
- treinamento curto  

Essas simplifica√ß√µes tornam o modelo vi√°vel para execu√ß√£o em CPU e ambiente educacional.

---

## Preparando o pr√≥ximo passo

Depois de construir o GPT, surge uma nova pergunta:

> Como treinamos modelos dessa escala de forma eficiente?

Esse ser√° o foco do pr√≥ximo cap√≠tulo, onde exploraremos:

- fun√ß√£o de perda  
- otimiza√ß√£o  
- estrat√©gias de treinamento  
- gera√ß√£o controlada  

---

## üßæ Gloss√°rio R√°pido ‚Äî Cap√≠tulo 04

**Transformer Block**  
Unidade b√°sica composta por self-attention, feedforward, residual e normaliza√ß√£o.

**Feedforward Network**  
Rede neural aplicada separadamente a cada posi√ß√£o da sequ√™ncia.

**Layer Normalization**  
T√©cnica que estabiliza distribui√ß√µes num√©ricas durante o treinamento.

**Residual Connection**  
Atalho que permite que entradas sejam somadas √†s sa√≠das de camadas profundas.

**Autoregressive Generation**  
Processo de gerar tokens sequencialmente com base nos tokens anteriores.

---

> Neste cap√≠tulo, sa√≠mos do territ√≥rio conceitual e entramos na constru√ß√£o real de modelos generativos.

---

### üöÄ Execute agora

- **Notebook:** `04-gpt-do-zero/notebook.ipynb`
- **Abrir direto no Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vongrossi/fazendo-um-llm-do-zero/blob/main/04-gpt-do-zero/notebook.ipynb)
