{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cap√≠tulo 04 ‚Äî Construindo um GPT do Zero\n",
        "\n",
        "Este notebook acompanha o Cap√≠tulo 04 da s√©rie **Fazendo um LLM do Zero**.\n",
        "\n",
        "Neste notebook vamos construir um GPT m√≠nimo (did√°tico), pe√ßa por pe√ßa.\n",
        "\n",
        "üéØ **Objetivos deste notebook:**\n",
        "- Montar um **TransformerBlock** completo\n",
        "- Implementar **Self-Attention com M√°scara Causal**\n",
        "- Adicionar **MLP (Feedforward)**\n",
        "- Aplicar **Residual Connections + LayerNorm**\n",
        "- Empilhar blocos para formar um **GPTMini**\n",
        "- Fazer uma **etapa de treino curta**\n",
        "- Testar **Gera√ß√£o Autoregressiva**\n"
      ],
      "metadata": {
        "id": "6Vqx__wymKTZ"
      },
      "id": "6Vqx__wymKTZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup e Configura√ß√£o"
      ],
      "metadata": {
        "id": "setup-header"
      },
      "id": "setup-header"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Setup do reposit√≥rio\n",
        "# ============================================================\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/vongrossi/fazendo-um-llm-do-zero.git\"\n",
        "REPO_DIR = \"fazendo-um-llm-do-zero\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone {REPO_URL}\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print(\"Diret√≥rio atual:\", os.getcwd())\n"
      ],
      "metadata": {
        "id": "-fcgbbJemGBD"
      },
      "id": "-fcgbbJemGBD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Depend√™ncias e Device"
      ],
      "metadata": {
        "id": "636mFT2bmNeX"
      },
      "id": "636mFT2bmNeX"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Depend√™ncias\n",
        "# ============================================================\n",
        "# Observa√ß√£o: o Colab geralmente j√° tem torch instalado.\n",
        "# Mas este pip garante consist√™ncia se voc√™ quiser travar vers√µes no repo.\n",
        "\n",
        "!pip -q install -r 04-gpt-do-zero/requirements.txt\n"
      ],
      "metadata": {
        "id": "6K6LASCumQWd"
      },
      "id": "6K6LASCumQWd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No Colab temos a possibilidade de uso de GPU isso da um poder extra de processamento para uma serie de atividades"
      ],
      "metadata": {
        "id": "krwlL2xYmRmv"
      },
      "id": "krwlL2xYmRmv"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# GPU opcional no Colab\n",
        "# ============================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "zDg1WkIxmfa8"
      },
      "id": "zDg1WkIxmfa8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. O que vamos construir?\n",
        "\n",
        "Um GPT (Decoder-Only Transformer) did√°tico tem:\n",
        "\n",
        "1) Token Embeddings  \n",
        "2) Positional Embeddings  \n",
        "3) Blocos Transformer empilhados, cada um com:\n",
        "   - LayerNorm\n",
        "   - Self-Attention (causal)\n",
        "   - Residual connection\n",
        "   - LayerNorm\n",
        "   - MLP / Feedforward\n",
        "   - Residual connection\n",
        "4) Linear head (logits para o vocabul√°rio)\n",
        "5) Loss (cross-entropy) para treino\n",
        "6) Gera√ß√£o autoregressiva (token por token)\n",
        "\n",
        "A ordem acima √© a ordem do c√≥digo.\n"
      ],
      "metadata": {
        "id": "3zG9wrV9mk1O"
      },
      "id": "3zG9wrV9mk1O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Dataset M√≠nimo"
      ],
      "metadata": {
        "id": "QxyC_pSCmr37"
      },
      "id": "QxyC_pSCmr37"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Dataset minimo\n",
        "# ============================================================\n",
        "# Aqui queremos um dataset que rode r√°pido.\n",
        "# Em modelos reais, isso seria enorme. Aqui √© s√≥ para demonstrar pipeline.\n",
        "\n",
        "text = \"\"\"\n",
        "o gato subiu no telhado\n",
        "o cachorro subiu no sofa\n",
        "o gato dormiu no sofa\n",
        "o cachorro dormiu no tapete\n",
        "o gato pulou no muro\n",
        "\"\"\"\n",
        "text = text.strip().lower()\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "lbRsTig8mv_J"
      },
      "id": "lbRsTig8mv_J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Tokeniza√ß√£o Simples"
      ],
      "metadata": {
        "id": "g4uocrsNm3AU"
      },
      "id": "g4uocrsNm3AU"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Tokeniza√ß√£o simples (did√°tica)\n",
        "# ============================================================\n",
        "tokens = text.split()\n",
        "vocab = sorted(set(tokens))\n",
        "\n",
        "token_to_id = {t:i for i,t in enumerate(vocab)}\n",
        "id_to_token = {i:t for t,i in token_to_id.items()}\n",
        "\n",
        "encoded = [token_to_id[t] for t in tokens]\n",
        "\n",
        "print(\"Vocab size:\", len(vocab))\n",
        "print(\"Tokens:\", tokens[:20])\n",
        "print(\"Encoded:\", encoded[:20])\n"
      ],
      "metadata": {
        "id": "FQmzfyNSm4L7"
      },
      "id": "FQmzfyNSm4L7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Sliding Window (Input/Target)"
      ],
      "metadata": {
        "id": "x63LbJMUm5eS"
      },
      "id": "x63LbJMUm5eS"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Pares input/target para previs√£o do pr√≥ximo token\n",
        "# ============================================================\n",
        "def build_dataset(token_ids, context_size):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(token_ids) - context_size):\n",
        "        X.append(token_ids[i:i+context_size])\n",
        "        Y.append(token_ids[i+context_size])\n",
        "    return torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "context_size = 5\n",
        "X, Y = build_dataset(encoded, context_size)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"Y shape:\", Y.shape)\n",
        "print(\"Exemplo input:\", X[0], \"-> target:\", Y[0], id_to_token[int(Y[0])])\n"
      ],
      "metadata": {
        "id": "2Gw11O5Bm8Af"
      },
      "id": "2Gw11O5Bm8Af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Componentes do GPT (Constru√ß√£o 1 a 1)\n",
        "\n",
        "### 4.1 Configura√ß√£o do Modelo"
      ],
      "metadata": {
        "id": "fxaI5e9jm9m5"
      },
      "id": "fxaI5e9jm9m5"
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    vocab_size: int\n",
        "    context_size: int\n",
        "    d_model: int = 64\n",
        "    n_heads: int = 4\n",
        "    n_layers: int = 2\n",
        "    dropout: float = 0.1\n",
        "\n",
        "config = GPTConfig(\n",
        "    vocab_size=len(vocab),\n",
        "    context_size=context_size,\n",
        "    d_model=64,\n",
        "    n_heads=4,\n",
        "    n_layers=2,\n",
        "    dropout=0.1\n",
        ")\n",
        "config\n"
      ],
      "metadata": {
        "id": "baU6axISnGpQ"
      },
      "id": "baU6axISnGpQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Token Embeddings + Positional Embeddings"
      ],
      "metadata": {
        "id": "pYcXBvDOnOCt"
      },
      "id": "pYcXBvDOnOCt"
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = nn.Embedding(config.context_size, config.d_model)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        # idx: (B, T)\n",
        "        B, T = idx.shape\n",
        "        positions = torch.arange(0, T, device=idx.device).unsqueeze(0)  # (1, T)\n",
        "        x = self.tok_emb(idx) + self.pos_emb(positions)                 # (B, T, C)\n",
        "        return self.dropout(x)\n",
        "\n",
        "emb_layer = TokenAndPositionEmbedding(config).to(device)\n",
        "\n",
        "dummy = X[:2].to(device)\n",
        "out = emb_layer(dummy)\n",
        "print(\"Emb output:\", out.shape)\n"
      ],
      "metadata": {
        "id": "hms7AHIqnOyC"
      },
      "id": "hms7AHIqnOyC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Self-Attention com M√°scara Causal"
      ],
      "metadata": {
        "id": "MQ7o9d2RnUSE"
      },
      "id": "MQ7o9d2RnUSE"
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_heads == 0\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.d_model // config.n_heads\n",
        "\n",
        "        # Proje√ß√µes Q, K, V em uma s√≥ camada (mais simples e comum)\n",
        "        self.qkv = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # M√°scara causal fixa (T x T)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.tril(torch.ones(config.context_size, config.context_size))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)  # (B, T, 3C)\n",
        "        q, k, v = qkv.split(C, dim=2)\n",
        "\n",
        "        # reshape para multi-head: (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # aten√ß√£o: (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # aplica m√°scara causal (bloqueia futuro)\n",
        "        att = att.masked_fill(self.mask[:T, :T] == 0, float(\"-inf\"))\n",
        "\n",
        "        # softmax nos scores\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.dropout(att)\n",
        "\n",
        "        # contexto: (B, nh, T, hs)\n",
        "        y = att @ v\n",
        "\n",
        "        # junta heads: (B, T, C)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        # projeta de volta\n",
        "        y = self.out_proj(y)\n",
        "        y = self.dropout(y)\n",
        "        return y\n",
        "\n",
        "attn = CausalSelfAttention(config).to(device)\n",
        "y = attn(out)\n",
        "print(\"Attn output:\", y.shape)\n"
      ],
      "metadata": {
        "id": "vHSbjNUgnVGr"
      },
      "id": "vHSbjNUgnVGr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Feedforward (MLP)"
      ],
      "metadata": {
        "id": "5qRqSlXmnXmD"
      },
      "id": "5qRqSlXmnXmD"
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model),\n",
        "            nn.Dropout(config.dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "ff = FeedForward(config).to(device)\n",
        "z = ff(y)\n",
        "print(\"FF output:\", z.shape)\n"
      ],
      "metadata": {
        "id": "ebLQ70exnaR2"
      },
      "id": "ebLQ70exnaR2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 TransformerBlock (LayerNorm + Residual)"
      ],
      "metadata": {
        "id": "uyO0QhO0nbzA"
      },
      "id": "uyO0QhO0nbzA"
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.ff = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-LN Transformer (est√°vel e comum em GPTs modernos)\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "block = TransformerBlock(config).to(device)\n",
        "b = block(out)\n",
        "print(\"Block output:\", b.shape)\n"
      ],
      "metadata": {
        "id": "ICheSCahnefS"
      },
      "id": "ICheSCahnefS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. GPTMini Completo"
      ],
      "metadata": {
        "id": "x9paWteVngZg"
      },
      "id": "x9paWteVngZg"
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTMini(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.emb = TokenAndPositionEmbedding(config)\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        x = self.emb(idx)              # (B, T, C)\n",
        "        x = self.blocks(x)             # (B, T, C)\n",
        "        x = self.ln_f(x)               # (B, T, C)\n",
        "        logits = self.head(x)          # (B, T, vocab)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # usamos apenas a √∫ltima posi√ß√£o para prever o pr√≥ximo token do contexto\n",
        "            logits_last = logits[:, -1, :]      # (B, vocab)\n",
        "            loss = F.cross_entropy(logits_last, targets)\n",
        "        return logits, loss\n",
        "\n",
        "model = GPTMini(config).to(device)\n",
        "\n",
        "logits, loss = model(X[:4].to(device), Y[:4].to(device))\n",
        "print(\"Logits:\", logits.shape, \"Loss:\", float(loss))\n"
      ],
      "metadata": {
        "id": "H4Rrg5H0nl0_"
      },
      "id": "H4Rrg5H0nl0_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Treinamento Curto"
      ],
      "metadata": {
        "id": "1xx7W2Suno1L"
      },
      "id": "1xx7W2Suno1L"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Treino did√°tico r√°pido\n",
        "# ============================================================\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "X_train = X.to(device)\n",
        "Y_train = Y.to(device)\n",
        "\n",
        "model.train()\n",
        "for step in range(300):\n",
        "    # mini-batch pequeno\n",
        "    idx = torch.randint(0, X_train.size(0), (16,), device=device)\n",
        "    xb = X_train[idx]\n",
        "    yb = Y_train[idx]\n",
        "\n",
        "    _, loss = model(xb, yb)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        print(f\"step {step:03d} | loss {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "KiuTx0IwnwG0"
      },
      "id": "KiuTx0IwnwG0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Gera√ß√£o Autoregressiva"
      ],
      "metadata": {
        "id": "I52Y3Dkpn1Id"
      },
      "id": "I52Y3Dkpn1Id"
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_text(s):\n",
        "    return [token_to_id[t] for t in s.lower().split() if t in token_to_id]\n",
        "\n",
        "def decode_ids(ids):\n",
        "    return \" \".join(id_to_token[i] for i in ids)\n"
      ],
      "metadata": {
        "id": "MtrOXwTzn28d"
      },
      "id": "MtrOXwTzn28d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Gera√ß√£o (Greedy + Temperature Sampling)"
      ],
      "metadata": {
        "id": "BJYu-QeLn8Yh"
      },
      "id": "BJYu-QeLn8Yh"
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, start_tokens, max_new_tokens=10, temperature=1.0):\n",
        "    model.eval()\n",
        "    idx = torch.tensor(start_tokens, dtype=torch.long, device=device).unsqueeze(0)  # (1, T)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # recorta para context_size\n",
        "        idx_cond = idx[:, -config.context_size:]\n",
        "\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / temperature  # (1, vocab)\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # greedy (mais previs√≠vel)\n",
        "        next_id = torch.argmax(probs, dim=-1, keepdim=True)  # (1, 1)\n",
        "\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "\n",
        "    return idx.squeeze(0).tolist()\n",
        "\n",
        "start = encode_text(\"o gato subiu\")\n",
        "generated_ids = generate(model, start, max_new_tokens=8)\n",
        "\n",
        "print(\"Entrada :\", decode_ids(start))\n",
        "print(\"Sa√≠da   :\", decode_ids(generated_ids))\n"
      ],
      "metadata": {
        "id": "Yg2GoeWvoAyt"
      },
      "id": "Yg2GoeWvoAyt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Conclus√£o\n",
        "\n",
        "Voc√™ acabou de implementar um GPT m√≠nimo com:\n",
        "\n",
        "- Embeddings (token + posi√ß√£o)\n",
        "- Self-attention com m√°scara causal\n",
        "- MLP (feedforward)\n",
        "- Residual + LayerNorm\n",
        "- Empilhamento de blocos Transformer\n",
        "- Head de sa√≠da para prever o pr√≥ximo token\n",
        "- Treinamento simples com cross-entropy\n",
        "- Gera√ß√£o autoregressiva\n",
        "\n",
        "Esse modelo √© pequeno, mas tem os mesmos princ√≠pios estruturais dos GPTs reais.\n",
        "\n",
        "No pr√≥ximo cap√≠tulo, vamos focar em **treinamento de verdade**:\n",
        "- loss em sequ√™ncia completa\n",
        "- batching melhor\n",
        "- avalia√ß√£o\n",
        "- melhorias de amostragem\n"
      ],
      "metadata": {
        "id": "Z8ppfn4roCnF"
      },
      "id": "Z8ppfn4roCnF"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}