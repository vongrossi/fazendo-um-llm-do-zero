# Infogr√°ficos ‚Äî Cap√≠tulo 05: Pr√©-Treinamento e Gera√ß√£o

Este diret√≥rio cont√©m os infogr√°ficos utilizados no Cap√≠tulo 05 da s√©rie
**Fazendo um LLM do Zero**.

Neste cap√≠tulo exploramos como modelos GPT aprendem linguagem e como geram
texto ap√≥s o treinamento.

Os infogr√°ficos deste cap√≠tulo ajudam a visualizar:
- o pipeline completo de treinamento
- como a fun√ß√£o de perda mede erros
- o ciclo iterativo de aprendizado (loop)
- estrat√©gias de gera√ß√£o de texto (decoding)
- persist√™ncia de modelos (checkpoints)

---

## üìä Lista de Infogr√°ficos do Cap√≠tulo

### 01 ‚Äî Pipeline de treinamento
**Arquivo:** `01-pipeline-treinamento.svg`  
**Se√ß√£o do di√°rio:** *O pipeline de treinamento de um LLM*  

Mostra o fluxo completo: Dataset ‚Üí Tokeniza√ß√£o ‚Üí Forward Pass ‚Üí Loss ‚Üí
Backpropagation ‚Üí Atualiza√ß√£o de Pesos.

---

### 02 ‚Äî Cross Entropy
**Arquivo:** `02-cross-entropy.svg`  
**Se√ß√£o do di√°rio:** *Cross Entropy: medindo o erro probabil√≠stico*  

Ilustra como a fun√ß√£o de perda compara a distribui√ß√£o prevista pelo modelo
com o token real para calcular o erro.

---

### 03 ‚Äî Loop de treinamento
**Arquivo:** `03-loop-treinamento.svg`  
**Se√ß√£o do di√°rio:** *O loop de treinamento completo*  

Visualiza o ciclo iterativo de batches e epochs que permite ao modelo
aprender gradualmente com os dados.

---

### 04 ‚Äî Estrat√©gias de decoding
**Arquivo:** `04-decoding-strategies.svg`  
**Se√ß√£o do di√°rio:** *Estrat√©gias de gera√ß√£o de texto*  

Compara m√©todos de gera√ß√£o como Greedy, Temperature, Top-k e Nucleus Sampling,
mostrando como afetam criatividade e coer√™ncia.

---

### 05 ‚Äî Checkpoints
**Arquivo:** `05-checkpoints.svg`  
**Se√ß√£o do di√°rio:** *Salvando modelos: checkpoints*  

Explica a import√¢ncia de salvar estados intermedi√°rios do modelo para
retomada de treino e reutiliza√ß√£o posterior.

---

## üé® Diretrizes visuais

Todos os infogr√°ficos devem:
- manter consist√™ncia visual entre si
- usar cores suaves e profissionais
- evitar excesso de texto
- priorizar leitura r√°pida em artigos t√©cnicos (Medium / Dev.to)