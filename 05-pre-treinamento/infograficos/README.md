
# InfogrÃ¡ficos â€” CapÃ­tulo 05 (PrÃ©-Treinamento e GeraÃ§Ã£o de Texto)

Este diretÃ³rio contÃ©m os infogrÃ¡ficos utilizados no CapÃ­tulo 05 da sÃ©rie:

**Fazendo um LLM do Zero**

Neste capÃ­tulo exploramos como modelos GPT aprendem linguagem e como geram texto apÃ³s o treinamento.

Os infogrÃ¡ficos deste capÃ­tulo tÃªm como objetivo ajudar o leitor a visualizar:

- como funciona o pipeline de treinamento de um LLM
- como a funÃ§Ã£o de perda mede qualidade do modelo
- como ocorre o loop de treinamento
- como diferentes estratÃ©gias influenciam geraÃ§Ã£o de texto
- como checkpoints permitem continuidade e reutilizaÃ§Ã£o de modelos

---

## ğŸ¯ EstratÃ©gia pedagÃ³gica dos infogrÃ¡ficos

Os infogrÃ¡ficos seguem a progressÃ£o:

```

Treinamento â†’ AvaliaÃ§Ã£o â†’ GeraÃ§Ã£o â†’ PersistÃªncia de Modelos

```

Essa progressÃ£o acompanha exatamente a narrativa do capÃ­tulo.

---

## ğŸ“Š Lista de InfogrÃ¡ficos

```

01-pipeline-treinamento.png
02-cross-entropy.png
03-loop-treinamento.png
04-decoding-strategies.png
05-checkpoints.png

```

---

## ğŸ§­ DescriÃ§Ã£o pedagÃ³gica de cada infogrÃ¡fico

---

### 01 â€” Pipeline de Treinamento de um LLM

ğŸ“ SeÃ§Ã£o do diÃ¡rio:
O pipeline de treinamento de um LLM

ğŸ¯ Objetivo didÃ¡tico:

Mostrar o fluxo completo do treinamento:

- Dataset de texto
- TokenizaÃ§Ã£o
- Forward pass no modelo
- CÃ¡lculo da loss
- Backpropagation
- AtualizaÃ§Ã£o dos pesos

Este infogrÃ¡fico apresenta a visÃ£o geral do aprendizado do modelo.

---

### 02 â€” Cross Entropy e AvaliaÃ§Ã£o ProbabilÃ­stica

ğŸ“ SeÃ§Ã£o do diÃ¡rio:
Cross entropy: medindo o erro probabilÃ­stico

ğŸ¯ Objetivo didÃ¡tico:

Mostrar como a cross entropy mede o erro do modelo:

- distribuiÃ§Ã£o real do token correto
- distribuiÃ§Ã£o prevista pelo modelo
- comparaÃ§Ã£o entre probabilidades
- interpretaÃ§Ã£o intuitiva da funÃ§Ã£o de perda

---

### 03 â€” Loop Completo de Treinamento

ğŸ“ SeÃ§Ã£o do diÃ¡rio:
O loop de treinamento completo

ğŸ¯ Objetivo didÃ¡tico:

Mostrar o ciclo iterativo do treinamento:

- batches
- forward pass
- cÃ¡lculo da loss
- backward pass
- atualizaÃ§Ã£o do modelo
- epochs

Este infogrÃ¡fico enfatiza repetiÃ§Ã£o e aprendizado gradual.

---

### 04 â€” EstratÃ©gias de GeraÃ§Ã£o de Texto

ğŸ“ SeÃ§Ã£o do diÃ¡rio:
EstratÃ©gias de geraÃ§Ã£o de texto

ğŸ¯ Objetivo didÃ¡tico:

Comparar diferentes formas de selecionar o prÃ³ximo token:

- greedy decoding
- temperature sampling
- top-k sampling
- nucleus sampling (top-p)

Este infogrÃ¡fico ajuda a entender como controlamos criatividade e diversidade do modelo.

---

### 05 â€” Checkpoints e PersistÃªncia de Modelos

ğŸ“ SeÃ§Ã£o do diÃ¡rio:
Salvando modelos: checkpoints

ğŸ¯ Objetivo didÃ¡tico:

Mostrar como modelos sÃ£o salvos durante treinamento:

- salvamento periÃ³dico de pesos
- retomada de treinamento
- reutilizaÃ§Ã£o de modelos
- compartilhamento e reprodutibilidade

---

## ğŸ¨ Diretrizes Visuais

Todos os infogrÃ¡ficos devem manter consistÃªncia visual com a sÃ©rie:

âœ” Estilo tÃ©cnico educacional  
âœ” Paleta de cores neutra e profissional  
âœ” Tipografia moderna e legÃ­vel  
âœ” Uso mÃ­nimo de texto  
âœ” ÃŠnfase em fluxogramas e diagramas conceituais  
âœ” ConsistÃªncia com capÃ­tulos anteriores  

---

## ğŸ§© ConvenÃ§Ã£o de Nome dos Arquivos

Os arquivos devem seguir o padrÃ£o:

```

XX-nome-do-conceito.png

```

Exemplo:

```

01-pipeline-treinamento.png

```

---

