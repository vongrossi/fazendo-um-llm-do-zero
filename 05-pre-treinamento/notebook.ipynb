{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cap√≠tulo 05 ‚Äî Pr√©-Treinamento: O Nascimento da Intelig√™ncia\n",
    "\n",
    "Neste cap√≠tulo, vamos tirar o nosso GPTMini da in√©rcia. Vamos aliment√°-lo com dados e observar o momento exato em que ele deixa de escolher letras aleat√≥rias e come√ßa a formar palavras e frases.\n",
    "\n",
    "--- \n",
    "### üè≠ A F√°brica de Predi√ß√£o\n",
    "O pr√©-treinamento ensina ao modelo a base da nossa l√≠ngua. Ele aprende que depois de \"o g\" provavelmente vem um \"a\", formando \"o ga...\".\n",
    "\n",
    "![Pipeline de Treinamento](./infograficos/01-pipeline-treinamento.png)"
   ],
   "metadata": { "id": "header" }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Setup do reposit√≥rio no Colab\n",
    "# ============================================================\n",
    "import os, sys\n",
    "REPO_NAME = \"fazendo-um-llm-do-zero\"\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if not os.path.exists(REPO_NAME):\n",
    "        get_ipython().system(f\"git clone https://github.com/vongrossi/{REPO_NAME}.git\")\n",
    "    if os.path.exists(REPO_NAME) and os.getcwd().split('/')[-1] != REPO_NAME:\n",
    "        os.chdir(REPO_NAME)\n",
    "if os.getcwd() not in sys.path: sys.path.append(os.getcwd())\n",
    "print(\"üìÇ Diret√≥rio atual:\", os.getcwd())"
   ],
   "metadata": { "id": "setup-repo" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Setup e Imports\n",
    "import os, sys, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lib.gptmini import GPTConfig, GPTMini\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "print(f\"‚úÖ Oficina pronta! Usando: {device}\")"
   ],
   "metadata": { "id": "setup" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. O Combust√≠vel: Texto com Estrutura\n",
    "\n",
    "Para que o modelo funcione nos cap√≠tulos seguintes, vamos inclu√≠r os s√≠mbolos `###`, `:`, `?` e quebras de linha no treino base."
   ],
   "metadata": { "id": "data-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "### comando:\n",
    "o que o gato fez?\n",
    "### resposta:\n",
    "o gato subiu no telhado e pulou o muro.\n",
    "\n",
    "### comando:\n",
    "onde o cachorro dormiu?\n",
    "### resposta:\n",
    "o cachorro dormiu no sofa e no tapete.\n",
    "\n",
    "inteligencia artificial e o estudo de algoritmos.\n",
    "machine learning permite que sistemas aprendam padroes.\n",
    "o gato dormiu no sofa enquanto o cachorro pulou o portao.\n",
    "\"\"\".strip().lower()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long).to(device)\n",
    "print(f\"Vocabul√°rio: {''.join(chars)} | Tamanho: {vocab_size}\")"
   ],
   "metadata": { "id": "data-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Treinamento: Memorizando Padr√µes\n",
    "\n",
    "O modelo olha para os √∫ltimos `context_size` caracteres para prever o pr√≥ximo."
   ],
   "metadata": { "id": "train-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "context_size = 32\n",
    "def get_batch(data, batch_size=16):\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "config = GPTConfig(vocab_size=vocab_size, context_size=context_size, d_model=128, n_heads=4, n_layers=3)\n",
    "model = GPTMini(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_history = []\n",
    "print(\"üî® Treinando o modelo...\")\n",
    "model.train()\n",
    "for step in range(1501):\n",
    "    xb, yb = get_batch(data)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_history.append(loss.item())\n",
    "    if step % 500 == 0: print(f\"Step {step:04d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(loss_history, color='#1A73E8')\n",
    "plt.title(\"Aprendizado do Modelo (Loss)\")\n",
    "plt.show()"
   ],
   "metadata": { "id": "train-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Teste de Gera√ß√£o\n",
    "\n",
    "Agora o modelo deve completar frases de forma minimamente coerente."
   ],
   "metadata": { "id": "gen-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start_str, max_new=100):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(encode(start_str)).unsqueeze(0).to(device)\n",
    "    for _ in range(max_new):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "        if itos[next_id.item()] == \".\": break\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "print(\"‚ú® TESTE DE COMPLETUDE:\")\n",
    "print(generate(model, \"### comando:\\no que o gato fez?\"))"
   ],
   "metadata": { "id": "test-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Persist√™ncia: Salvando a Intelig√™ncia\n",
    "\n",
    "O arquivo `gpt_checkpoint.pt` ser√° usado nos cap√≠tulos seguintes.\n",
    "\n",
    "![Checkpoints](./infograficos/05-checkpoints.png)"
   ],
   "metadata": { "id": "ckpt-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "checkpoint = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"stoi\": stoi, \"itos\": itos, \"config\": config\n",
    "}\n",
    "torch.save(checkpoint, \"gpt_checkpoint.pt\")\n",
    "print(\"‚úÖ Checkpoint completo salvo!\")\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import files\n",
    "    files.download(\"gpt_checkpoint.pt\")"
   ],
   "metadata": { "id": "save-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üèÅ Conclus√£o\n",
    "\n",
    "Voc√™ acabou de ensinar um GPT a aprender linguagem. No pr√≥ximo cap√≠tulo, vamos aprender como fazer o **Fine-Tuning** para tarefas espec√≠ficas."
   ],
   "metadata": { "id": "conclusion" }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}