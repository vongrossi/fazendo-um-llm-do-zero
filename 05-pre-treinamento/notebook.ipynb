{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cap√≠tulo 05 ‚Äî Pr√©-Treinamento: O Nascimento da Intelig√™ncia\n",
    "\n",
    "Neste cap√≠tulo, vamos tirar o nosso GPTMini da in√©rcia. Vamos aliment√°-lo com dados e observar o momento exato em que ele deixa de escolher palavras aleat√≥rias e come√ßa a entender os padr√µes da nossa l√≠ngua.\n",
    "\n",
    "--- \n",
    "### üè≠ A F√°brica de Predi√ß√£o\n",
    "O pr√©-treinamento √© a etapa mais pesada. √â aqui que o modelo aprende a gram√°tica, o vocabul√°rio e a l√≥gica do mundo atrav√©s da predi√ß√£o do pr√≥ximo token.\n",
    "\n",
    "![Pipeline de Treinamento](./infograficos/01-pipeline-treinamento.png)"
   ],
   "metadata": { "id": "header" }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Setup do reposit√≥rio no Colab\n",
    "# ============================================================\n",
    "import os, sys\n",
    "REPO_NAME = \"fazendo-um-llm-do-zero\"\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if not os.path.exists(REPO_NAME):\n",
    "        os.system(f\"git clone https://github.com/vongrossi/{REPO_NAME}.git\")\n",
    "    if os.path.exists(REPO_NAME) and os.getcwd().split('/')[-1] != REPO_NAME:\n",
    "        os.chdir(REPO_NAME)\n",
    "if os.getcwd() not in sys.path: sys.path.append(os.getcwd())\n",
    "print(\"üìÇ Diret√≥rio atual:\", os.getcwd())"
   ],
   "metadata": { "id": "setup-repo" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip -q install -r 05-pre-treinamento/requirements.txt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math, random, numpy as np\n",
    "from lib.gptmini import GPTConfig, GPTMini\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚úÖ Oficina pronta! Usando: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ],
   "metadata": { "id": "setup-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Prepara√ß√£o do Dataset\n",
    "\n",
    "Vamos usar um conjunto de frases curtas para que o treinamento seja r√°pido e did√°tico."
   ],
   "metadata": { "id": "data-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "o gato subiu no telhado\n",
    "o cachorro subiu no sofa\n",
    "o gato dormiu no sofa\n",
    "o cachorro dormiu no tapete\n",
    "o gato pulou no muro\n",
    "o cachorro pulou o portao\n",
    "\"\"\".strip().lower()\n",
    "\n",
    "tokens = text.split()\n",
    "vocab = sorted(set(tokens))\n",
    "stoi = {t:i for i,t in enumerate(vocab)}\n",
    "itos = {i:t for t,i in stoi.items()}\n",
    "vocab_size = len(vocab)\n",
    "encoded = [stoi[t] for t in tokens]\n",
    "\n",
    "def build_dataset(token_ids, context_size=5):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(token_ids) - context_size):\n",
    "        X.append(token_ids[i : i + context_size])\n",
    "        Y.append(token_ids[i + 1 : i + context_size + 1])\n",
    "    return torch.tensor(X).to(device), torch.tensor(Y).to(device)\n",
    "\n",
    "context_size = 5\n",
    "X, Y = build_dataset(encoded, context_size)\n",
    "print(f\"Vocabul√°rio: {vocab_size} palavras | Amostras: {len(X)}\")"
   ],
   "metadata": { "id": "data-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Otimiza√ß√£o e Treinamento\n",
    "\n",
    "Agora vamos instanciar o modelo e treinar usando a **Cross Entropy** em toda a sequ√™ncia.\n",
    "\n",
    "![Loop Treino](./infograficos/03-loop-treinamento.png)"
   ],
   "metadata": { "id": "loop-treino-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "config = GPTConfig(vocab_size=vocab_size, context_size=context_size, d_model=64, n_heads=4, n_layers=2)\n",
    "model = GPTMini(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loss_history = []\n",
    "print(\"üöÄ Iniciando Treinamento...\")\n",
    "model.train()\n",
    "for step in range(601):\n",
    "    idx = torch.randint(0, X.size(0), (16,))\n",
    "    xb, yb = X[idx], Y[idx]\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss_history.append(loss.item())\n",
    "    if step % 100 == 0: print(f\"Step {step:03d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(train_loss_history, color='#1A73E8')\n",
    "plt.title(\"Curva de Aprendizado (Cross Entropy Loss)\")\n",
    "plt.show()"
   ],
   "metadata": { "id": "train-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Estrat√©gias de Gera√ß√£o (Decoding)\n",
    "\n",
    "Aqui definimos as fun√ß√µes que permitem ao modelo escolher os pr√≥ximos tokens.\n",
    "\n",
    "![Decoding Strategies](./infograficos/04-decoding-strategies.png)"
   ],
   "metadata": { "id": "decoding-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "def encode_text(s): return [stoi[t] for t in s.lower().split() if t in stoi]\n",
    "def decode(ids): return \" \".join(itos[int(i)] for i in ids)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, start_str, max_new=10):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(encode_text(start_str)).unsqueeze(0).to(device)\n",
    "    for _ in range(max_new):\n",
    "        logits, _ = model(idx[:, -context_size:])\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "print(\"‚ú® TESTE DE GERA√á√ÉO:\")\n",
    "print(f\"Entrada: 'o gato' -> Sa√≠da: '{generate(model, 'o gato')}'\")"
   ],
   "metadata": { "id": "gen-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Persist√™ncia: Salvando a Intelig√™ncia\n",
    "\n",
    "**O arquivo `gpt_checkpoint.pt` √© o \"c√©rebro\" que acabamos de treinar.** \n",
    "\n",
    "Voc√™ precisar√° dele para os pr√≥ximos passos (Cap√≠tulos 06 e 07).\n",
    "\n",
    "![Checkpoints](./infograficos/05-checkpoints.png)"
   ],
   "metadata": { "id": "ckpt-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "checkpoint = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"stoi\": stoi,\n",
    "    \"itos\": itos,\n",
    "    \"config\": config\n",
    "}\n",
    "torch.save(checkpoint, \"gpt_checkpoint.pt\")\n",
    "print(\"‚úÖ Checkpoint salvo com sucesso!\")\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import files\n",
    "    files.download(\"gpt_checkpoint.pt\")"
   ],
   "metadata": { "id": "save-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üèÅ Conclus√£o\n",
    "\n",
    "Voc√™ acabou de ensinar um GPT a aprender linguagem. Viu como ele sai do caos aleat√≥rio para a ordem gramatical e como podemos controlar sua criatividade.\n",
    "\n",
    "No pr√≥ximo cap√≠tulo, vamos levar este modelo para a **especializa√ß√£o**, aprendendo como fazer o **Fine-Tuning** para tarefas espec√≠ficas como classifica√ß√£o de sentimentos e detec√ß√£o de spam."
   ],
   "metadata": { "id": "conclusion" }
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.12.3" }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}