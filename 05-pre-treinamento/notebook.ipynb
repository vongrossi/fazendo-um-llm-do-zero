{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cap√≠tulo 05 ‚Äî Pr√©-Treinamento: O Nascimento da Intelig√™ncia\n",
    "\n",
    "Neste cap√≠tulo, vamos tirar o nosso GPTMini da in√©rcia. Vamos aliment√°-lo com dados e observar o momento exato em que ele deixa de escolher palavras aleat√≥rias e come√ßa a entender os padr√µes da nossa l√≠ngua.\n",
    "\n",
    "--- \n",
    "### üè≠ A F√°brica de Predi√ß√£o\n",
    "O pr√©-treinamento √© a etapa mais pesada. √â aqui que o modelo aprende a gram√°tica, o vocabul√°rio e a l√≥gica do mundo atrav√©s da predi√ß√£o do pr√≥ximo token.\n",
    "\n",
    "![Pipeline de Treinamento](./infograficos/01-pipeline-treinamento.png)"
   ],
   "metadata": { "id": "header" }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setup e Configura√ß√£o"
   ],
   "metadata": { "id": "setup-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Setup do reposit√≥rio no Colab\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "\n",
    "REPO_URL = \"https://github.com/vongrossi/fazendo-um-llm-do-zero.git\"\n",
    "REPO_DIR = \"fazendo-um-llm-do-zero\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL}\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "sys.path.append(os.getcwd())\n",
    "print(\"Diret√≥rio atual:\", os.getcwd())\n"
   ],
   "metadata": { "id": "HTlrWocmIAF7" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip -q install -r 05-pre-treinamento/requirements.txt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from lib.gptmini import GPTConfig, GPTMini\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚úÖ Oficina pronta! Usando: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ],
   "metadata": { "id": "fuM-5y7IIGdp" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Prepara√ß√£o do Dataset\n",
    "\n",
    "Vamos usar um pequeno conjunto de frases sobre gatos e cachorros para que possamos monitorar o aprendizado de forma r√°pida e clara."
   ],
   "metadata": { "id": "FgIQByULIqm9" }
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "o gato subiu no telhado\n",
    "o cachorro subiu no sofa\n",
    "o gato dormiu no sofa\n",
    "o cachorro dormiu no tapete\n",
    "o gato pulou no muro\n",
    "o cachorro pulou o portao\n",
    "\"\"\".strip().lower()\n",
    "\n",
    "tokens = text.split()\n",
    "vocab = sorted(set(tokens))\n",
    "stoi = {t:i for i,t in enumerate(vocab)}\n",
    "itos = {i:t for t,i in stoi.items()}\n",
    "vocab_size = len(vocab)\n",
    "encoded = [stoi[t] for t in tokens]\n",
    "\n",
    "def build_dataset(token_ids, context_size=5):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(token_ids) - context_size):\n",
    "        X.append(token_ids[i : i + context_size])\n",
    "        Y.append(token_ids[i + 1 : i + context_size + 1])\n",
    "    return torch.tensor(X).to(device), torch.tensor(Y).to(device)\n",
    "\n",
    "context_size = 5\n",
    "X, Y = build_dataset(encoded)\n",
    "print(f\"Vocabul√°rio: {vocab_size} palavras | Amostras: {len(X)}\")"
   ],
   "metadata": { "id": "wG0YlI24ItcC" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. O Estado do Caos (Antes do Treino)\n",
    "\n",
    "Vamos ver o que o modelo gera agora, com seus pesos puramente aleat√≥rios."
   ],
   "metadata": { "id": "before-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "config = GPTConfig(vocab_size=vocab_size, context_size=5, d_model=64, n_heads=4, n_layers=2)\n",
    "model = GPTMini(config).to(device)\n",
    "\n",
    "def generate(model, start_text, max_new=5):\n",
    "    model.eval()\n",
    "    tokens = [stoi[t] for t in start_text.split() if t in stoi]\n",
    "    idx = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "    for _ in range(max_new):\n",
    "        logits, _ = model(idx[:, -5:])\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return \" \".join([itos[i.item()] for i in idx[0]])\n",
    "\n",
    "print(\"üé≤ Gera√ß√£o Aleat√≥ria:\")\n",
    "print(generate(model, \"o gato\"))"
   ],
   "metadata": { "id": "before-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. O Loop de Treinamento\n",
    "\n",
    "Agora vamos ensinar o modelo atrav√©s da **Cross Entropy**. Ele vai comparar sua predi√ß√£o com o alvo real e ajustar seus pesos via **Backpropagation**.\n",
    "\n",
    "![Loop Treino](./infograficos/03-loop-treinamento.png)"
   ],
   "metadata": { "id": "train-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss_history = []\n",
    "\n",
    "print(\"üî® Treinando o c√©rebro do modelo...\")\n",
    "model.train()\n",
    "for step in range(501):\n",
    "    idx = torch.randint(0, X.size(0), (8,))\n",
    "    xb, yb = X[idx], Y[idx]\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_history.append(loss.item())\n",
    "    if step % 100 == 0: print(f\"Step {step:03d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(loss_history, color='#1A73E8')\n",
    "plt.title(\"Curva de Aprendizado (Cross Entropy Loss)\")\n",
    "plt.show()"
   ],
   "metadata": { "id": "train-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Ordem a partir do Caos (Depois do Treino)\n",
    "\n",
    "Agora o modelo j√° deve ser capaz de completar frases sobre os animais do nosso dataset de forma coerente."
   ],
   "metadata": { "id": "after-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"‚ú® Gera√ß√£o Treinada:\")\n",
    "print(generate(model, \"o gato\"))\n",
    "print(generate(model, \"o cachorro\"))"
   ],
   "metadata": { "id": "after-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Estrat√©gias de Gera√ß√£o Avan√ßadas\n",
    "\n",
    "O modelo n√£o precisa ser sempre \"rob√≥tico\". Podemos usar a **Temperatura** para torn√°-lo mais criativo ou o **Top-P** para focar apenas nas melhores op√ß√µes.\n",
    "\n",
    "![Decoding Strategies](./infograficos/04-decoding-strategies.png)"
   ],
   "metadata": { "id": "decoding-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def generate_creative(model, start_text, temp=1.0):\n",
    "    model.eval()\n",
    "    tokens = [stoi[t] for t in start_text.split() if t in stoi]\n",
    "    idx = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "    for _ in range(5):\n",
    "        logits, _ = model(idx[:, -5:])\n",
    "        logits = logits[:, -1, :] / temp\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return \" \".join([itos[i.item()] for i in idx[0]])\n",
    "\n",
    "print(\"üå°Ô∏è Baixa Temperatura (Focado):\")\n",
    "print(generate_creative(model, \"o gato\", temp=0.1))\n",
    "\n",
    "print(\"\\nüî• Alta Temperatura (Aleat√≥rio/Criativo):\")\n",
    "print(generate_creative(model, \"o gato\", temp=1.5))"
   ],
   "metadata": { "id": "creative-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Persist√™ncia: Salvando o Checkpoint\n",
    "\n",
    "Para n√£o perdermos o progresso, salvamos o estado das matrizes de pesos.\n",
    "\n",
    "![Checkpoints](./infograficos/05-checkpoints.png)"
   ],
   "metadata": { "id": "ckpt-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(model.state_dict(), \"gpt_checkpoint.pt\")\n",
    "print(\"‚úÖ Checkpoint salvo como 'gpt_checkpoint.pt'!\")"
   ],
   "metadata": { "id": "ckpt-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üèÅ Conclus√£o\n",
    "\n",
    "Voc√™ acabou de ensinar um GPT a aprender linguagem. Viu como ele sai do caos aleat√≥rio para a ordem gramatical e como podemos controlar sua criatividade.\n",
    "\n",
    "No pr√≥ximo cap√≠tulo, vamos levar este modelo para a **especializa√ß√£o**, aprendendo como fazer o **Fine-Tuning** para tarefas espec√≠ficas como classifica√ß√£o de sentimentos e detec√ß√£o de spam."
   ],
   "metadata": { "id": "conclusion" }
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.12.3" }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}