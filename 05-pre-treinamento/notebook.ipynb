{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cap√≠tulo 05 ‚Äî Pr√©-Treinamento: O Nascimento da Intelig√™ncia\n",
    "\n",
    "Neste cap√≠tulo, vamos tirar o nosso GPTMini da in√©rcia. Vamos aliment√°-lo com dados e observar o momento exato em que ele deixa de escolher letras aleat√≥rias e come√ßa a formar palavras e frases.\n",
    "\n",
    "--- \n",
    "### üè≠ A F√°brica de Predi√ß√£o\n",
    "O pr√©-treinamento ensina ao modelo a base da nossa l√≠ngua. Ele aprende que depois de \"o g\" provavelmente vem um \"a\", formando \"o ga...\".\n",
    "\n",
    "![Pipeline de Treinamento](./infograficos/01-pipeline-treinamento.png)"
   ],
   "metadata": { "id": "header" }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Setup do reposit√≥rio no Colab\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "\n",
    "REPO_NAME = \"fazendo-um-llm-do-zero\"\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if not os.path.exists(REPO_NAME):\n",
    "        !git clone https://github.com/vongrossi/{REPO_NAME}.git\n",
    "    \n",
    "    if os.path.exists(REPO_NAME) and os.getcwd().split('/')[-1] != REPO_NAME:\n",
    "        os.chdir(REPO_NAME)\n",
    "\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "print(\"üìÇ Diret√≥rio atual:\", os.getcwd())"
   ],
   "metadata": { "id": "setup-repo" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Setup e Imports\n",
    "import os, sys, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lib.gptmini import GPTConfig, GPTMini\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "print(f\"‚úÖ Oficina pronta! Usando: {device}\")"
   ],
   "metadata": { "id": "setup" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. O Combust√≠vel: Tokeniza√ß√£o por Caracteres\n",
    "\n",
    "Para nosso modelo \"Mini\", vamos tratar cada letra e sinal de pontua√ß√£o como um token √∫nico."
   ],
   "metadata": { "id": "data-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "o gato subiu no telhado e pulou o muro.\n",
    "o cachorro subiu no sofa e dormiu no tapete.\n",
    "o gato dormiu no sofa enquanto o cachorro pulou o portao.\n",
    "inteligencia artificial e o estudo de algoritmos que aprendem.\n",
    "machine learning permite que sistemas aprendam padroes de dados.\n",
    "\"\"\".strip().lower()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long).to(device)\n",
    "print(f\"Vocabul√°rio: {''.join(chars)} | Tamanho: {vocab_size}\")"
   ],
   "metadata": { "id": "data-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Preparando a Janela de Contexto\n",
    "\n",
    "O modelo olha para os √∫ltimos `context_size` caracteres para prever o pr√≥ximo."
   ],
   "metadata": { "id": "batch-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "context_size = 32\n",
    "def get_batch(data, batch_size=16):\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(data)\n",
    "print(\"Exemplo de Entrada (X):\", decode(xb[0].tolist()))\n",
    "print(\"Exemplo de Alvo (Y)   :\", decode(yb[0].tolist()))"
   ],
   "metadata": { "id": "batch-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Treinamento: Da Aleatoriedade √† L√≥gica\n",
    "\n",
    "Vamos treinar o modelo e salvar o hist√≥rico de erro (Loss)."
   ],
   "metadata": { "id": "train-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "config = GPTConfig(vocab_size=vocab_size, context_size=context_size, d_model=128, n_heads=4, n_layers=3)\n",
    "model = GPTMini(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_history = []\n",
    "print(\"üî® Treinando...\")\n",
    "for step in range(1001):\n",
    "    xb, yb = get_batch(data)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_history.append(loss.item())\n",
    "    if step % 200 == 0: print(f\"Step {step:04d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(loss_history, color='#1A73E8')\n",
    "plt.title(\"Aprendizado do Modelo (Loss)\")\n",
    "plt.show()"
   ],
   "metadata": { "id": "train-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Teste de Gera√ß√£o e Salvamento\n",
    "\n",
    "Agora o modelo deve completar frases de forma minimamente coerente."
   ],
   "metadata": { "id": "gen-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start_str, max_new=50):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(encode(start_str)).unsqueeze(0).to(device)\n",
    "    for _ in range(max_new):\n",
    "        logits, _ = model(idx[:, -context_size:])\n",
    "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "print(\"‚ú® Resultado:\", generate(model, \"o gato \"))\n",
    "\n",
    "# SALVAMENTO CRUCIAL: Pesos + Dicion√°rio\n",
    "checkpoint = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"stoi\": stoi,\n",
    "    \"itos\": itos,\n",
    "    \"config\": config\n",
    "}\n",
    "torch.save(checkpoint, \"gpt_checkpoint.pt\")\n",
    "print(\"‚úÖ Checkpoint completo salvo!\")"
   ],
   "metadata": { "id": "save-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üèÅ Conclus√£o\n",
    "\n",
    "Voc√™ acabou de ensinar um GPT a aprender linguagem. Viu como ele sai do caos aleat√≥rio para a ordem gramatical e como podemos controlar sua criatividade.\n",
    "\n",
    "No pr√≥ximo cap√≠tulo, vamos levar este modelo para a **especializa√ß√£o**, aprendendo como fazer o **Fine-Tuning** para tarefas espec√≠ficas como classifica√ß√£o de sentimentos e detec√ß√£o de spam."
   ],
   "metadata": { "id": "conclusion" }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}