{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cap√≠tulo 05 ‚Äî Pr√©-Treinamento e Gera√ß√£o de Texto\n",
    "\n",
    "Este notebook acompanha o Cap√≠tulo 05 da s√©rie **Fazendo um LLM do Zero**.\n",
    "\n",
    "Neste notebook vamos ensinar o GPTMini a aprender linguagem.\n",
    "\n",
    "üéØ **Objetivos deste notebook:**\n",
    "- Como calcular loss probabil√≠stica\n",
    "- Como funciona o loop de treinamento\n",
    "- Como monitorar aprendizado\n",
    "- Como gerar texto com diferentes estrat√©gias (Greedy, Temperature, Top-K, Top-P)\n",
    "- Como salvar e carregar modelos\n"
   ],
   "metadata": {
    "id": "J8c7D-gBIBI3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setup e Configura√ß√£o"
   ],
   "metadata": {
    "id": "setup-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Setup do reposit√≥rio no Colab\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "\n",
    "REPO_URL = \"https://github.com/vongrossi/fazendo-um-llm-do-zero.git\"\n",
    "REPO_DIR = \"fazendo-um-llm-do-zero\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL}\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "sys.path.append(os.getcwd())\n",
    "print(\"Diret√≥rio atual:\", os.getcwd())\n"
   ],
   "metadata": {
    "id": "HTlrWocmIAF7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip -q install -r 05-pre-treinamento/requirements.txt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from lib.gptmini import GPTConfig, GPTMini\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"‚úÖ Device:\", device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ],
   "metadata": {
    "id": "fuM-5y7IIGdp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Prepara√ß√£o do Dataset\n",
    "\n",
    "Vamos usar um dataset de texto simples para observar o modelo aprendendo as transi√ß√µes de palavras."
   ],
   "metadata": {
    "id": "FgIQByULIqm9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "o gato subiu no telhado\n",
    "o cachorro subiu no sofa\n",
    "o gato dormiu no sofa\n",
    "o cachorro dormiu no tapete\n",
    "o gato pulou no muro\n",
    "\"\"\".strip().lower()\n",
    "\n",
    "tokens = text.split()\n",
    "vocab = sorted(set(tokens))\n",
    "\n",
    "stoi = {t:i for i,t in enumerate(vocab)}\n",
    "itos = {i:t for t,i in stoi.items()}\n",
    "\n",
    "encoded = [stoi[t] for t in tokens]\n",
    "\n",
    "def build_dataset(token_ids, context_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(token_ids) - context_size):\n",
    "        x = token_ids[i : i + context_size]\n",
    "        y = token_ids[i + 1 : i + context_size + 1]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return torch.tensor(X, dtype=torch.long), torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "context_size = 5\n",
    "X, Y = build_dataset(encoded, context_size)\n",
    "\n",
    "# Split Treino / Valida√ß√£o\n",
    "N = X.size(0)\n",
    "perm = torch.randperm(N)\n",
    "split = int(0.85 * N)\n",
    "train_idx = perm[:split]\n",
    "val_idx = perm[split:]\n",
    "\n",
    "X_train, Y_train = X[train_idx].to(device), Y[train_idx].to(device)\n",
    "X_val, Y_val = X[val_idx].to(device), Y[val_idx].to(device)\n"
   ],
   "metadata": {
    "id": "wG0YlI24ItcC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Otimiza√ß√£o e Treinamento\n",
    "\n",
    "Agora vamos instanciar o modelo e treinar usando a **Cross Entropy** em toda a sequ√™ncia."
   ],
   "metadata": {
    "id": "loop-treino-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=len(vocab),\n",
    "    context_size=context_size,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2\n",
    ")\n",
    "\n",
    "model = GPTMini(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_val_loss():\n",
    "    model.eval()\n",
    "    logits, _ = model(X_val)\n",
    "    B, T, V = logits.shape\n",
    "    loss = F.cross_entropy(logits.view(-1, V), Y_val.view(-1))\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "print(\"üöÄ Iniciando Treinamento...\")\n",
    "model.train()\n",
    "for step in range(601):\n",
    "    idx = torch.randint(0, X_train.size(0), (16,), device=device)\n",
    "    xb, yb = X_train[idx], Y_train[idx]\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss_history.append(loss.item())\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        vloss = eval_val_loss()\n",
    "        val_loss_history.append((step, vloss))\n",
    "        print(f\"Step {step:03d} | Train Loss: {loss.item():.4f} | Val Loss: {vloss:.4f}\")\n"
   ],
   "metadata": {
    "id": "hKekxcqFJS6E"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Visualiza√ß√£o de Performance\n",
    "\n",
    "Vamos observar como a Loss caiu durante o treinamento."
   ],
   "metadata": {
    "id": "plot-loss-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot do training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_loss_history, label=\"Train Loss\", alpha=0.6)\n",
    "if val_loss_history:\n",
    "    steps_v, losses_v = zip(*val_loss_history)\n",
    "    plt.scatter(steps_v, losses_v, color='red', label=\"Val Loss\")\n",
    "plt.title(\"Evolu√ß√£o do Treinamento\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "BC9FBiAJJZzg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Estrat√©gias de Gera√ß√£o (Decoding)\n",
    "\n",
    "Aqui definimos as fun√ß√µes que permitem ao modelo escolher os pr√≥ximos tokens."
   ],
   "metadata": {
    "id": "decoding-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def encode_text(s):\n",
    "    return [stoi[t] for t in s.lower().split() if t in stoi]\n",
    "\n",
    "def decode(ids):\n",
    "    return \" \".join(itos[int(i)] for i in ids)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_greedy(start_tokens, max_new_tokens=10):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(start_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, _ = model(idx[:, -context_size:])\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return idx.squeeze(0).tolist()\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_temperature(start_tokens, max_new_tokens=10, temperature=1.0):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(start_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, _ = model(idx[:, -context_size:])\n",
    "        logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return idx.squeeze(0).tolist()\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_top_k(start_tokens, max_new_tokens=10, temperature=1.0, k=5):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(start_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, _ = model(idx[:, -context_size:])\n",
    "        logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "        v, _ = torch.topk(logits, min(k, logits.size(-1)))\n",
    "        logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return idx.squeeze(0).tolist()\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_top_p(start_tokens, max_new_tokens=10, temperature=1.0, p=0.9):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(start_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, _ = model(idx[:, -context_size:])\n",
    "        logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > p\n",
    "        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "        sorted_indices_to_remove[:, 0] = False\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = -float('Inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return idx.squeeze(0).tolist()\n"
   ],
   "metadata": {
    "id": "decoding-functions"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Teste de Gera√ß√£o Comparativa\n",
    "\n",
    "Vamos ver como cada estrat√©gia se comporta com a mesma entrada."
   ],
   "metadata": {
    "id": "teste-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "start = encode_text(\"o gato\")\n",
    "print(\"Entrada:\", decode(start))\n",
    "print(\"-\" * 30)\n",
    "print(\"Greedy        :\", decode(generate_greedy(start, max_new_tokens=8)))\n",
    "print(\"Temperature 0.8:\", decode(generate_temperature(start, max_new_tokens=8, temperature=0.8)))\n",
    "print(\"Top-k (k=5)    :\", decode(generate_top_k(start, max_new_tokens=8, temperature=1.0, k=5)))\n",
    "print(\"Top-p (p=0.9)  :\", decode(generate_top_p(start, max_new_tokens=8, temperature=1.0, p=0.9)))\n"
   ],
   "metadata": {
    "id": "3me5ldEpKBY0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Persist√™ncia: Checkpoints\n",
    "\n",
    "Salvar o modelo permite que voc√™ o utilize em outros notebooks (como os de Fine-tuning)."
   ],
   "metadata": {
    "id": "checkpoint-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ckpt = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"stoi\": stoi,\n",
    "    \"itos\": itos,\n",
    "    \"config\": config\n",
    "}\n",
    "torch.save(ckpt, \"gpt_checkpoint.pt\")\n",
    "print(\"‚úÖ Checkpoint salvo com sucesso!\")\n"
   ],
   "metadata": {
    "id": "wt0ECP_TJz4V"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Conclus√£o\n",
    "\n",
    "Voc√™ acabou de ensinar um GPT a aprender linguagem.\n",
    "\n",
    "Voc√™ viu:\n",
    "- Como calcular cross entropy em sequ√™ncias\n",
    "- Como funciona o loop de treinamento iterativo\n",
    "- Como monitorar o aprendizado com gr√°ficos de Loss\n",
    "- Como controlar a criatividade da gera√ß√£o com Temperature e Nucleus Sampling\n",
    "\n",
    "No pr√≥ximo cap√≠tulo, vamos levar este modelo para a "
   ],
   "metadata": {
    "id": "1phtg9GkKS6i"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}