{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Setup do repositório no Colab\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/vongrossi/fazendo-um-llm-do-zero.git\"\n",
    "REPO_DIR = \"fazendo-um-llm-do-zero\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL}\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(\"Diretório atual:\", os.getcwd())\n"
   ],
   "metadata": {
    "id": "HTlrWocmIAF7"
   },
   "id": "HTlrWocmIAF7",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Garantir que o Python ache o diretório do projeto\n",
    "import sys, os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# ============================================================\n",
    "# Import do núcleo do GPTMini (reutilizado do Capítulo 04)\n",
    "# ============================================================\n",
    "# Premissa da série: evitar copiar e colar classes entre notebooks.\n",
    "# Se você ainda não criou lib/gptmini.py, crie antes de continuar.\n",
    "\n",
    "try:\n",
    "    from lib.gptmini import GPTConfig, GPTMini\n",
    "    print(\"✅ GPTMini importado de lib/gptmini.py\")\n",
    "except Exception as e:\n",
    "    raise ImportError(\n",
    "        \"Não foi possível importar 'lib.gptmini'.\\n\"\n",
    "        \"Verifique se o arquivo existe em: lib/gptmini.py\\n\"\n",
    "        \"e se você executou a célula de clone/cd do repositório.\"\n",
    "    ) from e\n"
   ],
   "metadata": {
    "id": "BYHBqNBZLmGJ"
   },
   "id": "BYHBqNBZLmGJ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Capítulo 05 — Pré-Treinamento e Geração de Texto\n",
    "\n",
    "Neste notebook vamos ensinar o GPTMini a aprender linguagem.\n",
    "\n",
    "Você verá:\n",
    "\n",
    "- Como calcular loss probabilística\n",
    "- Como funciona o loop de treinamento\n",
    "- Como monitorar aprendizado\n",
    "- Como gerar texto com diferentes estratégias\n",
    "- Como salvar e carregar modelos\n"
   ],
   "metadata": {
    "id": "J8c7D-gBIBI3"
   },
   "id": "J8c7D-gBIBI3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports e GPU config"
   ],
   "metadata": {
    "id": "98U2nbuXIIhm"
   },
   "id": "98U2nbuXIIhm"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "torch.manual_seed(42)\n"
   ],
   "metadata": {
    "id": "fuM-5y7IIGdp"
   },
   "id": "fuM-5y7IIGdp",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reaproveitando o GPTMini do Capítulo 04 (sem copiar código)\n",
    "\n",
    "Neste capítulo vamos focar em **treinamento e geração**.\n",
    "\n",
    "Para isso, reaproveitamos a implementação do GPTMini que está em `lib/gptmini.py`.\n",
    "Assim, os capítulos seguintes (fine-tuning, instruction tuning, etc.) também podem reutilizar o mesmo núcleo.\n"
   ],
   "metadata": {
    "id": "PMwUbSmiIjjl"
   },
   "id": "PMwUbSmiIjjl"
  },
  {
   "cell_type": "code",
   "source": [
    "# (Removido) Import antigo por caminho de notebook.\n",
    "# Agora usamos: from lib.gptmini import GPTConfig, GPTMini\n"
   ],
   "metadata": {
    "id": "WlM8BF-_Ik37"
   },
   "id": "WlM8BF-_Ik37",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Criando o Dataset"
   ],
   "metadata": {
    "id": "FgIQByULIqm9"
   },
   "id": "FgIQByULIqm9"
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "o gato subiu no telhado\n",
    "o cachorro subiu no sofa\n",
    "o gato dormiu no sofa\n",
    "o cachorro dormiu no tapete\n",
    "o gato pulou no muro\n",
    "\"\"\".strip().lower()\n",
    "\n",
    "tokens = text.split()\n",
    "vocab = sorted(set(tokens))\n",
    "\n",
    "stoi = {t:i for i,t in enumerate(vocab)}\n",
    "itos = {i:t for t,i in stoi.items()}\n",
    "\n",
    "encoded = [stoi[t] for t in tokens]\n"
   ],
   "metadata": {
    "id": "wG0YlI24ItcC"
   },
   "id": "wG0YlI24ItcC",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sliding Window"
   ],
   "metadata": {
    "id": "Vv26wPnPIxon"
   },
   "id": "Vv26wPnPIxon"
  },
  {
   "cell_type": "code",
   "source": [
    "def build_dataset(token_ids, context_size):\n",
    "    \"\"\"Cria pares (X, Y) para language modeling.\n",
    "\n",
    "    X: sequência de tamanho T (context_size)\n",
    "    Y: sequência de tamanho T (próximo token em cada posição)\n",
    "       Ex: X = [t0,t1,t2,t3,t4]\n",
    "           Y = [t1,t2,t3,t4,t5]\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    for i in range(len(token_ids) - context_size):\n",
    "        x = token_ids[i : i + context_size]\n",
    "        y = token_ids[i + 1 : i + context_size + 1]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return torch.tensor(X, dtype=torch.long), torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "context_size = 5\n",
    "X, Y = build_dataset(encoded, context_size)\n",
    "\n",
    "print(\"X shape:\", X.shape, \"Y shape:\", Y.shape)\n",
    "print(\"Exemplo X:\", X[0].tolist())\n",
    "print(\"Exemplo Y:\", Y[0].tolist())\n"
   ],
   "metadata": {
    "id": "TMddzXKpIybW"
   },
   "id": "TMddzXKpIybW",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trainvalsplit"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Split treino / validação (didático)\n",
    "# ============================================================\n",
    "# Premissa: queremos enxergar se o modelo está realmente aprendendo\n",
    "# e evitar \"achar\" que aprendeu só porque o loss de treino caiu.\n",
    "\n",
    "N = X.size(0)\n",
    "perm = torch.randperm(N)\n",
    "\n",
    "split = int(0.85 * N)\n",
    "train_idx = perm[:split]\n",
    "val_idx = perm[split:]\n",
    "\n",
    "X_train, Y_train = X[train_idx].to(device), Y[train_idx].to(device)\n",
    "X_val, Y_val = X[val_idx].to(device), Y[val_idx].to(device)\n",
    "\n",
    "print(\"Treino:\", X_train.shape, Y_train.shape)\n",
    "print(\"Val   :\", X_val.shape, Y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Criar modelo"
   ],
   "metadata": {
    "id": "1MDQu1b8Iz7H"
   },
   "id": "1MDQu1b8Iz7H"
  },
  {
   "cell_type": "code",
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=len(vocab),\n",
    "    context_size=context_size,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2\n",
    ")\n",
    "\n",
    "model = GPTMini(config).to(device)\n"
   ],
   "metadata": {
    "id": "TSVMnDMLI2r1"
   },
   "id": "TSVMnDMLI2r1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parte 1 — Entendendo Cross Entropy\n",
    "Demonstração manual da loss"
   ],
   "metadata": {
    "id": "RqIGMaLCI4U8"
   },
   "id": "RqIGMaLCI4U8"
  },
  {
   "cell_type": "code",
   "source": [
    "logits = torch.tensor([[2.0, 0.5, 0.1]])\n",
    "target = torch.tensor([0])\n",
    "\n",
    "loss = F.cross_entropy(logits, target)\n",
    "print(loss)\n"
   ],
   "metadata": {
    "id": "5sSjAW9pI_S1"
   },
   "id": "5sSjAW9pI_S1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualização probabilística"
   ],
   "metadata": {
    "id": "qTBd7qgfJCvK"
   },
   "id": "qTBd7qgfJCvK"
  },
  {
   "cell_type": "code",
   "source": [
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "plt.bar(range(len(probs[0])), probs[0].cpu())\n",
    "plt.title(\"Distribuição de Probabilidades\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "x34q8HAgJE7G"
   },
   "id": "x34q8HAgJE7G",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parte 2 — Loop de Treinamento\n",
    "\n",
    "Otimizador"
   ],
   "metadata": {
    "id": "Ai-jgeDVJF9r"
   },
   "id": "Ai-jgeDVJF9r"
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n"
   ],
   "metadata": {
    "id": "X30ON7VwJJw9"
   },
   "id": "X30ON7VwJJw9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================\n",
    "# Treinamento (didático, mas com loss em sequência inteira)\n",
    "# ============================================================\n",
    "# Agora a loss é calculada para TODAS as posições do contexto (B, T),\n",
    "# que é a forma mais comum em language modeling.\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "def compute_loss(logits, targets):\n",
    "    # logits: (B, T, V) | targets: (B, T)\n",
    "    return F.cross_entropy(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_val_loss():\n",
    "    model.eval()\n",
    "    logits, _ = model(X_val)  # (B, T, V)\n",
    "    loss = compute_loss(logits, Y_val)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "model.train()\n",
    "\n",
    "steps = 600\n",
    "batch_size = 16\n",
    "eval_every = 50\n",
    "\n",
    "for step in range(steps):\n",
    "    idx = torch.randint(0, X_train.size(0), (batch_size,), device=device)\n",
    "    xb = X_train[idx]\n",
    "    yb = Y_train[idx]\n",
    "\n",
    "    logits, _ = model(xb)     # não passamos targets, calculamos loss aqui\n",
    "    loss = compute_loss(logits, yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss_history.append(loss.item())\n",
    "\n",
    "    if step % eval_every == 0:\n",
    "        vloss = eval_val_loss()\n",
    "        val_loss_history.append((step, vloss))\n",
    "        ppl = math.exp(vloss) if vloss < 20 else float(\"inf\")\n",
    "        print(f\"step {step:03d} | train_loss {loss.item():.4f} | val_loss {vloss:.4f} | val_ppl {ppl:.2f}\")\n"
   ],
   "metadata": {
    "id": "hKekxcqFJS6E"
   },
   "id": "hKekxcqFJS6E"
  },
  {
   "cell_type": "code",
   "source": [
    "loss_history = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for step in range(500):\n",
    "\n",
    "    idx = torch.randint(0, X.size(0), (16,))\n",
    "    xb = X[idx]\n",
    "    yb = Y[idx]\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(step, loss.item())\n"
   ],
   "metadata": {
    "id": "OHrxrs5AJVGR"
   },
   "id": "OHrxrs5AJVGR",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot do training loss (passo a passo)\n",
    "plt.plot(train_loss_history)\n",
    "plt.title(\"Training Loss (por step)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "# Plot do validation loss (avaliado periodicamente)\n",
    "if len(val_loss_history) > 0:\n",
    "    steps_v, losses_v = zip(*val_loss_history)\n",
    "    plt.plot(list(steps_v), list(losses_v))\n",
    "    plt.title(\"Validation Loss (a cada avaliação)\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "id": "tQIP4MNPJYMB"
   },
   "id": "tQIP4MNPJYMB"
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(loss_history)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "BC9FBiAJJZzg"
   },
   "id": "BC9FBiAJJZzg",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "def encode_text(s):\n",
    "    return [stoi[t] for t in s.lower().split() if t in stoi]\n",
    "\n",
    "def decode(ids):\n",
    "    return \" \".join(itos[int(i)] for i in ids)\n"
   ],
   "metadata": {
    "id": "aHEk-wL0Jiqg"
   },
   "id": "aHEk-wL0Jiqg"
  },
  {
   "cell_type": "code",
   "source": [
    "def encode_text(s):\n",
    "    return [stoi[t] for t in s.split() if t in stoi]\n",
    "\n",
    "def decode(ids):\n",
    "    return \" \".join(itos[i] for i in ids)\n"
   ],
   "metadata": {
    "id": "4KqbZHukJl-m"
   },
   "id": "4KqbZHukJl-m",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "@torch.no_grad()\n",
    "def generate_greedy(start_tokens, max_new_tokens=10):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(start_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    model.train()\n",
    "    return idx.squeeze(0).tolist()\n"
   ],
   "metadata": {
    "id": "P1KkJ8t0Jn6P"
   },
   "id": "P1KkJ8t0Jn6P"
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def generate_greedy(start_tokens, max_new_tokens=10):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(start_tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, _ = model(idx[:, -context_size:])\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "        idx = torch.cat([idx, next_id.unsqueeze(1)], dim=1)\n",
    "\n",
    "    return idx.squeeze().tolist()\n"
   ],
   "metadata": {
    "id": "c17bPu34Jq5m"
   },
   "id": "c17bPu34Jq5m",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "@torch.no_grad()\n",
    "def generate_temperature(start_tokens, max_new_tokens=10, temperature=1.0):\n",
    "    model.eval()\n",
    "    temperature = max(float(temperature), 1e-6)\n",
    "\n",
    "    idx = torch.tensor(start_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    model.train()\n",
    "    return idx.squeeze(0).tolist()\n"
   ],
   "metadata": {
    "id": "jt4JbTDjJsyx"
   },
   "id": "jt4JbTDjJsyx"
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def generate_temperature(start_tokens, temp=1.0):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(start_tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(10):\n",
    "        logits, _ = model(idx[:, -context_size:])\n",
    "        logits = logits[:, -1, :] / temp\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return idx.squeeze().tolist()\n"
   ],
   "metadata": {
    "id": "Ernn9sFgJwTj"
   },
   "id": "Ernn9sFgJwTj",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "@torch.no_grad()\n",
    "def generate_top_k(start_tokens, k=5, max_new_tokens=10, temperature=1.0):\n",
    "    model.eval()\n",
    "    temperature = max(float(temperature), 1e-6)\n",
    "    k = int(max(1, k))\n",
    "\n",
    "    idx = torch.tensor(start_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        topk_vals, topk_idx = torch.topk(logits, k, dim=-1)   # (1, k)\n",
    "        probs = F.softmax(topk_vals, dim=-1)                  # (1, k)\n",
    "\n",
    "        sample = torch.multinomial(probs, num_samples=1)      # (1, 1) em [0..k-1]\n",
    "        next_id = topk_idx.gather(-1, sample)                 # (1, 1) token real\n",
    "\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    model.train()\n",
    "    return idx.squeeze(0).tolist()\n"
   ],
   "metadata": {
    "id": "kh0mJsCqJzIR"
   },
   "id": "kh0mJsCqJzIR"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nucleus"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_top_p(start_tokens, p=0.9, max_new_tokens=10, temperature=1.0):\n",
    "    \"\"\"Nucleus sampling (top-p): escolhe entre o menor conjunto de tokens\n",
    "    cuja probabilidade acumulada >= p.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    temperature = max(float(temperature), 1e-6)\n",
    "    p = float(min(max(p, 1e-6), 1.0))\n",
    "\n",
    "    idx = torch.tensor(start_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1).squeeze(0)  # (V,)\n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "\n",
    "        cum_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "        cutoff = torch.searchsorted(cum_probs, torch.tensor(p, device=device))\n",
    "\n",
    "        cutoff = int(cutoff.item()) + 1\n",
    "        candidate_probs = sorted_probs[:cutoff]\n",
    "        candidate_idx = sorted_idx[:cutoff]\n",
    "\n",
    "        candidate_probs = candidate_probs / candidate_probs.sum()\n",
    "        sample = torch.multinomial(candidate_probs, num_samples=1)\n",
    "        next_id = candidate_idx[sample].view(1, 1)\n",
    "\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    model.train()\n",
    "    return idx.squeeze(0).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Salvando um checkpoint completo (pesos + config + vocabulário)\n",
    "ckpt = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"config\": config.__dict__ if hasattr(config, \"__dict__\") else dict(config),\n",
    "    \"stoi\": stoi,\n",
    "    \"itos\": itos,\n",
    "    \"context_size\": context_size,\n",
    "}\n",
    "torch.save(ckpt, \"05-pre-treinamento_gptmini_checkpoint.pt\")\n",
    "print(\"✅ Checkpoint salvo:\", \"05-pre-treinamento_gptmini_checkpoint.pt\")\n"
   ],
   "metadata": {
    "id": "wt0ECP_TJz4V"
   },
   "id": "wt0ECP_TJz4V",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parte 5 — Checkpoints\n",
    "Salvar modelo"
   ],
   "metadata": {
    "id": "nza4n-S4J1Vf"
   },
   "id": "nza4n-S4J1Vf"
  },
  {
   "cell_type": "code",
   "source": [
    "# Carregando checkpoint\n",
    "ckpt = torch.load(\"05-pre-treinamento_gptmini_checkpoint.pt\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"✅ Checkpoint carregado\")\n"
   ],
   "metadata": {
    "id": "4ZZZoT2TJ8Pi"
   },
   "id": "4ZZZoT2TJ8Pi",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Carregar modelo"
   ],
   "metadata": {
    "id": "T9NJk3OlJ-Wn"
   },
   "id": "T9NJk3OlJ-Wn"
  },
  {
   "cell_type": "code",
   "source": [
    "# Teste de geração (comparando estratégias)\n",
    "start = encode_text(\"o gato\")\n",
    "\n",
    "print(\"Prompt:\", decode(start))\n",
    "print(\"Greedy        :\", decode(generate_greedy(start, max_new_tokens=8)))\n",
    "print(\"Temperature 0.8:\", decode(generate_temperature(start, max_new_tokens=8, temperature=0.8)))\n",
    "print(\"Top-k (k=5)    :\", decode(generate_top_k(start, k=5, max_new_tokens=8, temperature=1.0)))\n",
    "print(\"Top-p (p=0.9)  :\", decode(generate_top_p(start, p=0.9, max_new_tokens=8, temperature=1.0)))\n"
   ],
   "metadata": {
    "id": "3me5ldEpKBY0"
   },
   "id": "3me5ldEpKBY0",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comparação Antes vs Depois"
   ],
   "metadata": {
    "id": "COMgJqUAKC45"
   },
   "id": "COMgJqUAKC45"
  },
  {
   "cell_type": "code",
   "source": [
    "# Teste de Geracao\n",
    "\n",
    "start = encode_text(\"o gato\")\n",
    "\n",
    "print(\"Greedy:\", decode(generate_greedy(start)))\n",
    "print(\"Temp:\", decode(generate_temperature(start)))\n",
    "print(\"Top-k:\", decode(generate_top_k(start)))\n"
   ],
   "metadata": {
    "id": "HMiJ-Vn5KILK"
   },
   "id": "HMiJ-Vn5KILK",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Você acabou de ensinar um GPT a aprender linguagem.\n",
    "\n",
    "Você viu:\n",
    "\n",
    "- Como calcular cross entropy\n",
    "- Como funciona o loop de treinamento\n",
    "- Como monitorar aprendizado\n",
    "- Como controlar geração de texto\n",
    "- Como salvar e reutilizar modelos\n",
    "\n",
    "No próximo capítulo exploraremos fine-tuning e especialização de modelos.\n"
   ],
   "metadata": {
    "id": "1phtg9GkKS6i"
   },
   "id": "1phtg9GkKS6i"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}