# Cap√≠tulo 05 ‚Äî Pr√©-Treinamento e Gera√ß√£o de Texto

Nos cap√≠tulos anteriores, constru√≠mos o corpo de um modelo GPT.

- aprendemos como texto vira n√∫meros  
- aprendemos como embeddings representam linguagem  
- aprendemos como self-attention constr√≥i contexto  
- montamos um GPT funcional  

Agora surge a pergunta mais importante:

> Como o modelo aprende linguagem?

Este cap√≠tulo responde exatamente isso.

---

## O que significa treinar um modelo de linguagem

Um modelo GPT n√£o aprende gram√°tica explicitamente.

Ele aprende padr√µes estat√≠sticos observando sequ√™ncias de texto.

A tarefa fundamental de treinamento √©:

> prever o pr√≥ximo token com base nos tokens anteriores

Pode parecer simples, mas essa tarefa cont√©m toda a complexidade da linguagem.

---

## O pipeline de treinamento de um LLM

Treinar um modelo envolve v√°rias etapas que se repetem milhares ou milh√µes de vezes.

![Pipeline de treinamento de um LLM](./infograficos/01-pipeline-treinamento.png)

O fluxo geral √©:

1. Dataset de texto  
2. Tokeniza√ß√£o  
3. Forward pass no modelo  
4. C√°lculo da loss  
5. Backpropagation  
6. Atualiza√ß√£o dos pesos  

Esse ciclo √© conhecido como loop de treinamento.

---

## Como avaliamos se o modelo est√° aprendendo

Quando o modelo prev√™ o pr√≥ximo token, ele n√£o escolhe uma palavra diretamente.

Ele produz uma distribui√ß√£o de probabilidades para todas as palavras do vocabul√°rio.

A qualidade do modelo depende de qu√£o pr√≥ximas essas probabilidades est√£o do token correto.

---

## Cross Entropy: medindo o erro probabil√≠stico

Para medir o erro do modelo usamos uma fun√ß√£o chamada **cross entropy**.

![Cross entropy explicada](./infograficos/02-cross-entropy.png)

A cross entropy mede:

- qu√£o diferente a distribui√ß√£o prevista est√° da distribui√ß√£o real
- quanto o modelo ficou "surpreso" com o token correto

Quanto menor a cross entropy, melhor o modelo.

---

## Perplexidade: interpretando a qualidade do modelo

Outra m√©trica importante √© a perplexidade.

Ela representa o grau de incerteza do modelo ao prever o pr√≥ximo token.

Uma perplexidade alta indica:

- modelo confuso
- muitas possibilidades plaus√≠veis

Uma perplexidade baixa indica:

- modelo confiante
- previs√µes mais precisas

---

## O loop de treinamento completo

O aprendizado do modelo ocorre dentro de um ciclo repetitivo.

![Loop de treinamento completo](./infograficos/03-loop-treinamento.png)

O ciclo consiste em:

### Forward Pass
O modelo recebe tokens e produz previs√µes.

### C√°lculo da Loss
A previs√£o √© comparada com o token real.

### Backpropagation
O erro √© propagado pela rede para calcular gradientes.

### Atualiza√ß√£o de Pesos
Os pesos s√£o ajustados para reduzir o erro.

Esse processo √© repetido em batches e epochs.

---

## Treinamento em batches

Treinar usando todo o dataset de uma vez seria computacionalmente invi√°vel.

Por isso, dividimos os dados em pequenos grupos chamados batches.

Isso permite:

- melhor efici√™ncia computacional
- estabilidade do treinamento
- paraleliza√ß√£o

---

## Monitorando o aprendizado do modelo

Durante o treinamento, monitoramos:

- training loss
- validation loss

Se a loss de valida√ß√£o come√ßa a aumentar enquanto a loss de treinamento diminui, pode indicar overfitting.

---

## Depois do treinamento: gera√ß√£o de texto

Depois que o modelo aprende padr√µes da linguagem, ele pode gerar texto.

Mas gera√ß√£o n√£o √© determin√≠stica.

O modelo produz probabilidades e precisamos decidir como escolher o pr√≥ximo token.

---

## Estrat√©gias de gera√ß√£o de texto

Existem v√°rias formas de gerar texto a partir das probabilidades.

![Estrat√©gias de decoding](./infograficos/04-decoding-strategies.png)

### Greedy Decoding
Sempre escolhe o token mais prov√°vel.

- Texto previs√≠vel
- Pouca criatividade

### Temperature Sampling
Controla aleatoriedade das probabilidades.

- Temperatura baixa ‚Üí mais previs√≠vel  
- Temperatura alta ‚Üí mais criativo  

### Top-k Sampling
Escolhe entre os k tokens mais prov√°veis.

- Controla diversidade
- Evita escolhas improv√°veis

### Nucleus Sampling (Top-p)
Escolhe tokens que comp√µem uma probabilidade acumulada.

- Estrat√©gia usada em muitos LLMs modernos
- Equilibra coer√™ncia e diversidade

---

## Salvando modelos: checkpoints

Treinar modelos grandes pode levar horas, dias ou semanas.

Por isso salvamos estados intermedi√°rios do modelo chamados checkpoints.

![Checkpoints de treinamento](./infograficos/05-checkpoints.png)

Checkpoints permitem:

- retomar treinamento
- evitar perda de progresso
- compartilhar modelos
- reproduzir experimentos

---

## Pesos pr√©-treinados e modelos fundacionais

Modelos modernos raramente s√£o treinados do zero para cada tarefa.

Eles s√£o:

1. Pr√©-treinados em grandes datasets  
2. Adaptados para tarefas espec√≠ficas  

Esse conceito √© conhecido como foundation models.

---

## Limita√ß√µes do treinamento did√°tico

Neste projeto usamos:

- datasets pequenos  
- modelos compactos  
- treinamento curto  

Essas simplifica√ß√µes permitem execu√ß√£o no Colab, mas mant√™m os princ√≠pios fundamentais do treinamento de LLMs.

---

## O que construiremos no notebook

Neste cap√≠tulo vamos implementar:

- c√°lculo da cross entropy loss
- loop de treinamento completo
- monitoramento do treinamento
- diferentes estrat√©gias de gera√ß√£o
- salvamento e carregamento de modelos
- compara√ß√£o entre modelos antes e depois do treinamento

---

## Preparando os pr√≥ximos cap√≠tulos

Depois de aprender como treinar um modelo, surge o pr√≥ximo passo natural:

> Como adaptar um modelo para tarefas espec√≠ficas?

Nos pr√≥ximos cap√≠tulos exploraremos:

- fine-tuning  
- instruction tuning  
- alinhamento de modelos  

---

## üßæ Gloss√°rio R√°pido ‚Äî Cap√≠tulo 05

**Cross Entropy**  
M√©trica que mede diferen√ßa entre distribui√ß√µes de probabilidade.

**Perplexidade**  
Medida de incerteza do modelo ao prever tokens.

**Forward Pass**  
Etapa onde o modelo produz previs√µes.

**Backpropagation**  
Processo de ajuste dos pesos com base no erro.

**Batch**  
Grupo de exemplos usados em uma etapa do treinamento.

**Epoch**  
Uma passagem completa pelo dataset.

**Sampling**  
Processo de escolha do pr√≥ximo token com base em probabilidades.

**Checkpoint**  
Salvamento do estado do modelo durante o treinamento.

---

> Nos cap√≠tulos anteriores constru√≠mos o c√©rebro do modelo.  
> Neste cap√≠tulo come√ßamos a ensinar o modelo a pensar.
