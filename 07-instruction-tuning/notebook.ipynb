{"cells": [{"cell_type": "markdown", "source": ["# Cap√≠tulo 07 ‚Äî Instruction Tuning: Criando um Assistente\n", "\n", "üéØ **Objetivos:** Transformar o modelo completador em um assistente √∫til usando **SFT (Supervised Fine-Tuning)**.\n", "\n", "![SFT](./infograficos/04-pipeline-sft.png)"], "metadata": {"id": "header"}}, {"cell_type": "code", "source": ["# ============================================================\n", "# Setup do reposit√≥rio no Colab\n", "# ============================================================\n", "import os, sys\n", "REPO_NAME = \"fazendo-um-llm-do-zero\"\n", "if 'google.colab' in str(get_ipython()):\n", "    if not os.path.exists(REPO_NAME):\n", "        get_ipython().system(f\"git clone https://github.com/vongrossi/{REPO_NAME}.git\")\n", "    if os.path.exists(REPO_NAME) and os.getcwd().split('/')[-1] != REPO_NAME:\n", "        os.chdir(REPO_NAME)\n", "if os.getcwd() not in sys.path: sys.path.append(os.getcwd())\n", "print(\"üìÇ Diret√≥rio atual:\", os.getcwd())"], "metadata": {"id": "setup-repo"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# ============================================================\n", "# 1. Setup e Conex√£o com a Intelig√™ncia Base\n", "# ============================================================\n", "import os, sys, torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "from lib.gptmini import GPTConfig, GPTMini\n", "\n", "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "\n", "if not os.path.exists(\"gpt_checkpoint.pt\"):\n", "    from google.colab import files\n", "    print(\"üì§ O arquivo 'gpt_checkpoint.pt' n√£o foi encontrado.\")\n", "    print(\"Por favor, suba o checkpoint gerado no final do Cap√≠tulo 05:\")\n", "    uploaded = files.upload()\n", "\n", "try:\n", "    ckpt = torch.load(\"gpt_checkpoint.pt\", map_location=device, weights_only=False)\n", "    stoi, itos = ckpt['stoi'], ckpt['itos']\n", "    config = ckpt['config']\n", "    context_size = config.context_size\n", "    print(f\"‚úÖ Intelig√™ncia Base Carregada! Vocabul√°rio: {len(stoi)} caracteres.\")\n", "    print(f\"üìè Contexto: {context_size}\")\n", "except Exception as e:\n", "    print(f\"‚ùå ERRO AO CARREGAR: {e}\")\n", "    raise\n", "\n", "# Encoder alinhado ao Cap 05 (mapeia desconhecidos para espa√ßo)\n", "def encode(s):\n", "    unk_id = stoi.get(' ', 0)\n", "    return [stoi.get(c, unk_id) for c in s.lower()]\n", "\n", "decode = lambda l: ''.join([itos[i] for i in l])\n"], "metadata": {"id": "setup"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## 1. Dataset de Instru√ß√µes\n", "\n", "Criamos pares de Pergunta e Resposta para o alinhamento."], "metadata": {"id": "data-header"}}, {"cell_type": "code", "source": ["instructions = [\n", "    {\"q\": \"o que o gato fez?\", \"a\": \"o gato subiu no telhado e pulou o muro.\"},\n", "    {\"q\": \"onde o cachorro dormiu?\", \"a\": \"o cachorro dormiu no sofa e no tapete.\"},\n", "    {\"q\": \"defina inteligencia artificial\", \"a\": \"inteligencia artificial e o estudo de algoritmos.\"},\n", "    {\"q\": \"o que e machine learning?\", \"a\": \"machine learning permite que sistemas aprendam padroes.\"}\n", "]\n", "\n", "PROMPT_PREFIX = \"### comando:\\n\"\n", "PROMPT_SUFFIX = \"### resposta:\\n\"\n", "\n", "def build_sft_dataset(data, context_size):\n", "    X, Y, masks = [], [], []\n", "    pad_id = stoi.get(' ', 0)\n", "    for item in data:\n", "        cmd = f\"{PROMPT_PREFIX}{item['q']}\\n{PROMPT_SUFFIX}\"\n", "        full = cmd + item['a'] + \"\\n\"\n", "        cmd_ids = encode(cmd)\n", "        full_ids = encode(full)\n", "        if len(full_ids) < 2:\n", "            continue\n", "        # Mant√©m o come√ßo para preservar o prompt\n", "        if len(full_ids) > context_size + 1:\n", "            full_ids = full_ids[: context_size + 1]\n", "        cmd_len = min(len(cmd_ids), len(full_ids))\n", "        x = full_ids[:-1]\n", "        y = full_ids[1:]\n", "        if cmd_len >= len(x):\n", "            continue\n", "        m = [1 if (i + 1) >= cmd_len else 0 for i in range(len(x))]\n", "        if len(x) < context_size:\n", "            pad_len = context_size - len(x)\n", "            x = x + [pad_id] * pad_len\n", "            y = y + [pad_id] * pad_len\n", "            m = m + [0] * pad_len\n", "        X.append(x); Y.append(y); masks.append(m)\n", "    return torch.tensor(X).to(device), torch.tensor(Y).to(device), torch.tensor(masks).to(device)\n", "\n", "X, Y, M = build_sft_dataset(instructions, context_size)\n", "print(f\"üì¶ Amostras de Alinhamento: {len(X)}\")\n", "# Debug de comprimentos\n", "for item in instructions:\n", "    cmd = f\"{PROMPT_PREFIX}{item['q']}\\n{PROMPT_SUFFIX}\"\n", "    full = cmd + item['a'] + \"\\n\"\n", "    print('---')\n", "    print('q:', item['q'])\n", "    print('cmd_len:', len(encode(cmd)), 'full_len:', len(encode(full)))\n"], "metadata": {"id": "data-code"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## 2. Treinamento com M√°scara de Loss\n", "\n", "Otimizamos apenas a gera√ß√£o da resposta.\n", "\n", "![Masking](./infograficos/03-mascaramento-loss-resposta.png)"], "metadata": {"id": "train-header"}}, {"cell_type": "code", "source": ["model = GPTMini(config).to(device)\n", "model.load_state_dict(ckpt['state_dict'])\n", "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n", "\n", "loss_history = []\n", "model.train()\n", "print(\"üî® Alinhando o assistente...\")\n", "if len(X) == 0:\n", "    raise RuntimeError(\"Dataset vazio. Re-treine o Cap√≠tulo 05 com context_size maior e recarregue o checkpoint.\")\n", "batch_size = min(4, len(X))\n", "for step in range(401):\n", "    idx = torch.randint(len(X), (batch_size,))\n", "    logits, _ = model(X[idx])\n", "    B, T, V = logits.shape\n", "    loss = F.cross_entropy(logits.view(-1, V), Y[idx].view(-1), reduction='none')\n", "    mask = M[idx].view(-1)\n", "    loss = (loss * mask).sum() / mask.sum().clamp(min=1)\n", "    optimizer.zero_grad(set_to_none=True); loss.backward()\n", "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n", "    optimizer.step()\n", "    loss_history.append(loss.item())\n", "    if step % 100 == 0: print(f\"Step {step} | Loss {loss.item():.4f}\")\n", "\n", "plt.plot(loss_history, color='#34A853')\n", "plt.title(\"Curva de Alinhamento (SFT)\")\n", "plt.show()\n"], "metadata": {"id": "train-code"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## 3. Teste do Assistente Alinhado\n", "\n", "O modelo agora responde apenas o que foi solicitado."], "metadata": {"id": "test-header"}}, {"cell_type": "code", "source": ["@torch.no_grad()\n", "def ask(model, question, max_new_tokens=80):\n", "    model.eval()\n", "    prompt = f\"{PROMPT_PREFIX}{question}\\n{PROMPT_SUFFIX}\"\n", "    idx = torch.tensor(encode(prompt)).unsqueeze(0).to(device)\n", "    prompt_len = idx.shape[1]\n", "\n", "    for _ in range(max_new_tokens):\n", "        idx_cond = idx[:, -context_size:]\n", "        logits, _ = model(idx_cond)\n", "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n", "        idx = torch.cat([idx, next_id], dim=1)\n", "        if itos[next_id.item()] == '\\n':\n", "            break\n", "\n", "    return decode(idx[0][prompt_len:].tolist())\n", "\n", "print(\"ü§ñ TESTE DE INTERA√á√ÉO:\")\n", "print(\"-\" * 30)\n", "q1 = \"o que o gato fez?\"\n", "print(f\"Pergunta: {q1}\\nResposta: {ask(model, q1)}\")\n", "\n", "print(\"\\n\" + \"-\" * 30)\n", "q2 = \"o que e machine learning?\"\n", "print(f\"Pergunta: {q2}\\nResposta: {ask(model, q2)}\")\n", "\n", "print(\"\\n\" + \"-\" * 30)\n", "q3 = \"defina inteligencia artificial\"\n", "print(f\"Pergunta: {q3}\\nResposta: {ask(model, q3)}\")\n", "\n", "print(\"\\n\" + \"-\" * 30)\n", "q4 = \"onde o cachorro dormiu?\"\n", "print(f\"Pergunta: {q4}\\nResposta: {ask(model, q4)}\")\n"], "metadata": {"id": "test-code"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## üèÅ Conclus√£o da Jornada\n", "\n", "Voc√™ completou a s√©rie! \n", "\n", "Transformou um modelo estat√≠stico em um assistente capaz de seguir inten√ß√µes humanas. Este √© o fundamento do alinhamento de IA.\n", "\n", "![Avalia√ß√£o](./infograficos/05-avaliacao-respostas.png)"], "metadata": {"id": "footer"}}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.12.3"}}, "nbformat": 4, "nbformat_minor": 5}