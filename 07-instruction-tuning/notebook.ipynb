{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cap√≠tulo 07 ‚Äî Instruction Tuning: Criando um Assistente\n",
    "\n",
    "Neste cap√≠tulo, atingimos o √°pice da nossa jornada. Vamos transformar um modelo que apenas \"completa frases\" em um **assistente que segue instru√ß√µes**.\n",
    "\n",
    "--- \n",
    "### üß™ O conceito de Alinhamento\n",
    "Modelos base aprendem a probabilidade estat√≠stica das palavras. Mas eles n√£o sabem que, quando um humano faz uma pergunta, ele espera uma resposta direta. O **Supervised Fine-Tuning (SFT)** √© o processo de mostrar ao modelo milhares de exemplos de `[Comando]` -> `[Resposta]`. \n",
    "\n",
    "![Pipeline SFT](./infograficos/04-pipeline-sft.png)"
   ],
   "metadata": { "id": "header" }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setup e Persist√™ncia de Dados\n",
    "\n",
    "**‚ö†Ô∏è Importante:** Como o Google Colab apaga os arquivos ao fechar a sess√£o, se voc√™ n√£o tiver o `gpt_checkpoint.pt` do Cap√≠tulo 05 aqui, o modelo come√ßar√° do zero (sem saber falar portugu√™s)."
   ],
   "metadata": { "id": "setup-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lib.gptmini import GPTConfig, GPTMini\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚úÖ Executando em: {device}\")\n",
    "\n",
    "# Se o arquivo n√£o existir, vamos criar um aviso claro\n",
    "if not os.path.exists(\"gpt_checkpoint.pt\"):\n",
    "    print(\"‚ùå CHECKPOINT N√ÉO ENCONTRADO!\")\n",
    "    print(\"Para melhores resultados, fa√ßa o upload do arquivo 'gpt_checkpoint.pt' gerado no Cap√≠tulo 05.\")\n",
    "else:\n",
    "    print(\"‚ú® Checkpoint detectado e pronto para carregar.\")"
   ],
   "metadata": { "id": "setup-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Dataset de Instru√ß√µes Curado\n",
    "\n",
    "Para que o modelo responda algo com sentido, precisamos de um dataset que cubra os t√≥picos que vamos testar. Vamos ensinar o modelo a ser um mini-especialista em IA."
   ],
   "metadata": { "id": "dataset-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = [\n",
    "    {\n",
    "        \"instruction\": \"Explique Machine Learning\",\n",
    "        \"response\": \"Machine Learning e o estudo de algoritmos que aprendem padroes de dados.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"O que e um LLM?\",\n",
    "        \"response\": \"LLM e um modelo de linguagem treinado em grandes volumes de texto para entender e gerar linguagem.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Defina Inteligencia Artificial\",\n",
    "        \"response\": \"IA e a simulacao de processos de inteligencia humana por maquinas e sistemas de computacao.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"O que e o mecanismo de Atencao?\",\n",
    "        \"response\": \"Atencao permite que o modelo foque em partes importantes do texto para entender o contexto.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def format_instruction(item):\n",
    "    return f\"### Comando:\\n{item['instruction']}\\n\\n### Resposta:\\n{item['response']}\"\n",
    "\n",
    "all_text = \"\".join([format_instruction(d) for d in dataset])\n",
    "chars = sorted(set(all_text))\n",
    "stoi = {c:i for i,c in enumerate(chars)}\n",
    "itos = {i:c for c,i in stoi.items()}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "def encode(text): return [stoi[c] for c in text if c in stoi]\n",
    "def decode(tokens): return \"\".join([itos[t] for t in tokens])\n",
    "\n",
    "print(f\"Tamanho do Vocabulario: {vocab_size} caracteres\")"
   ],
   "metadata": { "id": "dataset-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Inicializa√ß√£o com Transfer√™ncia de Pesos\n",
    "\n",
    "Vamos carregar a \"intelig√™ncia base\" do Cap√≠tulo 05. Mesmo que o vocabul√°rio seja um pouco diferente, os **Transformer Blocks** (as camadas de aten√ß√£o) carregam o conhecimento estrutural da l√≠ngua."
   ],
   "metadata": { "id": "model-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "config = GPTConfig(vocab_size=vocab_size, context_size=64, d_model=128, n_heads=4, n_layers=2)\n",
    "backbone = GPTMini(config).to(device)\n",
    "\n",
    "try:\n",
    "    ckpt = torch.load(\"gpt_checkpoint.pt\", map_location=device)\n",
    "    state_dict = ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
    "    model_dict = backbone.state_dict()\n",
    "    # Carrega apenas camadas com shapes id√™nticos\n",
    "    pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
    "    backbone.load_state_dict(pretrained_dict, strict=False)\n",
    "    print(f\"üß† Intelig√™ncia Base carregada: {len(pretrained_dict)} camadas reaproveitadas.\")\n",
    "except:\n",
    "    print(\"üå± Iniciando treinamento do zero (sem pesos pr√©vios).\")"
   ],
   "metadata": { "id": "model-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. O Cora√ß√£o do Cap√≠tulo: Mascaramento da Loss\n",
    "\n",
    "No Instruction Tuning, n√£o queremos que o modelo aprenda a prever a pergunta (n√≥s j√° sabemos a pergunta!). Queremos que ele foque 100% na resposta. \n",
    "\n",
    "Usamos uma **M√°scara Bin√°ria** onde a Loss do Comando √© multiplicada por 0 e a da Resposta por 1.\n",
    "\n",
    "![Masking Loss](./infograficos/03-mascaramento-loss-resposta.png)"
   ],
   "metadata": { "id": "mask-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "def build_dataset(data, context_size=64):\n",
    "    X, Y, masks = [], [], []\n",
    "    for item in data:\n",
    "        cmd = f\"### Comando:\\n{item['instruction']}\\n\\n### Resposta:\\n\"\n",
    "        resp = item[\"response\"]\n",
    "        full = cmd + resp\n",
    "        \n",
    "        full_tokens = encode(full)\n",
    "        cmd_tokens = encode(cmd)\n",
    "        \n",
    "        for i in range(len(full_tokens) - context_size):\n",
    "            x = full_tokens[i : i+context_size]\n",
    "            y = full_tokens[i+1 : i+context_size+1]\n",
    "            # M√°scara: 0 para o que for comando, 1 para o que for resposta\n",
    "            m = [0]*len(cmd_tokens) + [1]*(context_size - len(cmd_tokens))\n",
    "            \n",
    "            X.append(x); Y.append(y); masks.append(m[:context_size])\n",
    "            \n",
    "    return torch.tensor(X).to(device), torch.tensor(Y).to(device), torch.tensor(masks).to(device)\n",
    "\n",
    "X, Y, MASK = build_dataset(dataset)\n",
    "\n",
    "class InstructWrapper(nn.Module):\n",
    "    def __init__(self, gpt): \n",
    "        super().__init__(); self.gpt = gpt\n",
    "    def forward(self, x, y=None, m=None):\n",
    "        logits, _ = self.gpt(x)\n",
    "        loss = None\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), reduction='none')\n",
    "            loss = (loss * m.view(-1)).mean()\n",
    "        return logits, loss\n",
    "\n",
    "model = InstructWrapper(backbone).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)"
   ],
   "metadata": { "id": "mask-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Treinamento e Monitoramento\n",
    "\n",
    "Vamos ver o modelo \"se alinhando\" √†s nossas instru√ß√µes."
   ],
   "metadata": { "id": "train-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "loss_history = []\n",
    "print(\"üöÄ Alinhando o modelo...\")\n",
    "\n",
    "for step in range(401):\n",
    "    idx = torch.randint(0, X.size(0), (8,))\n",
    "    logits, loss = model(X[idx], Y[idx], MASK[idx])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "    if step % 100 == 0: print(f\"Step {step:03d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(loss_history, color='#34A853')\n",
    "plt.title(\"Curva de Alinhamento (SFT Loss)\")\n",
    "plt.show()"
   ],
   "metadata": { "id": "train-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. O Teste Final: O Assistente em A√ß√£o\n",
    "\n",
    "Agora, o modelo deve ser capaz de reconhecer o formato de comando e gerar uma resposta baseada no que aprendeu."
   ],
   "metadata": { "id": "test-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def generate_response(model, instruction, max_new=50):\n",
    "    model.eval()\n",
    "    context = f\"### Comando:\\n{instruction}\\n\\n### Resposta:\\n\"\n",
    "    tokens = torch.tensor(encode(context)).unsqueeze(0).to(device)\n",
    "    \n",
    "    for _ in range(max_new):\n",
    "        idx_cond = tokens[:, -64:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "        if itos[next_token.item()] == \"\\n\": break\n",
    "        \n",
    "    return decode(tokens[0].tolist())\n",
    "\n",
    "print(\"ü§ñ TESTE DO ASSISTENTE:\")\n",
    "print(\"-\" * 30)\n",
    "pergunta = \"Explique Machine Learning\"\n",
    "print(generate_response(model, pergunta))"
   ],
   "metadata": { "id": "test-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üèÅ Conclus√£o da Jornada\n",
    "\n",
    "Parab√©ns! Voc√™ construiu um LLM do zero absoluto at√© o alinhamento de instru√ß√µes. \n",
    "\n",
    "O que vimos hoje √© a base de como empresas como a OpenAI e o Google treinam seus assistentes. A diferen√ßa √© apenas a escala (bilh√µes de par√¢metros e bilh√µes de exemplos), mas a matem√°tica e a l√≥gica do **Mascaramento de Loss** e do **SFT** s√£o exatamente estas.\n",
    "\n",
    "![Avalia√ß√£o](./infograficos/05-avaliacao-respostas.png)"
   ],
   "metadata": { "id": "footer" }
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.12.3" }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}