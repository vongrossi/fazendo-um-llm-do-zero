{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CapÃ­tulo 07 â€” Instruction Tuning: Criando um Assistente\n",
    "\n",
    "ðŸŽ¯ **Objetivos:** Transformar o modelo completador em um assistente Ãºtil usando **SFT (Supervised Fine-Tuning)**.\n",
    "\n",
    "![SFT](./infograficos/04-pipeline-sft.png)"
   ],
   "metadata": { "id": "header" }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Setup do repositÃ³rio no Colab\n",
    "# ============================================================\n",
    "import os, sys\n",
    "REPO_NAME = \"fazendo-um-llm-do-zero\"\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if not os.path.exists(REPO_NAME):\n",
    "        get_ipython().system(f\"git clone https://github.com/vongrossi/{REPO_NAME}.git\")\n",
    "    if os.path.exists(REPO_NAME) and os.getcwd().split('/')[-1] != REPO_NAME:\n",
    "        os.chdir(REPO_NAME)\n",
    "if os.getcwd() not in sys.path: sys.path.append(os.getcwd())\n",
    "print(\"ðŸ“‚ DiretÃ³rio atual:\", os.getcwd())"
   ],
   "metadata": { "id": "setup-repo" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.gptmini import GPTConfig, GPTMini\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ðŸ“‚ Carregamento do Checkpoint do Cap 05\n",
    "if not os.path.exists(\"gpt_checkpoint.pt\"):\n",
    "    from google.colab import files\n",
    "    print(\"ðŸ“¤ Por favor, suba o 'gpt_checkpoint.pt' gerado no CapÃ­tulo 05:\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "ckpt = torch.load(\"gpt_checkpoint.pt\", map_location=device, weights_only=False)\n",
    "stoi, itos = ckpt['stoi'], ckpt['itos']\n",
    "encode = lambda s: [stoi[c] for c in s.lower() if c in stoi]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "config = ckpt['config']\n",
    "context_size = config.context_size\n",
    "print(f\"ðŸ§  VocabulÃ¡rio Carregado: {len(stoi)} caracteres.\")"
   ],
   "metadata": { "id": "setup" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Dataset de InstruÃ§Ãµes\n",
    "\n",
    "Ensinamos o modelo que a estrutura `### comando:` pede uma resposta em `### resposta:`."
   ],
   "metadata": { "id": "data-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "instructions = [\n",
    "    {\"q\": \"o que o gato fez?\", \"a\": \"o gato subiu no telhado e pulou o muro.\"},\n",
    "    {\"q\": \"onde o cachorro dormiu?\", \"a\": \"o cachorro dormiu no sofa e no tapete.\"},\n",
    "    {\"q\": \"defina inteligencia artificial\", \"a\": \"inteligencia artificial e o estudo de algoritmos.\"},\n",
    "    {\"q\": \"o que e machine learning?\", \"a\": \"machine learning permite que sistemas aprendam padroes.\"}\n",
    "]\n",
    "\n",
    "def build_sft_dataset(data, context_size):\n",
    "    X, Y, masks = [], [], []\n",
    "    for item in data:\n",
    "        cmd = f\"### comando:\\n{item['q']}\\n### resposta:\\n\"\n",
    "        full = cmd + item['a']\n",
    "        ids = encode(full)\n",
    "        cmd_len = len(encode(cmd))\n",
    "        for i in range(len(ids) - context_size):\n",
    "            X.append(ids[i : i+context_size])\n",
    "            Y.append(ids[i+1 : i+context_size+1])\n",
    "            # MÃ¡scara de Loss: 0 para o comando, 1 para a resposta\n",
    "            m = []\n",
    "            for j in range(i, i + context_size):\n",
    "                if j < cmd_len: m.append(0)\n",
    "                else: m.append(1)\n",
    "            masks.append(m)\n",
    "    return torch.tensor(X).to(device), torch.tensor(Y).to(device), torch.tensor(masks).to(device)\n",
    "\n",
    "X, Y, M = build_sft_dataset(instructions, context_size)\n",
    "print(f\"ðŸ“¦ Amostras de Alinhamento: {len(X)}\")"
   ],
   "metadata": { "id": "data-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = GPTMini(config).to(device)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "loss_history = []\n",
    "model.train()\n",
    "print(\"ðŸ”¨ Alinhando o assistente...\")\n",
    "for step in range(801):\n",
    "    idx = torch.randint(len(X), (8,))\n",
    "    logits, _ = model(X[idx])\n",
    "    B, T, V = logits.shape\n",
    "    loss = F.cross_entropy(logits.view(-1, V), Y[idx].view(-1), reduction='none')\n",
    "    loss = (loss * M[idx].view(-1)).mean()\n",
    "    optimizer.zero_grad(set_to_none=True); loss.backward(); optimizer.step()\n",
    "    loss_history.append(loss.item())\n",
    "    if step % 200 == 0: print(f\"Step {step} | Loss {loss.item():.4f}\")\n",
    "\n",
    "plt.plot(loss_history, color='#34A853')\n",
    "plt.title(\"Curva de Alinhamento (SFT)\")\n",
    "plt.show()"
   ],
   "metadata": { "id": "train-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def ask(model, question):\n",
    "    model.eval()\n",
    "    # Usamos o mesmo template que o modelo aprendeu\n",
    "    prompt = f\"### comando:\\n{question.lower()}\\n### resposta:\\n\"\n",
    "    idx = torch.tensor(encode(prompt)).unsqueeze(0).to(device)\n",
    "    for _ in range(60):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "        if itos[next_id.item()] == \".\": break\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "print(\"ðŸ¤– TESTE DE INTERAÃ‡ÃƒO:\")\n",
    "print(\"-\" * 30)\n",
    "print(ask(model, \"o que o gato fez?\"))\n",
    "print(\"\\n---\\n\")\n",
    "print(ask(model, \"o que e machine learning?\"))"
   ],
   "metadata": { "id": "test-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ ConclusÃ£o da Jornada\n",
    "\n",
    "ParabÃ©ns! O modelo agora entende os caracteres de controle e o formato de diÃ¡logo. \n",
    "\n",
    "![AvaliaÃ§Ã£o](./infograficos/05-avaliacao-respostas.png)"
   ],
   "metadata": { "id": "footer" }
  }
 ]\n,
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.12.3" }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}