{
  "cells": [
      {
       "cell_type": "markdown",
       "source": [
        "# Cap√≠tulo 07 ‚Äî Instruction Tuning\n",
        "\n",
        "Este notebook acompanha o Cap√≠tulo 07 da s√©rie **Fazendo um LLM do Zero**.\n",
        "\n",
        "Neste notebook vamos ensinar o modelo a seguir instru√ß√µes humanas.\n",
        "\n",
        "üéØ **Objetivos deste notebook:**\n",
        "- Interpretar instru√ß√µes\n",
        "- Responder perguntas\n",
        "- Produzir respostas orientadas\n",
        "\n",
        "Este √© o passo que transforma modelos base em assistentes conversacionais.\n"
       ],
       "metadata": {
        "id": "yHYcgMwubMDn"
       },
       "id": "yHYcgMwubMDn"
      },
      {
       "cell_type": "markdown",
       "source": [
        "## 1. Setup e Configura√ß√£o"
       ],
       "metadata": {
        "id": "setup-header"
       },
       "id": "setup-header"
      },
      {
       "cell_type": "code",
       "source": [
        "# ============================================================\n",
        "# Setup do reposit√≥rio\n",
        "# ============================================================\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/vongrossi/fazendo-um-llm-do-zero.git\"\n",
        "REPO_DIR = \"fazendo-um-llm-do-zero\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone {REPO_URL}\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print(\"Diret√≥rio atual:\", os.getcwd())\n"
       ],
       "metadata": {
        "id": "88oBgcoFbLC8"
       },
       "id": "88oBgcoFbLC8",
       "execution_count": null,
       "outputs": []
      },
      {
       "cell_type": "markdown",
       "source": [
        "### 1.1 Depend√™ncias e Imports"
       ],
       "metadata": {
        "id": "4V5oP5n0bR7N"
       },
       "id": "4V5oP5n0bR7N"
      },
      {
       "cell_type": "code",
       "source": [
        "!pip -q install -r 07-instruction-tuning/requirements.txt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# Adiciona raiz ao path para imports locais\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(42)\n"
       ],
       "metadata": {
        "id": "tUTLQ9NybatA"
       },
       "id": "tUTLQ9NybatA",
       "execution_count": null,
       "outputs": []
      },    {
      "cell_type": "markdown",
      "source": [
        "Importando criado GPTMini"
      ],
      "metadata": {
        "id": "LTaNuvpKbepE"
      },
      "id": "LTaNuvpKbepE"
    },
      {
       "cell_type": "code",
       "source": [
        "from lib.gptmini import GPTConfig, GPTMini"
       ],
       "metadata": {
        "id": "mX0-rwvybhgz"
       },
       "id": "mX0-rwvybhgz",
       "execution_count": null,
       "outputs": []
      },    {
      "cell_type": "markdown",
      "source": [
        "Carregar Pesos do criado no Cap√≠tulo 05"
      ],
      "metadata": {
        "id": "ZMIF93vEbj1w"
      },
      "id": "ZMIF93vEbj1w"
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"05-pre-treinamento/gpt_checkpoint.pt\"\n",
        "\n",
        "config = GPTConfig(\n",
        "    vocab_size=2000,\n",
        "    context_size=64,\n",
        "    d_model=128,\n",
        "    n_heads=4,\n",
        "    n_layers=2\n",
        ")\n",
        "\n",
        "backbone = GPTMini(config).to(device)\n",
        "\n",
        "try:\n",
        "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "    backbone.load_state_dict(ckpt, strict=False)\n",
        "    print(\"Pesos do Cap 05 carregados ‚úÖ\")\n",
        "except:\n",
        "    print(\"Checkpoint n√£o encontrado ‚Äî usando modelo inicial\")\n"
      ],
      "metadata": {
        "id": "CLDeXzeKbodE"
      },
      "id": "CLDeXzeKbodE",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "## 3. Dataset de Instru√ß√µes\n",
        "\n",
        "### 3.1 Dataset M√©dio"
       ],
       "metadata": {
        "id": "vYw3-VGCbvaR"
       },
       "id": "vYw3-VGCbvaR"
      },    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    {\n",
        "        \"instruction\": \"Explique o que √© Machine Learning\",\n",
        "        \"response\": \"Machine Learning √© uma √°rea da intelig√™ncia artificial que permite que sistemas aprendam padr√µes a partir de dados.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Traduza para ingl√™s: bom dia\",\n",
        "        \"response\": \"Good morning\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Explique o que √© um token em LLMs\",\n",
        "        \"response\": \"Token √© uma unidade de texto convertida em representa√ß√£o num√©rica usada pelo modelo.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Resuma: Modelos GPT usam Transformers\",\n",
        "        \"response\": \"Modelos GPT utilizam arquitetura Transformer para processar linguagem.\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "xizXMmFEbzZZ"
      },
      "id": "xizXMmFEbzZZ",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "### 3.2 Fun√ß√£o de Prompt Estruturado"
       ],
       "metadata": {
        "id": "QtgOGHuxb8y8"
       },
       "id": "QtgOGHuxb8y8"
      },    {
      "cell_type": "code",
      "source": [
        "def format_prompt(item):\n",
        "    return f\"\"\"\n",
        "### Instru√ß√£o:\n",
        "{item['instruction']}\n",
        "\n",
        "### Resposta:\n",
        "{item['response']}\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "t7MUM3wNb9og"
      },
      "id": "t7MUM3wNb9og",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "### 3.3 Tokeniza√ß√£o Simples (Char-level)"
       ],
       "metadata": {
        "id": "kX6sdpJHcA2_"
       },
       "id": "kX6sdpJHcA2_"
      },    {
      "cell_type": "code",
      "source": [
        "all_text = \"\".join([format_prompt(d) for d in dataset])\n",
        "\n",
        "chars = sorted(set(all_text))\n",
        "stoi = {c:i for i,c in enumerate(chars)}\n",
        "itos = {i:c for c,i in stoi.items()}\n",
        "\n",
        "vocab_size = len(chars)\n"
      ],
      "metadata": {
        "id": "aHsJ_UxvcBmN"
      },
      "id": "aHsJ_UxvcBmN",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "### 3.4 Encoder/Decoder"
       ],
       "metadata": {
        "id": "yXCn3ufrcEGT"
       },
       "id": "yXCn3ufrcEGT"
      },    {
      "cell_type": "code",
      "source": [
        "def encode(text):\n",
        "    return [stoi[c] for c in text if c in stoi]\n",
        "\n",
        "def decode(tokens):\n",
        "    return \"\".join([itos[t] for t in tokens])\n"
      ],
      "metadata": {
        "id": "7NnnmUh8cGvE"
      },
      "id": "7NnnmUh8cGvE",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "### 3.5 Criar Dataset com Masking"
       ],
       "metadata": {
        "id": "Tb72JOWccIuc"
       },
       "id": "Tb72JOWccIuc"
      },    {
      "cell_type": "code",
      "source": [
        "def build_instruction_dataset(data, context_size=64):\n",
        "\n",
        "    X, Y, mask = [], [], []\n",
        "\n",
        "    for item in data:\n",
        "        prompt = f\"### Instru√ß√£o:\\n{item['instruction']}\\n\\n### Resposta:\\n\"\n",
        "        full = prompt + item[\"response\"]\n",
        "\n",
        "        prompt_tokens = encode(prompt)\n",
        "        full_tokens = encode(full)\n",
        "\n",
        "        for i in range(len(full_tokens)-context_size):\n",
        "\n",
        "            x = full_tokens[i:i+context_size]\n",
        "            y = full_tokens[i+1:i+context_size+1]\n",
        "\n",
        "            m = [0]*len(prompt_tokens)\n",
        "            m = m + [1]*(context_size-len(prompt_tokens))\n",
        "\n",
        "            X.append(x)\n",
        "            Y.append(y)\n",
        "            mask.append(m[:context_size])\n",
        "\n",
        "    return (\n",
        "        torch.tensor(X),\n",
        "        torch.tensor(Y),\n",
        "        torch.tensor(mask)\n",
        "    )\n",
        "\n",
        "context_size = 64\n",
        "X, Y, MASK = build_instruction_dataset(dataset, context_size)\n",
        "\n",
        "X, Y, MASK = X.to(device), Y.to(device), MASK.to(device)\n"
      ],
      "metadata": {
        "id": "lDoU1pMNcK61"
      },
      "id": "lDoU1pMNcK61",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "## 4. Modelo Instruction Tuned\n",
        "\n",
        "Wrapper com Loss Mascarada"
       ],
       "metadata": {
        "id": "j6JQWaxbcNRb"
       },
       "id": "j6JQWaxbcNRb"
      },    {
      "cell_type": "code",
      "source": [
        "class InstructionGPT(nn.Module):\n",
        "\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "\n",
        "        logits, _ = self.backbone(x)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if y is not None:\n",
        "\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                y.view(-1),\n",
        "                reduction=\"none\"\n",
        "            )\n",
        "\n",
        "            loss = loss * mask.view(-1)\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "model = InstructionGPT(backbone).to(device)\n"
      ],
      "metadata": {
        "id": "wBRPXMpXcT15"
      },
      "id": "wBRPXMpXcT15",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "## 5. Treinamento SFT\n",
        "\n",
        "Otimizador"
       ],
       "metadata": {
        "id": "UKJhslsicZyw"
       },
       "id": "UKJhslsicZyw"
      },    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n"
      ],
      "metadata": {
        "id": "g7tkKnB3ce8b"
      },
      "id": "g7tkKnB3ce8b",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "### 5.1 Loop de Treino"
       ],
       "metadata": {
        "id": "w_MsuhqEciRP"
       },
       "id": "w_MsuhqEciRP"
      },    {
      "cell_type": "code",
      "source": [
        "loss_history = []\n",
        "\n",
        "for step in range(300):\n",
        "\n",
        "    idx = torch.randint(0, X.size(0), (8,))\n",
        "    xb, yb, mb = X[idx], Y[idx], MASK[idx]\n",
        "\n",
        "    logits, loss = model(xb, yb, mb)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        print(step, loss.item())\n"
      ],
      "metadata": {
        "id": "U1eiYXkhci9L"
      },
      "id": "U1eiYXkhci9L",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "### 5.2 Visualizar Loss"
       ],
       "metadata": {
        "id": "cNOkvgioclLW"
       },
       "id": "cNOkvgioclLW"
      },    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.title(\"Loss Instruction Tuning\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TBz9cXLicoAE"
      },
      "id": "TBz9cXLicoAE",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "## 6. Compara√ß√£o e Teste\n",
        "\n",
        "Fun√ß√£o de Gera√ß√£o"
       ],
       "metadata": {
        "id": "Q1UCRxoEcpn3"
       },
       "id": "Q1UCRxoEcpn3"
      },    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, start, max_tokens=80):\n",
        "\n",
        "    tokens = encode(start)\n",
        "    tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    for _ in range(max_tokens):\n",
        "        logits, _ = model(tokens)\n",
        "        next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
        "        tokens = torch.cat([tokens, next_token.unsqueeze(1)], dim=1)\n",
        "\n",
        "    return decode(tokens.squeeze().tolist())\n"
      ],
      "metadata": {
        "id": "PJ51BOYHcvMA"
      },
      "id": "PJ51BOYHcvMA",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "### 6.1 Testando o modelo"
       ],
       "metadata": {
        "id": "_teBygOZcw2M"
       },
       "id": "_teBygOZcw2M"
      },    {
      "cell_type": "code",
      "source": [
        "prompt = \"### Instru√ß√£o:\\nExplique o que √© Machine Learning\\n\\n### Resposta:\\n\"\n",
        "\n",
        "print(generate(model, prompt))\n"
      ],
      "metadata": {
        "id": "Dg5Sks5rc1zs"
      },
      "id": "Dg5Sks5rc1zs",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "## 7. Salvar Checkpoint Final"
       ],
       "metadata": {
        "id": "MMbzHP-3c52j"
       },
       "id": "MMbzHP-3c52j"
      },    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"07_instruction_gpt.pt\")\n"
      ],
      "metadata": {
        "id": "atEAnTBOc8t0"
      },
      "id": "atEAnTBOc8t0",
      "execution_count": null,
      "outputs": []
    },
      {
       "cell_type": "markdown",
       "source": [
        "## 8. Encerramento da S√©rie\n",
        "\n",
        "Neste cap√≠tulo voc√™ ensinou um modelo a:\n",
        "\n",
        "‚Ä¢ Interpretar instru√ß√µes  \n",
        "‚Ä¢ Produzir respostas orientadas  \n",
        "‚Ä¢ Aprender comportamento conversacional  \n",
        "\n",
        "Voc√™ percorreu toda a jornada:\n",
        "\n",
        "Texto ‚Üí Tokens ‚Üí Aten√ß√£o ‚Üí GPT ‚Üí Treinamento ‚Üí Fine-Tuning ‚Üí Instruction Tuning\n",
        "\n",
        "Este √© o pipeline fundamental dos assistentes baseados em LLMs modernos.\n"
       ],
       "metadata": {
        "id": "cn8GHMsGc_3Z"
       },
       "id": "cn8GHMsGc_3Z"
      }  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}