{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cap√≠tulo 07 ‚Äî Instruction Tuning: Criando um Assistente\n",
        "\n",
        "üéØ **Objetivos:** Transformar o modelo completador em um assistente √∫til usando **SFT (Supervised Fine-Tuning)**.\n",
        "\n",
        "![SFT](./infograficos/04-pipeline-sft.png)"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Setup do reposit√≥rio no Colab\n",
        "# ============================================================\n",
        "import os, sys\n",
        "REPO_NAME = \"fazendo-um-llm-do-zero\"\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    if not os.path.exists(REPO_NAME):\n",
        "        get_ipython().system(f\"git clone https://github.com/vongrossi/{REPO_NAME}.git\")\n",
        "    if os.path.exists(REPO_NAME) and os.getcwd().split('/')[-1] != REPO_NAME:\n",
        "        os.chdir(REPO_NAME)\n",
        "if os.getcwd() not in sys.path: sys.path.append(os.getcwd())\n",
        "print(\"üìÇ Diret√≥rio atual:\", os.getcwd())"
      ],
      "metadata": {
        "id": "setup-repo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from lib.gptmini import GPTConfig, GPTMini\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# üìÇ Carregamento do Checkpoint do Cap 05\n",
        "if not os.path.exists(\"gpt_checkpoint.pt\"):\n",
        "    from google.colab import files\n",
        "    print(\"üì§ Por favor, suba o 'gpt_checkpoint.pt' gerado no Cap√≠tulo 05:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "ckpt = torch.load(\"gpt_checkpoint.pt\", map_location=device, weights_only=False)\n",
        "stoi, itos = ckpt['stoi'], ckpt['itos']\n",
        "config = ckpt['config']\n",
        "context_size = config.context_size\n",
        "\n",
        "# Encoder: mapeia caracteres desconhecidos para espa√ßo (evita colapso do prompt)\n",
        "def encode(s):\n",
        "    res = []\n",
        "    unk_id = stoi.get(' ', 0)\n",
        "    for c in s.lower():\n",
        "        res.append(stoi.get(c, unk_id))\n",
        "    return res\n",
        "\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(f\"üß† Modelo pr√©-treinado carregado!\")\n",
        "print(f\"üìè Contexto: {context_size} | Vocabul√°rio: {len(stoi)} caracteres\")\n",
        "if '#' not in stoi: print(\"‚ö†Ô∏è AVISO: Seu checkpoint n√£o possui o caractere '#'. Re-execute o Cap√≠tulo 05 com o novo dataset.\")\n"
      ],
      "metadata": {
        "id": "setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset de Instru√ß√µes\n",
        "\n",
        "Criamos pares de Pergunta e Resposta para o alinhamento."
      ],
      "metadata": {
        "id": "data-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = [\n",
        "    {\"q\": \"o que o gato fez?\", \"a\": \"o gato subiu no telhado e pulou o muro.\"},\n",
        "    {\"q\": \"onde o cachorro dormiu?\", \"a\": \"o cachorro dormiu no sofa e no tapete.\"},\n",
        "    {\"q\": \"defina inteligencia artificial\", \"a\": \"inteligencia artificial e o estudo de algoritmos.\"},\n",
        "    {\"q\": \"o que e machine learning?\", \"a\": \"machine learning permite que sistemas aprendam padroes.\"}\n",
        "]\n",
        "\n",
        "def build_prompt_ids(question, answer=None, context_size=32):\n",
        "    # Prompt compacto sem pontuacao rara no vocab\n",
        "    prefix = \"pergunta\\n\"\n",
        "    suffix = \"resposta\\n\"\n",
        "    q_ids = encode(question)\n",
        "    if answer is not None:\n",
        "        # adiciona \"\\n\" ao final para ensinar parada\n",
        "        a_ids = encode(answer + \"\\n\")\n",
        "    else:\n",
        "        a_ids = []\n",
        "    base_ids = encode(prefix) + encode(suffix)\n",
        "\n",
        "    # Prioriza manter a pergunta inteira; trunca a resposta se necess√°rio\n",
        "    max_a = max(0, context_size + 1 - len(base_ids) - len(q_ids))\n",
        "    if max_a < len(a_ids):\n",
        "        a_ids = a_ids[:max_a]\n",
        "\n",
        "    # Se a pergunta for grande demais, trunca o in√≠cio para caber\n",
        "    max_q = max(1, context_size + 1 - len(base_ids) - len(a_ids))\n",
        "    if len(q_ids) > max_q:\n",
        "        q_ids = q_ids[-max_q:]\n",
        "\n",
        "    cmd_ids = encode(prefix) + q_ids + encode(suffix)\n",
        "    full_ids = cmd_ids + a_ids\n",
        "    return cmd_ids, full_ids\n",
        "\n",
        "def build_sft_dataset(data, context_size):\n",
        "    X, Y, masks = [], [], []\n",
        "    pad_id = stoi.get(' ', 0)\n",
        "    for item in data:\n",
        "        cmd_ids, full_ids = build_prompt_ids(item['q'], item['a'], context_size)\n",
        "        cmd_len = len(cmd_ids)\n",
        "        if len(full_ids) < 2:\n",
        "            continue\n",
        "        if len(full_ids) > context_size + 1:\n",
        "            full_ids = full_ids[: context_size + 1]\n",
        "            cmd_len = min(cmd_len, len(full_ids))\n",
        "        x = full_ids[:-1]\n",
        "        y = full_ids[1:]\n",
        "        if cmd_len >= len(x):\n",
        "            continue\n",
        "        # M√°scara: 0 no comando/padding, 1 na resposta\n",
        "        m = [1 if (i + 1) >= cmd_len else 0 for i in range(len(x))]\n",
        "        if len(x) < context_size:\n",
        "            pad_len = context_size - len(x)\n",
        "            x = x + [pad_id] * pad_len\n",
        "            y = y + [pad_id] * pad_len\n",
        "            m = m + [0] * pad_len\n",
        "        X.append(x)\n",
        "        Y.append(y)\n",
        "        masks.append(m)\n",
        "    return torch.tensor(X).to(device), torch.tensor(Y).to(device), torch.tensor(masks).to(device)\n",
        "\n",
        "X, Y, M = build_sft_dataset(instructions, context_size)\n",
        "print(f\"üì¶ Amostras de Alinhamento: {len(X)}\")\n"
      ],
      "metadata": {
        "id": "data-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Treinamento com M√°scara de Loss\n",
        "\n",
        "Otimizamos apenas a gera√ß√£o da resposta.\n",
        "\n",
        "![Masking](./infograficos/03-mascaramento-loss-resposta.png)"
      ],
      "metadata": {
        "id": "train-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTMini(config).to(device)\n",
        "model.load_state_dict(ckpt['state_dict'])\n",
        "# Desliga dropout para memorizar o pequeno dataset\n",
        "for m in model.modules():\n",
        "    if isinstance(m, nn.Dropout):\n",
        "        m.p = 0.0\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.0)\n",
        "\n",
        "loss_history = []\n",
        "model.train()\n",
        "print(\"üî® Alinhando o assistente...\")\n",
        "batch_size = len(X)\n",
        "for step in range(2001):\n",
        "    idx = torch.randint(len(X), (batch_size,))\n",
        "    logits, _ = model(X[idx])\n",
        "    B, T, V = logits.shape\n",
        "    loss = F.cross_entropy(logits.view(-1, V), Y[idx].view(-1), reduction='none')\n",
        "    mask = M[idx].view(-1)\n",
        "    loss = (loss * mask).sum() / mask.sum().clamp(min=1)\n",
        "    optimizer.zero_grad(set_to_none=True); loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    loss_history.append(loss.item())\n",
        "    if step % 500 == 0: print(f\"Step {step} | Loss {loss.item():.4f}\")\n",
        "\n",
        "plt.plot(loss_history, color='#34A853')\n",
        "plt.title(\"Curva de Alinhamento (SFT)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "train-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Teste do Assistente Alinhado\n",
        "\n",
        "O modelo agora responde apenas o que foi solicitado."
      ],
      "metadata": {
        "id": "test-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def ask(model, question, max_new_tokens=80, no_repeat_ngram=3):\n",
        "    model.eval()\n",
        "    cmd_ids, _ = build_prompt_ids(question, answer=None, context_size=context_size)\n",
        "    idx = torch.tensor(cmd_ids).unsqueeze(0).to(device)\n",
        "    prompt_len = len(cmd_ids)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        logits, _ = model(idx_cond)\n",
        "        # no-repeat ngram (n=3)\n",
        "        if no_repeat_ngram and idx.shape[1] >= no_repeat_ngram - 1:\n",
        "            n = no_repeat_ngram\n",
        "            prefix = tuple(idx[0, -(n-1):].tolist())\n",
        "            seen = set()\n",
        "            seq = idx[0].tolist()\n",
        "            for i in range(len(seq) - n + 1):\n",
        "                seen.add(tuple(seq[i:i+n]))\n",
        "            for cand in range(logits.shape[-1]):\n",
        "                if prefix + (cand,) in seen:\n",
        "                    logits[0, -1, cand] = -float('inf')\n",
        "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "        if itos[next_id.item()] in ['\\n', '.']:\n",
        "            break\n",
        "\n",
        "    # Retornamos apenas a parte gerada (Resposta)\n",
        "    return decode(idx[0][prompt_len:].tolist())\n",
        "\n",
        "print(\"ü§ñ TESTE DE INTERA√á√ÉO:\")\n",
        "print(\"-\" * 30)\n",
        "q1 = \"o que o gato fez?\"\n",
        "print(f\"Pergunta: {q1}\\nResposta: {ask(model, q1)}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 30)\n",
        "q2 = \"o que e machine learning?\"\n",
        "print(f\"Pergunta: {q2}\\nResposta: {ask(model, q2)}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 30)\n",
        "q3 = \"defina inteligencia artificial\"\n",
        "print(f\"Pergunta: {q3}\\nResposta: {ask(model, q3)}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 30)\n",
        "q4 = \"onde o cachorro dormiu?\"\n",
        "print(f\"Pergunta: {q4}\\nResposta: {ask(model, q4)}\")\n"
      ],
      "metadata": {
        "id": "test-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèÅ Conclus√£o da Jornada\n",
        "\n",
        "Voc√™ completou a s√©rie! \n",
        "\n",
        "Transformou um modelo estat√≠stico em um assistente capaz de seguir inten√ß√µes humanas. Este √© o fundamento do alinhamento de IA.\n",
        "\n",
        "![Avalia√ß√£o](./infograficos/05-avaliacao-respostas.png)"
      ],
      "metadata": {
        "id": "footer"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
