{
  "cells": [
      {
       "cell_type": "markdown",
       "source": [
        "# Cap√≠tulo 07 ‚Äî Instruction Tuning\n",
        "\n",
        "Este notebook acompanha o Cap√≠tulo 07 da s√©rie **Fazendo um LLM do Zero**.\n",
        "\n",
        "üéØ **Objetivos deste notebook:**\n",
        "- Interpretar instru√ß√µes estruturadas\n",
        "- Implementar Mascaramento de Loss para focar na resposta\n",
        "- Transformar um modelo base em um assistente conversacional\n"
       ],
       "metadata": {
        "id": "yHYcgMwubMDn"
       }
      },
      {
       "cell_type": "markdown",
       "source": [
        "## 1. Setup e Configura√ß√£o"
       ],
       "metadata": {
        "id": "setup-header"
       }
      },
      {
       "cell_type": "code",
       "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "REPO_URL = \"https://github.com/vongrossi/fazendo-um-llm-do-zero.git\"\n",
        "REPO_DIR = \"fazendo-um-llm-do-zero\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone {REPO_URL}\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "sys.path.append(os.getcwd())\n",
        "print(\"Diret√≥rio atual:\", os.getcwd())\n"
       ],
       "metadata": {
        "id": "88oBgcoFbLC8"
       },
       "execution_count": null,
       "outputs": []
      },
      {
       "cell_type": "code",
       "source": [
        "!pip -q install -r 07-instruction-tuning/requirements.txt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import numpy as np\n",
        "from lib.gptmini import GPTConfig, GPTMini\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(42)\n"
       ],
       "metadata": {
        "id": "tUTLQ9NybatA"
       },
       "execution_count": null,
       "outputs": []
      },
      {
       "cell_type": "markdown",
       "source": [
        "## 2. Dataset e Vocabul√°rio\n",
        "\n",
        "Diferente dos cap√≠tulos anteriores, agora trabalhamos com exemplos de **Comando** e **Resposta**. Definimos o vocabul√°rio antes de carregar o modelo."
       ],
       "metadata": {
        "id": "dataset-header"
       }
      },
      {
       "cell_type": "code",
       "source": [
        "dataset = [\n",
        "    {\"instruction\": \"Explique Machine Learning\", \"response\": \"√â uma √°rea da IA que permite aprender padr√µes a partir de dados.\"},\n",
        "    {\"instruction\": \"Traduza: bom dia\", \"response\": \"Good morning\"},\n",
        "    {\"instruction\": \"O que √© um token?\", \"response\": \"√â a unidade b√°sica de texto processada pelo modelo.\"},\n",
        "    {\"instruction\": \"Resuma: GPTs s√£o Transformers\", \"response\": \"Modelos GPT usam arquitetura Transformer para linguagem.\"}\n",
        "]\n",
        "\n",
        "def format_instruction(item):\n",
        "    return f\"### Instru√ß√£o:\\n{item['instruction']}\\n\\n### Resposta:\\n{item['response']}\"\n",
        "\n",
        "all_text = \"\".join([format_instruction(d) for d in dataset])\n",
        "chars = sorted(set(all_text))\n",
        "stoi = {c:i for i,c in enumerate(chars)}\n",
        "itos = {i:c for c,i in stoi.items()}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "def encode(text): return [stoi[c] for c in text if c in stoi]\n",
        "def decode(tokens): return \"\".join([itos[t] for t in tokens])\n",
        "\n",
        "print(f\"Vocabul√°rio carregado: {vocab_size} caracteres\")\n"
       ],
       "metadata": {
        "id": "xizXMmFEbzZZ"
       },
       "execution_count": null,
       "outputs": []
      },
      {
       "cell_type": "markdown",
       "source": [
        "## 3. Inicializa√ß√£o e Carregamento de Pesos\n",
        "\n",
        "Vamos carregar o modelo pr√©-treinado no Cap√≠tulo 05 para servir de c√©rebro base."
       ],
       "metadata": {
        "id": "model-init-header"
       }
      },
      {
       "cell_type": "code",
       "source": [
        "config = GPTConfig(\n",
        "    vocab_size=vocab_size, \n",
        "    context_size=64,\n",
        "    d_model=128,\n",
        "    n_heads=4,\n",
        "    n_layers=2\n",
        ")\n",
        "backbone = GPTMini(config).to(device)\n",
        "\n",
        "checkpoint_path = \"gpt_checkpoint.pt\"\n",
        "try:\n",
        "    p = checkpoint_path if os.path.exists(checkpoint_path) else \"05-pre-treinamento/\" + checkpoint_path\n",
        "    ckpt = torch.load(p, map_location=device)\n",
        "    state_dict = ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
        "    model_dict = backbone.state_dict()\n",
        "    # Filtra apenas o que √© compat√≠vel\n",
        "    pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
        "    backbone.load_state_dict(pretrained_dict, strict=False)\n",
        "    print(f\"‚úÖ {len(pretrained_dict)} camadas do Cap√≠tulo 05 carregadas!\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Checkpoint n√£o encontrado ‚Äî iniciando modelo do zero\")\n"
       ],
       "metadata": {
        "id": "CLDeXzeKbodE"
       },
       "execution_count": null,
       "outputs": []
      },
      {
       "cell_type": "markdown",
       "source": [
        "## 4. Pipeline de Treinamento SFT\n",
        "\n",
        "Preparamos o dataset com a **M√°scara de Loss** para que o modelo aprenda apenas com a resposta."
       ],
       "metadata": {
        "id": "sft-header"
       }
      },
      {
       "cell_type": "code",
       "source": [
        "def build_instruction_dataset(data, context_size=64):\n",
        "    X, Y, mask = [], [], []\n",
        "    for item in data:\n",
        "        input_text = f\"### Instru√ß√£o:\\n{item['instruction']}\\n\\n### Resposta:\\n\"\n",
        "        full = input_text + item[\"response\"]\n",
        "        instruction_tokens = encode(input_text)\n",
        "        full_tokens = encode(full)\n",
        "\n",
        "        for i in range(len(full_tokens)-context_size):\n",
        "            x = full_tokens[i:i+context_size]\n",
        "            y = full_tokens[i+1:i+context_size+1]\n",
        "            m = [0]*len(instruction_tokens) + [1]*(context_size-len(instruction_tokens))\n",
        "            X.append(x); Y.append(y); mask.append(m[:context_size])\n",
        "\n",
        "    return torch.tensor(X).to(device), torch.tensor(Y).to(device), torch.tensor(mask).to(device)\n",
        "\n",
        "X, Y, MASK = build_instruction_dataset(dataset)\n",
        "\n",
        "class InstructionGPT(nn.Module):\n",
        "    def __init__(self, backbone): \n",
        "        super().__init__(); self.backbone = backbone\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        logits, _ = self.backbone(x)\n",
        "        loss = None\n",
        "        if y is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), reduction=\"none\")\n",
        "            loss = (loss * mask.view(-1)).mean()\n",
        "        return logits, loss\n",
        "\n",
        "model = InstructionGPT(backbone).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n"
       ],
       "metadata": {
        "id": "lDoU1pMNcK61"
       },
       "execution_count": null,
       "outputs": []
      },
      {
       "cell_type": "code",
       "source": [
        "print(\"üöÄ Treinando Alinhamento...\")\n",
        "for step in range(301):\n",
        "    idx = torch.randint(0, X.size(0), (8,))\n",
        "    logits, loss = model(X[idx], Y[idx], MASK[idx])\n",
        "    optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "    if step % 100 == 0: print(f\"Step {step} | Loss {loss.item():.4f}\")\n"
       ],
       "metadata": {
        "id": "U1eiYXkhci9L"
       },
       "execution_count": null,
       "outputs": []
      },
      {
       "cell_type": "markdown",
       "source": [
        "## 5. Testando o Assistente\n",
        "\n",
        "Agora o modelo deve ser capaz de responder comandos."
       ],
       "metadata": {
        "id": "test-header"
       }
      },
      {
       "cell_type": "code",
       "source": [
        "@torch.no_grad()\n",
        "def generate(model, start, max_tokens=80):\n",
        "    tokens = encode(start)\n",
        "    tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "    for _ in range(max_tokens):\n",
        "        logits, _ = model(tokens)\n",
        "        next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
        "        tokens = torch.cat([tokens, next_token.unsqueeze(1)], dim=1)\n",
        "    return decode(tokens.squeeze().tolist())\n",
        "\n",
        "print(generate(model, \"### Instru√ß√£o:\\nExplique Machine Learning\\n\\n### Resposta:\\n\"))\n"
       ],
       "metadata": {
        "id": "PJ51BOYHcvMA"
       },
       "execution_count": null,
       "outputs": []
      },
      {
       "cell_type": "code",
       "source": [
        "torch.save(model.state_dict(), \"07_instruction_gpt.pt\")\n",
        "print(\"‚úÖ Checkpoint final salvo!\")\n"
       ],
       "metadata": {
        "id": "atEAnTBOc8t0"
       },
       "execution_count": null,
       "outputs": []
      },
      {
       "cell_type": "markdown",
       "source": [
        "## 6. Conclus√£o\n",
        "\n",
        "Voc√™ completou a s√©rie! \n",
        "\n",
        "Transformou um modelo que apenas previa o pr√≥ximo token em um assistente capaz de seguir inten√ß√µes humanas. \n",
        "\n",
        "Este √© o fundamento do que faz o ChatGPT, o Claude e o Gemini serem t√£o √∫teis no nosso dia a dia."
       ],
       "metadata": {
        "id": "cn8GHMsGc_3Z"
       }
      }  ],
  "metadata": {
    "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
    "language_info": { "name": "python", "version": "3.12.3" },
    "colab": { "provenance": [] }
  },
  "nbformat": 4, "nbformat_minor": 5
}