{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CapÃ­tulo 07 â€” Instruction Tuning: Criando um Assistente\n",
    "\n",
    "ðŸŽ¯ **Objetivos:** Transformar o modelo completador em um assistente Ãºtil usando **SFT (Supervised Fine-Tuning)**.\n",
    "\n",
    "![SFT](./infograficos/04-pipeline-sft.png)"
   ],
   "metadata": { "id": "header" }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Setup do repositÃ³rio no Colab\n",
    "# ============================================================\n",
    "import os, sys\n",
    "REPO_NAME = \"fazendo-um-llm-do-zero\"\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if not os.path.exists(REPO_NAME):\n",
    "        !git clone https://github.com/vongrossi/{REPO_NAME}.git\n",
    "    if os.path.exists(REPO_NAME) and os.getcwd().split('/')[-1] != REPO_NAME:\n",
    "        os.chdir(REPO_NAME)\n",
    "if os.getcwd() not in sys.path: sys.path.append(os.getcwd())\n",
    "print(\"ðŸ“‚ DiretÃ³rio atual:\", os.getcwd())"
   ],
   "metadata": { "id": "setup-repo" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.gptmini import GPTConfig, GPTMini\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"âœ… Usando: {device}\")\n",
    "\n",
    "# ðŸ“‚ Carregamento do Checkpoint\n",
    "if not os.path.exists(\"gpt_checkpoint.pt\"):\n",
    "    from google.colab import files\n",
    "    print(\"ðŸ“¤ Por favor, faÃ§a o upload do 'gpt_checkpoint.pt' gerado no CapÃ­tulo 05:\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "ckpt = torch.load(\"gpt_checkpoint.pt\", map_location=device)\n",
    "stoi, itos = ckpt['stoi'], ckpt['itos']\n",
    "encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "print(f\"ðŸ§  VocabulÃ¡rio Carregado: {vocab_size} caracteres.\")"
   ],
   "metadata": { "id": "setup" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Dataset de InstruÃ§Ãµes\n",
    "\n",
    "Ensinamos o modelo que a estrutura `### Comando:` pede uma resposta em `### Resposta:`."
   ],
   "metadata": { "id": "data-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "instructions = [\n",
    "    {\"q\": \"o que o gato fez?\", \"a\": \"o gato subiu no telhado e pulou o muro.\"},\n",
    "    {\"q\": \"onde o cachorro dormiu?\", \"a\": \"o cachorro dormiu no tapete do sofa.\"},\n",
    "    {\"q\": \"defina inteligencia artificial\", \"a\": \"artificial inteligÃªncia e o estudo de algoritmos.\"},\n",
    "    {\"q\": \"o que e machine learning?\", \"a\": \"machine learning permite que sistemas aprendam padroes.\"}\n",
    "]\n",
    "\n",
    "def build_sft_dataset(data, context_size=64):\n",
    "    X, Y, masks = [], [], []\n",
    "    for item in data:\n",
    "        cmd = f\"### Comando:\\n{item['q']}\\n\\n### Resposta:\\n\"\n",
    "        full = cmd + item['a']\n",
    "        \n",
    "        ids = encode(full)\n",
    "        cmd_len = len(encode(cmd))\n",
    "        \n",
    "        for i in range(len(ids) - context_size):\n",
    "            X.append(ids[i : i+context_size])\n",
    "            Y.append(ids[i+1 : i+context_size+1])\n",
    "            # MÃ¡scara: 0 no comando, 1 na resposta\n",
    "            m = [0]*cmd_len + [1]*(context_size - cmd_len)\n",
    "            masks.append(m[:context_size])\n",
    "            \n",
    "    return torch.tensor(X).to(device), torch.tensor(Y).to(device), torch.tensor(masks).to(device)\n",
    "\n",
    "X, Y, M = build_sft_dataset(instructions)\n",
    "print(f\"Amostras de Alinhamento: {len(X)}\")"
   ],
   "metadata": { "id": "data-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Treinamento com MÃ¡scara de Loss\n",
    "\n",
    "Otimizamos apenas a geraÃ§Ã£o da resposta.\n",
    "\n",
    "![Masking](./infograficos/03-mascaramento-loss-resposta.png)"
   ],
   "metadata": { "id": "train-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "model = GPTMini(ckpt['config']).to(device)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "loss_history = []\n",
    "model.train()\n",
    "for step in range(501):\n",
    "    idx = torch.randint(len(X), (8,))\n",
    "    logits, _ = model(X[idx])\n",
    "    \n",
    "    # CÃ¡lculo manual da Loss Mascarada\n",
    "    B, T, V = logits.shape\n",
    "    loss = F.cross_entropy(logits.view(-1, V), Y[idx].view(-1), reduction='none')\n",
    "    loss = (loss * M[idx].view(-1)).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    if step % 100 == 0: print(f\"Step {step} | Loss {loss.item():.4f}\")\n",
    "\n",
    "plt.plot(loss_history, color='#34A853')\n",
    "plt.title(\"Curva de Alinhamento (SFT Loss)\")\n",
    "plt.show()"
   ],
   "metadata": { "id": "train-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Teste do Assistente Alinhado\n",
    "\n",
    "O modelo agora entende o protocolo de diÃ¡logo."
   ],
   "metadata": { "id": "test-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def ask(model, question):\n",
    "    model.eval()\n",
    "    prompt = f\"### Comando:\\n{question.lower()}\\n\\n### Resposta:\\n\"\n",
    "    idx = torch.tensor(encode(prompt)).unsqueeze(0).to(device)\n",
    "    for _ in range(60):\n",
    "        logits, _ = model(idx[:, -32:])\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "        if itos[next_id.item()] == \".\": break\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "print(\"ðŸ¤– RESPOSTA:\")\n",
    "print(ask(model, \"o que o gato fez?\"))"
   ],
   "metadata": { "id": "test-code" },
   "execution_count": null,
   "outputs": []
  }
 ]\n,
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.12.3" }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}