{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "\n",
        "REPO_URL = \"https://github.com/vongrossi/fazendo-um-llm-do-zero.git\"\n",
        "REPO_DIR = \"fazendo-um-llm-do-zero\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone {REPO_URL}\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "print(\"DiretÃ³rio atual:\", os.getcwd())\n"
      ],
      "metadata": {
        "id": "88oBgcoFbLC8"
      },
      "id": "88oBgcoFbLC8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CapÃ­tulo 07 â€” Instruction Tuning\n",
        "\n",
        "Neste notebook vamos ensinar o modelo a seguir instruÃ§Ãµes humanas.\n",
        "\n",
        "AtÃ© agora o modelo aprendeu:\n",
        "\n",
        "â€¢ Como linguagem funciona  \n",
        "â€¢ Como gerar texto  \n",
        "â€¢ Como resolver tarefas supervisionadas  \n",
        "\n",
        "Agora vamos ensinar o modelo a:\n",
        "\n",
        "ðŸ‘‰ Interpretar instruÃ§Ãµes  \n",
        "ðŸ‘‰ Responder perguntas  \n",
        "ðŸ‘‰ Produzir respostas orientadas  \n",
        "\n",
        "Este Ã© o passo que transforma modelos base em assistentes conversacionais.\n"
      ],
      "metadata": {
        "id": "yHYcgMwubMDn"
      },
      "id": "yHYcgMwubMDn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports e configuraÃ§Ã£o de uso de GPU"
      ],
      "metadata": {
        "id": "4V5oP5n0bR7N"
      },
      "id": "4V5oP5n0bR7N"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(42)\n"
      ],
      "metadata": {
        "id": "tUTLQ9NybatA"
      },
      "id": "tUTLQ9NybatA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando criado GPTMini"
      ],
      "metadata": {
        "id": "LTaNuvpKbepE"
      },
      "id": "LTaNuvpKbepE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "from lib.gptmini import GPTConfig, GPTMini\n"
      ],
      "metadata": {
        "id": "mX0-rwvybhgz"
      },
      "id": "mX0-rwvybhgz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregar Pesos do criado no CapÃ­tulo 05"
      ],
      "metadata": {
        "id": "ZMIF93vEbj1w"
      },
      "id": "ZMIF93vEbj1w"
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"05-pre-treinamento/gpt_checkpoint.pt\"\n",
        "\n",
        "config = GPTConfig(\n",
        "    vocab_size=2000,\n",
        "    context_size=64,\n",
        "    d_model=128,\n",
        "    n_heads=4,\n",
        "    n_layers=2\n",
        ")\n",
        "\n",
        "backbone = GPTMini(config).to(device)\n",
        "\n",
        "try:\n",
        "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "    backbone.load_state_dict(ckpt, strict=False)\n",
        "    print(\"Pesos do Cap 05 carregados âœ…\")\n",
        "except:\n",
        "    print(\"Checkpoint nÃ£o encontrado â€” usando modelo inicial\")\n"
      ],
      "metadata": {
        "id": "CLDeXzeKbodE"
      },
      "id": "CLDeXzeKbodE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“Š Parte A â€” Dataset de InstruÃ§Ãµes\n",
        "\n",
        "Dataset MÃ©dio de InstruÃ§Ãµes"
      ],
      "metadata": {
        "id": "vYw3-VGCbvaR"
      },
      "id": "vYw3-VGCbvaR"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    {\n",
        "        \"instruction\": \"Explique o que Ã© Machine Learning\",\n",
        "        \"response\": \"Machine Learning Ã© uma Ã¡rea da inteligÃªncia artificial que permite que sistemas aprendam padrÃµes a partir de dados.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Traduza para inglÃªs: bom dia\",\n",
        "        \"response\": \"Good morning\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Explique o que Ã© um token em LLMs\",\n",
        "        \"response\": \"Token Ã© uma unidade de texto convertida em representaÃ§Ã£o numÃ©rica usada pelo modelo.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Resuma: Modelos GPT usam Transformers\",\n",
        "        \"response\": \"Modelos GPT utilizam arquitetura Transformer para processar linguagem.\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "xizXMmFEbzZZ"
      },
      "id": "xizXMmFEbzZZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FunÃ§Ã£o de Prompt Estruturado"
      ],
      "metadata": {
        "id": "QtgOGHuxb8y8"
      },
      "id": "QtgOGHuxb8y8"
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(item):\n",
        "    return f\"\"\"\n",
        "### InstruÃ§Ã£o:\n",
        "{item['instruction']}\n",
        "\n",
        "### Resposta:\n",
        "{item['response']}\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "t7MUM3wNb9og"
      },
      "id": "t7MUM3wNb9og",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TokenizaÃ§Ã£o Simples (char-level)"
      ],
      "metadata": {
        "id": "kX6sdpJHcA2_"
      },
      "id": "kX6sdpJHcA2_"
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = \"\".join([format_prompt(d) for d in dataset])\n",
        "\n",
        "chars = sorted(set(all_text))\n",
        "stoi = {c:i for i,c in enumerate(chars)}\n",
        "itos = {i:c for c,i in stoi.items()}\n",
        "\n",
        "vocab_size = len(chars)\n"
      ],
      "metadata": {
        "id": "aHsJ_UxvcBmN"
      },
      "id": "aHsJ_UxvcBmN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder"
      ],
      "metadata": {
        "id": "yXCn3ufrcEGT"
      },
      "id": "yXCn3ufrcEGT"
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text):\n",
        "    return [stoi[c] for c in text if c in stoi]\n",
        "\n",
        "def decode(tokens):\n",
        "    return \"\".join([itos[t] for t in tokens])\n"
      ],
      "metadata": {
        "id": "7NnnmUh8cGvE"
      },
      "id": "7NnnmUh8cGvE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar Dataset com Masking"
      ],
      "metadata": {
        "id": "Tb72JOWccIuc"
      },
      "id": "Tb72JOWccIuc"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_instruction_dataset(data, context_size=64):\n",
        "\n",
        "    X, Y, mask = [], [], []\n",
        "\n",
        "    for item in data:\n",
        "        prompt = f\"### InstruÃ§Ã£o:\\n{item['instruction']}\\n\\n### Resposta:\\n\"\n",
        "        full = prompt + item[\"response\"]\n",
        "\n",
        "        prompt_tokens = encode(prompt)\n",
        "        full_tokens = encode(full)\n",
        "\n",
        "        for i in range(len(full_tokens)-context_size):\n",
        "\n",
        "            x = full_tokens[i:i+context_size]\n",
        "            y = full_tokens[i+1:i+context_size+1]\n",
        "\n",
        "            m = [0]*len(prompt_tokens)\n",
        "            m = m + [1]*(context_size-len(prompt_tokens))\n",
        "\n",
        "            X.append(x)\n",
        "            Y.append(y)\n",
        "            mask.append(m[:context_size])\n",
        "\n",
        "    return (\n",
        "        torch.tensor(X),\n",
        "        torch.tensor(Y),\n",
        "        torch.tensor(mask)\n",
        "    )\n",
        "\n",
        "context_size = 64\n",
        "X, Y, MASK = build_instruction_dataset(dataset, context_size)\n",
        "\n",
        "X, Y, MASK = X.to(device), Y.to(device), MASK.to(device)\n"
      ],
      "metadata": {
        "id": "lDoU1pMNcK61"
      },
      "id": "lDoU1pMNcK61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“Š Parte B â€” Modelo Instruction Tuned\n",
        "Wrapper com Loss Mascarada"
      ],
      "metadata": {
        "id": "j6JQWaxbcNRb"
      },
      "id": "j6JQWaxbcNRb"
    },
    {
      "cell_type": "code",
      "source": [
        "class InstructionGPT(nn.Module):\n",
        "\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "\n",
        "        logits, _ = self.backbone(x)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if y is not None:\n",
        "\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                y.view(-1),\n",
        "                reduction=\"none\"\n",
        "            )\n",
        "\n",
        "            loss = loss * mask.view(-1)\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "model = InstructionGPT(backbone).to(device)\n"
      ],
      "metadata": {
        "id": "wBRPXMpXcT15"
      },
      "id": "wBRPXMpXcT15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“Š Parte C â€” Treinamento SFT\n",
        "Otimizador"
      ],
      "metadata": {
        "id": "UKJhslsicZyw"
      },
      "id": "UKJhslsicZyw"
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n"
      ],
      "metadata": {
        "id": "g7tkKnB3ce8b"
      },
      "id": "g7tkKnB3ce8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loop de Treino"
      ],
      "metadata": {
        "id": "w_MsuhqEciRP"
      },
      "id": "w_MsuhqEciRP"
    },
    {
      "cell_type": "code",
      "source": [
        "loss_history = []\n",
        "\n",
        "for step in range(300):\n",
        "\n",
        "    idx = torch.randint(0, X.size(0), (8,))\n",
        "    xb, yb, mb = X[idx], Y[idx], MASK[idx]\n",
        "\n",
        "    logits, loss = model(xb, yb, mb)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        print(step, loss.item())\n"
      ],
      "metadata": {
        "id": "U1eiYXkhci9L"
      },
      "id": "U1eiYXkhci9L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizar Loss"
      ],
      "metadata": {
        "id": "cNOkvgioclLW"
      },
      "id": "cNOkvgioclLW"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.title(\"Loss Instruction Tuning\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TBz9cXLicoAE"
      },
      "id": "TBz9cXLicoAE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“Š Parte D â€” ComparaÃ§Ã£o Base vs Instruction Tuned\n",
        "FunÃ§Ã£o de GeraÃ§Ã£o"
      ],
      "metadata": {
        "id": "Q1UCRxoEcpn3"
      },
      "id": "Q1UCRxoEcpn3"
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, start, max_tokens=80):\n",
        "\n",
        "    tokens = encode(start)\n",
        "    tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    for _ in range(max_tokens):\n",
        "        logits, _ = model(tokens)\n",
        "        next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
        "        tokens = torch.cat([tokens, next_token.unsqueeze(1)], dim=1)\n",
        "\n",
        "    return decode(tokens.squeeze().tolist())\n"
      ],
      "metadata": {
        "id": "PJ51BOYHcvMA"
      },
      "id": "PJ51BOYHcvMA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testando o modelo !"
      ],
      "metadata": {
        "id": "_teBygOZcw2M"
      },
      "id": "_teBygOZcw2M"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"### InstruÃ§Ã£o:\\nExplique o que Ã© Machine Learning\\n\\n### Resposta:\\n\"\n",
        "\n",
        "print(generate(model, prompt))\n"
      ],
      "metadata": {
        "id": "Dg5Sks5rc1zs"
      },
      "id": "Dg5Sks5rc1zs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“Š Parte E â€” Salvar Checkpoint Final\n"
      ],
      "metadata": {
        "id": "MMbzHP-3c52j"
      },
      "id": "MMbzHP-3c52j"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"07_instruction_gpt.pt\")\n"
      ],
      "metadata": {
        "id": "atEAnTBOc8t0"
      },
      "id": "atEAnTBOc8t0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encerramento da SÃ©rie\n",
        "\n",
        "Neste capÃ­tulo vocÃª ensinou um modelo a:\n",
        "\n",
        "â€¢ Interpretar instruÃ§Ãµes  \n",
        "â€¢ Produzir respostas orientadas  \n",
        "â€¢ Aprender comportamento conversacional  \n",
        "\n",
        "VocÃª percorreu toda a jornada:\n",
        "\n",
        "Texto â†’ Tokens â†’ AtenÃ§Ã£o â†’ GPT â†’ Treinamento â†’ Fine-Tuning â†’ Instruction Tuning\n",
        "\n",
        "Este Ã© o pipeline fundamental dos assistentes baseados em LLMs modernos.\n"
      ],
      "metadata": {
        "id": "cn8GHMsGc_3Z"
      },
      "id": "cn8GHMsGc_3Z"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}