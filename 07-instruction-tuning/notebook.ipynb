{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cap√≠tulo 07 ‚Äî Instruction Tuning: Criando um Assistente\n",
    "\n",
    "üéØ **Objetivos:** Transformar o modelo completador em um assistente √∫til usando **SFT (Supervised Fine-Tuning)**.\n",
    "\n",
    "![SFT](./infograficos/04-pipeline-sft.png)"
   ],
   "metadata": { "id": "header" }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Setup do reposit√≥rio no Colab\n",
    "# ============================================================\n",
    "import os, sys\n",
    "REPO_NAME = \"fazendo-um-llm-do-zero\"\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if not os.path.exists(REPO_NAME):\n",
    "        get_ipython().system(f\"git clone https://github.com/vongrossi/{REPO_NAME}.git\")\n",
    "    if os.path.exists(REPO_NAME) and os.getcwd().split('/')[-1] != REPO_NAME:\n",
    "        os.chdir(REPO_NAME)\n",
    "if os.getcwd() not in sys.path: sys.path.append(os.getcwd())\n",
    "print(\"üìÇ Diret√≥rio atual:\", os.getcwd())"
   ],
   "metadata": { "id": "setup-repo" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.gptmini import GPTConfig, GPTMini\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# üìÇ Carregamento do Checkpoint do Cap 05\n",
    "if not os.path.exists(\"gpt_checkpoint.pt\"):\n",
    "    from google.colab import files\n",
    "    print(\"üì§ Por favor, suba o 'gpt_checkpoint.pt' gerado no Cap√≠tulo 05:\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "ckpt = torch.load(\"gpt_checkpoint.pt\", map_location=device, weights_only=False)\n",
    "stoi, itos = ckpt['stoi'], ckpt['itos']\n",
    "config = ckpt['config']\n",
    "context_size = config.context_size # Importante: manter o mesmo do pre-treino\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(f\"üß† Modelo pr√©-treinado carregado!\")\n",
    "print(f\"üìè Janela de contexto: {context_size} | Vocabul√°rio: {len(stoi)}\")"
   ],
   "metadata": { "id": "setup" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Dataset de Instru√ß√µes\n",
    "\n",
    "Ensinamos o modelo que a estrutura `### Comando:` pede uma resposta em `### Resposta:`.\n",
    "**Nota:** Agora usamos o `context_size` sincronizado com o modelo ({context_size})."
   ],
   "metadata": { "id": "data-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "instructions = [\n",
    "    {\"q\": \"o que o gato fez?\", \"a\": \"o gato subiu no telhado e pulou o muro.\"},\n",
    "    {\"q\": \"onde o cachorro dormiu?\", \"a\": \"o cachorro dormiu no tapete do sofa.\"},\n",
    "    {\"q\": \"defina inteligencia artificial\", \"a\": \"artificial inteligencia e o estudo de algoritmos.\"},\n",
    "    {\"q\": \"o que e machine learning?\", \"a\": \"machine learning permite que sistemas aprendam padroes.\"}\n",
    "]\n",
    "\n",
    "def build_sft_dataset(data, context_size):\n",
    "    X, Y, masks = [], [], []\n",
    "    for item in data:\n",
    "        cmd = f\"### Comando:\\n{item['q']}\\n\\n### Resposta:\\n\"\n",
    "        full = cmd + item['a']\n",
    "        \n",
    "        ids = encode(full)\n",
    "        cmd_tokens = encode(cmd)\n",
    "        cmd_len = len(cmd_tokens)\n",
    "        \n",
    "        for i in range(len(ids) - context_size):\n",
    "            x_batch = ids[i : i+context_size]\n",
    "            y_batch = ids[i+1 : i+context_size+1]\n",
    "            \n",
    "            # L√≥gica da M√°scara: 0 no que for comando, 1 no que for resposta\n",
    "            # Precisamos calcular onde o comando termina dentro dessa janela x_batch\n",
    "            m = []\n",
    "            for j in range(i, i + context_size):\n",
    "                if j < cmd_len: m.append(0)\n",
    "                else: m.append(1)\n",
    "            \n",
    "            X.append(x_batch)\n",
    "            Y.append(y_batch)\n",
    "            masks.append(m)\n",
    "            \n",
    "    return torch.tensor(X).to(device), torch.tensor(Y).to(device), torch.tensor(masks).to(device)\n",
    "\n",
    "X, Y, M = build_sft_dataset(instructions, context_size)\n",
    "print(f\"üì¶ Amostras de Alinhamento: {len(X)}\")"
   ],
   "metadata": { "id": "data-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Treinamento com M√°scara de Loss\n",
    "\n",
    "Otimizamos apenas a gera√ß√£o da resposta, ignorando o comando na hora de calcular o erro.\n",
    "\n",
    "![Masking](./infograficos/03-mascaramento-loss-resposta.png)"
   ],
   "metadata": { "id": "train-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "model = GPTMini(config).to(device)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "loss_history = []\n",
    "model.train()\n",
    "print(\"üî® Alinhando o assistente...\")\n",
    "for step in range(601):\n",
    "    idx = torch.randint(len(X), (8,))\n",
    "    xb, yb, mb = X[idx], Y[idx], M[idx]\n",
    "    \n",
    "    logits, _ = model(xb)\n",
    "    \n",
    "    # C√°lculo da Loss Mascarada\n",
    "    B, T, V = logits.shape\n",
    "    loss = F.cross_entropy(logits.view(-1, V), yb.view(-1), reduction='none')\n",
    "    loss = (loss * mb.view(-1)).mean()\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    if step % 200 == 0: print(f\"Step {step:03d} | Loss {loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(loss_history, color='#34A853')\n",
    "plt.title(\"Curva de Alinhamento (SFT)\")\n",
    "plt.show()"
   ],
   "metadata": { "id": "train-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Teste do Assistente Alinhado\n",
    "\n",
    "O modelo agora deve responder seguindo o protocolo de comando."
   ],
   "metadata": { "id": "test-header" }
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def ask(model, question):\n",
    "    model.eval()\n",
    "    prompt = f\"### Comando:\\n{question.lower()}\\n\\n### Resposta:\\n\"\n",
    "    idx = torch.tensor(encode(prompt)).unsqueeze(0).to(device)\n",
    "    \n",
    "    for _ in range(60):\n",
    "        # GARANTIA: Nunca ultrapassar o contexto original do modelo\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        \n",
    "        # Escolha determin√≠stica (Greedy)\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "        \n",
    "        if itos[next_id.item()] == \".\": break\n",
    "        \n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "print(\"ü§ñ TESTE DE INTERA√á√ÉO:\")\n",
    "print(\"-\" * 30)\n",
    "print(ask(model, \"o que o gato fez?\"))\n",
    "print(ask(model, \"o que e machine learning?\"))"
   ],
   "metadata": { "id": "test-code" },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üèÅ Conclus√£o da Jornada\n",
    "\n",
    "Voc√™ completou a s√©rie! \n",
    "\n",
    "Transformou um modelo estat√≠stico em um assistente capaz de seguir inten√ß√µes humanas. Este √© o fundamento do alinhamento de IA.\n",
    "\n",
    "![Avalia√ß√£o](./infograficos/05-avaliacao-respostas.png)"
   ],
   "metadata": { "id": "footer" }
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.12.3" }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}