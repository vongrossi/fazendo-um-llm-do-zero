{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cap√≠tulo 07 ‚Äî Instruction Tuning: Criando um Assistente\n",
        "\n",
        "üéØ **Objetivos:** Transformar o modelo completador em um assistente √∫til usando **SFT (Supervised Fine-Tuning)**.\n",
        "\n",
        "![SFT](./infograficos/04-pipeline-sft.png)"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Setup do reposit√≥rio no Colab\n",
        "# ============================================================\n",
        "import os, sys\n",
        "REPO_NAME = \"fazendo-um-llm-do-zero\"\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    if not os.path.exists(REPO_NAME):\n",
        "        get_ipython().system(f\"git clone https://github.com/vongrossi/{REPO_NAME}.git\")\n",
        "    if os.path.exists(REPO_NAME) and os.getcwd().split('/')[-1] != REPO_NAME:\n",
        "        os.chdir(REPO_NAME)\n",
        "if os.getcwd() not in sys.path: sys.path.append(os.getcwd())\n",
        "print(\"üìÇ Diret√≥rio atual:\", os.getcwd())"
      ],
      "metadata": {
        "id": "setup-repo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from lib.gptmini import GPTConfig, GPTMini\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# üìÇ Carregamento do Checkpoint do Cap 05\n",
        "if not os.path.exists(\"gpt_checkpoint.pt\"):\n",
        "    from google.colab import files\n",
        "    print(\"üì§ Por favor, suba o 'gpt_checkpoint.pt' gerado no Cap√≠tulo 05:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "ckpt = torch.load(\"gpt_checkpoint.pt\", map_location=device, weights_only=False)\n",
        "stoi, itos = ckpt['stoi'], ckpt['itos']\n",
        "config = ckpt['config']\n",
        "context_size = config.context_size\n",
        "\n",
        "# Encoder que ignora caracteres desconhecidos com aviso\n",
        "def encode(s):\n",
        "    res = []\n",
        "    for c in s.lower():\n",
        "        if c in stoi: res.append(stoi[c])\n",
        "    return res\n",
        "\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(f\"üß† Modelo pr√©-treinado carregado!\")\n",
        "print(f\"üìè Contexto: {context_size} | Vocabul√°rio: {len(stoi)} caracteres\")\n",
        "if '#' not in stoi: print(\"‚ö†Ô∏è AVISO: Seu checkpoint n√£o possui o caractere '#'. Re-execute o Cap√≠tulo 05 com o novo dataset.\")"
      ],
      "metadata": {
        "id": "setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset de Instru√ß√µes\n",
        "\n",
        "Criamos pares de Pergunta e Resposta para o alinhamento."
      ],
      "metadata": {
        "id": "data-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = [\n",
        "    {\"q\": \"o que o gato fez?\", \"a\": \"o gato subiu no telhado e pulou o muro.\"},\n",
        "    {\"q\": \"onde o cachorro dormiu?\", \"a\": \"o cachorro dormiu no sofa e no tapete.\"},\n",
        "    {\"q\": \"defina inteligencia artificial\", \"a\": \"inteligencia artificial e o estudo de algoritmos.\"},\n",
        "    {\"q\": \"o que e machine learning?\", \"a\": \"machine learning permite que sistemas aprendam padroes.\"}\n",
        "]\n",
        "\n",
        "def build_sft_dataset(data, context_size):\n",
        "    X, Y, masks = [], [], []\n",
        "    for item in data:\n",
        "        cmd = f\"### comando:\\n{item['q']}\\n### resposta:\\n\"\n",
        "        full = cmd + item['a']\n",
        "        ids = encode(full)\n",
        "        cmd_len = len(encode(cmd))\n",
        "        for i in range(len(ids) - context_size):\n",
        "            X.append(ids[i : i+context_size])\n",
        "            Y.append(ids[i+1 : i+context_size+1])\n",
        "            # M√°scara: 0 no comando, 1 na resposta\n",
        "            m = []\n",
        "            for j in range(i, i + context_size):\n",
        "                if j < cmd_len: m.append(0)\n",
        "                else: m.append(1)\n",
        "            masks.append(m)\n",
        "    return torch.tensor(X).to(device), torch.tensor(Y).to(device), torch.tensor(masks).to(device)\n",
        "\n",
        "X, Y, M = build_sft_dataset(instructions, context_size)\n",
        "print(f\"üì¶ Amostras de Alinhamento: {len(X)}\")"
      ],
      "metadata": {
        "id": "data-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Treinamento com M√°scara de Loss\n",
        "\n",
        "Otimizamos apenas a gera√ß√£o da resposta.\n",
        "\n",
        "![Masking](./infograficos/03-mascaramento-loss-resposta.png)"
      ],
      "metadata": {
        "id": "train-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTMini(config).to(device)\n",
        "model.load_state_dict(ckpt['state_dict'])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "\n",
        "loss_history = []\n",
        "model.train()\n",
        "print(\"üî® Alinhando o assistente...\")\n",
        "for step in range(801):\n",
        "    idx = torch.randint(len(X), (8,))\n",
        "    logits, _ = model(X[idx])\n",
        "    B, T, V = logits.shape\n",
        "    loss = F.cross_entropy(logits.view(-1, V), Y[idx].view(-1), reduction='none')\n",
        "    loss = (loss * M[idx].view(-1)).mean()\n",
        "    optimizer.zero_grad(set_to_none=True); loss.backward(); optimizer.step()\n",
        "    loss_history.append(loss.item())\n",
        "    if step % 200 == 0: print(f\"Step {step} | Loss {loss.item():.4f}\")\n",
        "\n",
        "plt.plot(loss_history, color='#34A853')\n",
        "plt.title(\"Curva de Alinhamento (SFT)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "train-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Teste do Assistente Alinhado\n",
        "\n",
        "O modelo agora responde apenas o que foi solicitado."
      ],
      "metadata": {
        "id": "test-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def ask(model, question):\n",
        "    model.eval()\n",
        "    prompt_text = f\"### comando:\\n{question.lower()}\\n### resposta:\\n\"\n",
        "    \n",
        "    # Debug: mostrar o que o modelo est√° realmente lendo\n",
        "    unknowns = [c for c in prompt_text if c not in stoi]\n",
        "    if unknowns: print(f\"‚ö†Ô∏è Ignorando: {set(unknowns)}\")\n",
        "    \n",
        "    tokens = encode(prompt_text)\n",
        "    idx = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "    prompt_len = len(tokens)\n",
        "    \n",
        "    for _ in range(60):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        logits, _ = model(idx_cond)\n",
        "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "        if itos[next_id.item()] == \".\": break\n",
        "        \n",
        "    # Retornamos apenas a parte gerada (Resposta)\n",
        "    return decode(idx[0][prompt_len:].tolist())\n",
        "\n",
        "print(\"ü§ñ TESTE DE INTERA√á√ÉO:\")\n",
        "print(\"-\" * 30)\n",
        "q1 = \"o que o gato fez?\"\n",
        "print(f\"Pergunta: {q1}\\nResposta: {ask(model, q1)}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 30)\n",
        "q2 = \"o que e machine learning?\"\n",
        "print(f\"Pergunta: {q2}\\nResposta: {ask(model, q2)}\")"
      ],
      "metadata": {
        "id": "test-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèÅ Conclus√£o da Jornada\n",
        "\n",
        "Voc√™ completou a s√©rie! \n",
        "\n",
        "Transformou um modelo estat√≠stico em um assistente capaz de seguir inten√ß√µes humanas. Este √© o fundamento do alinhamento de IA.\n",
        "\n",
        "![Avalia√ß√£o](./infograficos/05-avaliacao-respostas.png)"
      ],
      "metadata": {
        "id": "footer"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
