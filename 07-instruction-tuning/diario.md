# Cap√≠tulo 07 ‚Äî Instruction Tuning

Nos cap√≠tulos anteriores, constru√≠mos progressivamente um modelo de linguagem completo.

N√≥s ensinamos o modelo a:

- transformar texto em n√∫meros  
- construir representa√ß√µes sem√¢nticas  
- aprender rela√ß√µes contextuais com aten√ß√£o  
- gerar linguagem coerente  
- adaptar comportamento para tarefas supervisionadas  

Agora surge a pergunta final desta jornada:

> Como ensinar o modelo a responder instru√ß√µes humanas?

Este cap√≠tulo responde essa pergunta.

---

## Do Modelo Base ao Assistente Conversacional

Durante o pr√©-treinamento, um modelo aprende padr√µes da linguagem observando grandes volumes de texto.

Ele aprende:

- como frases s√£o constru√≠das  
- como ideias s√£o conectadas  
- como palavras costumam aparecer juntas  

Mas esse aprendizado √© passivo.

O modelo aprende **como a linguagem continua**, n√£o **como responder perguntas ou executar tarefas**.

---

## O Surgimento do Instruction Tuning

Instruction tuning √© o processo de ensinar o modelo a responder instru√ß√µes humanas.

Ele transforma o modelo de:

üëâ um gerador de texto  
üëâ em um assistente capaz de interagir com pessoas  

---

## Uma Analogia Humana

Podemos comparar o treinamento de LLMs com a forma√ß√£o de um profissional.

Pr√©-treinamento √© equivalente √† educa√ß√£o geral.

Fine-tuning supervisionado √© equivalente a uma especializa√ß√£o t√©cnica.

Instruction tuning √© equivalente a aprender a trabalhar com clientes e interpretar pedidos humanos.

![Modelo Base vs Modelo Alinhado](./infograficos/01-instruction-tuning-visao-geral.png)

---

## Como Modelos Aprendem a Seguir Instru√ß√µes

Modelos n√£o entendem linguagem da forma como humanos entendem.

Eles aprendem observando exemplos.

Se queremos que o modelo responda instru√ß√µes, precisamos fornecer exemplos de:

- instru√ß√µes humanas  
- contexto adicional (quando necess√°rio)  
- respostas desejadas  

---

## Estrutura de Dados para Instruction Tuning

Datasets de instruction tuning geralmente seguem um formato estruturado.

Eles podem conter tr√™s componentes:

1. Instruction  
2. Input (opcional)  
3. Response  

![Formato de Dados para Instru√ß√µes](./infograficos/02-formato-dados-instrucao.png)

Esse formato permite que o modelo aprenda padr√µes de intera√ß√£o.

---

## A Import√¢ncia do Prompt Estruturado

Durante o treinamento, o modelo n√£o recebe apenas perguntas.

Ele recebe prompts estruturados que indicam claramente:

- qual √© a tarefa  
- qual √© o contexto  
- qual resposta √© esperada  

Essa estrutura ensina o modelo a interpretar inten√ß√µes humanas.

---

## Mascaramento da Loss

Instruction tuning utiliza uma estrat√©gia fundamental:

O modelo aprende apenas com a resposta.

![Mascaramento da Loss](./infograficos/03-mascaramento-loss-resposta.png)

Durante o treinamento:

- tokens da instru√ß√£o s√£o usados como contexto  
- tokens da resposta s√£o usados para c√°lculo da loss  

Isso impede que o modelo aprenda a reproduzir o prompt e o incentiva a aprender a responder.

---

## Pipeline de Supervised Fine-Tuning (SFT)

Instruction tuning √© implementado como uma forma especializada de fine-tuning supervisionado.

![Pipeline SFT](./infograficos/04-pipeline-sft.png)

O pipeline envolve:

1. Dataset de instru√ß√µes  
2. Tokeniza√ß√£o estruturada  
3. Reutiliza√ß√£o de pesos pr√©-treinados  
4. Treinamento supervisionado  
5. Avalia√ß√£o das respostas  

---

## Avaliando Modelos Conversacionais

Modelos conversacionais s√£o dif√≠ceis de avaliar com m√©tricas tradicionais.

Accuracy e F1-score n√£o capturam:

- coer√™ncia sem√¢ntica  
- utilidade da resposta  
- naturalidade do texto  
- ader√™ncia √† instru√ß√£o  

![Avalia√ß√£o de Respostas](./infograficos/05-avaliacao-respostas.png)

Por isso, avalia√ß√£o qualitativa se torna essencial.

---

## O Papel da Qualidade do Dataset

Instruction tuning depende fortemente da qualidade dos exemplos fornecidos.

Datasets com respostas claras e bem estruturadas produzem modelos mais confi√°veis.

Datasets inconsistentes podem gerar comportamento imprevis√≠vel.

---

## Limita√ß√µes do Instruction Tuning Did√°tico

Neste projeto utilizamos:

- datasets pequenos  
- modelos compactos  
- treinamento simplificado  

Essas simplifica√ß√µes permitem execu√ß√£o no Colab e facilitam aprendizado conceitual.

Sistemas reais utilizam:

- milh√µes de exemplos  
- m√∫ltiplas etapas de alinhamento  
- avalia√ß√£o humana cont√≠nua  

---

## O Que Construiremos no Notebook

Neste cap√≠tulo vamos implementar:

- dataset educacional de instru√ß√µes  
- formata√ß√£o de prompts estruturados  
- mascaramento da loss  
- treinamento supervisionado orientado a respostas  
- compara√ß√£o entre modelo base e modelo instruction-tuned  
- avalia√ß√£o qualitativa de respostas  

---

## Conectando Toda a Jornada da S√©rie

Ao longo desta s√©rie voc√™ aprendeu como:

- linguagem √© representada numericamente  
- aten√ß√£o permite compreens√£o contextual  
- modelos GPT s√£o constru√≠dos  
- modelos aprendem linguagem  
- modelos s√£o adaptados para tarefas espec√≠ficas  
- modelos aprendem a seguir instru√ß√µes humanas  

Este √© o fluxo fundamental que sustenta os assistentes baseados em LLMs modernos.

---

## Para Onde a Jornada Continua

Instruction tuning √© apenas uma etapa do alinhamento de modelos.

Sistemas modernos tamb√©m utilizam:

- reinforcement learning com feedback humano (RLHF)  
- avalia√ß√£o automatizada de qualidade  
- ajuste cont√≠nuo com dados reais  

Esses temas representam a fronteira atual da pesquisa em LLMs.

---

## üßæ Gloss√°rio R√°pido ‚Äî Cap√≠tulo 07

**Instruction Tuning**  
Processo de treinar modelos para seguir instru√ß√µes humanas.

**Supervised Fine-Tuning (SFT)**  
Treinamento supervisionado usando pares instru√ß√£o-resposta.

**Prompt Estruturado**  
Formato padronizado usado para orientar o modelo.

**Masking da Loss**  
T√©cnica que calcula erro apenas sobre tokens da resposta.

**Alignment**  
Processo de adaptar comportamento do modelo para expectativas humanas.

---

> No in√≠cio da s√©rie, ensinamos o modelo a reconhecer padr√µes da linguagem.  
> No final da s√©rie, ensinamos o modelo a conversar.
