{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Setup do reposit√≥rio\n",
        "# ============================================================\n",
        "# Quando abrimos um notebook pelo GitHub no Google Colab,\n",
        "# apenas o arquivo .ipynb √© carregado.\n",
        "# Esta c√©lula garante que TODO o reposit√≥rio esteja dispon√≠vel.\n",
        "\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/vongrossi/fazendo-um-llm-do-zero.git\"\n",
        "REPO_DIR = \"fazendo-um-llm-do-zero\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone {REPO_URL}\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "print(\"Diret√≥rio atual:\", os.getcwd())\n",
        "print(\"Conte√∫do da raiz:\", os.listdir(\".\"))\n"
      ],
      "metadata": {
        "id": "DTLKdfkztk8m"
      },
      "id": "DTLKdfkztk8m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cap√≠tulo 01 ‚Äî O que √© um LLM (de verdade)\n",
        "\n",
        "Este notebook acompanha o Cap√≠tulo 01 da s√©rie **Fazendo um LLM do Zero**.\n",
        "\n",
        "üéØ Objetivos deste notebook:\n",
        "- Entender LLMs como modelos de previs√£o do pr√≥ximo token\n",
        "- Ver como comportamento emerge de regras simples\n",
        "- Conectar o conceito te√≥rico com um exemplo pr√°tico\n",
        "- Preparar o terreno para tokeniza√ß√£o e Transformers\n"
      ],
      "metadata": {
        "id": "PaN7xPRjtp2h"
      },
      "id": "PaN7xPRjtp2h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instala√ß√£o de depend√™ncias"
      ],
      "metadata": {
        "id": "XTeVagaNtuZn"
      },
      "id": "XTeVagaNtuZn"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Instala√ß√£o de depend√™ncias do cap√≠tulo\n",
        "# ============================================================\n",
        "# No Colab, muitas bibliotecas j√° v√™m instaladas,\n",
        "# mas usamos este passo para garantir consist√™ncia.\n",
        "\n",
        "!pip -q install -r 01-o-que-e-um-llm/requirements.txt"
      ],
      "metadata": {
        "id": "jqY0etqct2Hk"
      },
      "id": "jqY0etqct2Hk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports e ambiente"
      ],
      "metadata": {
        "id": "7VTf6Crit7jJ"
      },
      "id": "7VTf6Crit7jJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Imports b√°sicos\n",
        "# ============================================================\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Permite importar o colab_setup.py do cap√≠tulo\n",
        "sys.path.append(\"01-o-que-e-um-llm\")\n",
        "\n",
        "from colab_setup import seed_everything\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "print(\"Ambiente configurado com seed fixa.\")\n"
      ],
      "metadata": {
        "id": "dMbW9C2Jt-Ch"
      },
      "id": "dMbW9C2Jt-Ch",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entendendo o Conceito\n",
        "#### LLMs come√ßam com algo simples\n",
        "\n",
        "Antes de Transformers, aten√ß√£o ou bilh√µes de par√¢metros,\n",
        "um LLM come√ßa resolvendo uma tarefa b√°sica:\n",
        "\n",
        "> **Dado um contexto, prever o pr√≥ximo token mais prov√°vel.**\n",
        "\n",
        "Neste notebook, vamos construir um modelo extremamente simples\n",
        "que faz exatamente isso ‚Äî sem deep learning ainda.\n",
        "\n",
        "A ideia √© mostrar que **comportamento interessante pode emergir**\n",
        "mesmo de regras estat√≠sticas simples.\n"
      ],
      "metadata": {
        "id": "qMpSyAYAuH9r"
      },
      "id": "qMpSyAYAuH9r"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Dataset simples de frases\n",
        "# ============================================================\n",
        "# Em LLMs reais, usamos bilh√µes de tokens.\n",
        "# Aqui usamos um conjunto pequeno apenas para entendimento.\n",
        "\n",
        "sentences = [\n",
        "    \"o gato subiu no telhado\",\n",
        "    \"o cachorro subiu no sof√°\",\n",
        "    \"o gato dormiu no sof√°\",\n",
        "    \"o cachorro dormiu no tapete\",\n",
        "    \"o gato pulou no muro\",\n",
        "]\n",
        "\n",
        "print(\"Dataset de exemplo:\")\n",
        "for s in sentences:\n",
        "    print(\"-\", s)\n"
      ],
      "metadata": {
        "id": "7uLBnSn4uLe0"
      },
      "id": "7uLBnSn4uLe0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokeniza√ß√£o simples (por palavra)"
      ],
      "metadata": {
        "id": "OlGjCgmiuZfa"
      },
      "id": "OlGjCgmiuZfa"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Tokeniza√ß√£o simples\n",
        "# ============================================================\n",
        "# Aqui, cada palavra ser√° tratada como um token.\n",
        "# Isso N√ÉO √© como LLMs reais fazem, mas ajuda a entender a ideia.\n",
        "\n",
        "tokenized_sentences = [s.split() for s in sentences]\n",
        "\n",
        "print(\"Frases tokenizadas:\")\n",
        "for ts in tokenized_sentences:\n",
        "    print(ts)\n"
      ],
      "metadata": {
        "id": "iAfHjnJSub9Y"
      },
      "id": "iAfHjnJSub9Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construindo estat√≠sticas de pr√≥ximo token"
      ],
      "metadata": {
        "id": "aHaMn6rAuigP"
      },
      "id": "aHaMn6rAuigP"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Contando transi√ß√µes de tokens\n",
        "# ============================================================\n",
        "# Vamos contar:\n",
        "# Dado um token atual, quais tokens costumam vir depois?\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "next_token_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for sentence in tokenized_sentences:\n",
        "    for i in range(len(sentence) - 1):\n",
        "        current_token = sentence[i]\n",
        "        next_token = sentence[i + 1]\n",
        "        next_token_counts[current_token][next_token] += 1\n",
        "\n",
        "print(\"Exemplo de transi√ß√µes aprendidas:\")\n",
        "for token, transitions in next_token_counts.items():\n",
        "    print(f\"\\n'{token}' ‚Üí {dict(transitions)}\")\n"
      ],
      "metadata": {
        "id": "sOKKGL9luk6e"
      },
      "id": "sOKKGL9luk6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convertendo contagens em probabilidades"
      ],
      "metadata": {
        "id": "jXh9UE6yup-e"
      },
      "id": "jXh9UE6yup-e"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Convertendo contagens em probabilidades\n",
        "# ============================================================\n",
        "\n",
        "next_token_probs = {}\n",
        "\n",
        "for token, transitions in next_token_counts.items():\n",
        "    total = sum(transitions.values())\n",
        "    probs = {t: count / total for t, count in transitions.items()}\n",
        "    next_token_probs[token] = probs\n",
        "\n",
        "print(\"Probabilidades aprendidas:\")\n",
        "for token, probs in next_token_probs.items():\n",
        "    print(f\"\\nAp√≥s '{token}':\")\n",
        "    for t, p in probs.items():\n",
        "        print(f\"  {t}: {p:.2f}\")\n"
      ],
      "metadata": {
        "id": "ECGP2dbjusRM"
      },
      "id": "ECGP2dbjusRM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gerando texto (previs√£o do pr√≥ximo token)"
      ],
      "metadata": {
        "id": "gboaVWAfuxjT"
      },
      "id": "gboaVWAfuxjT"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Fun√ß√£o simples de gera√ß√£o de texto\n",
        "# ============================================================\n",
        "\n",
        "def generate_text(start_token, max_steps=10):\n",
        "    current_token = start_token\n",
        "    generated = [current_token]\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        if current_token not in next_token_probs:\n",
        "            break\n",
        "\n",
        "        tokens = list(next_token_probs[current_token].keys())\n",
        "        probabilities = list(next_token_probs[current_token].values())\n",
        "\n",
        "        # Escolhe o pr√≥ximo token com base na distribui√ß√£o\n",
        "        current_token = random.choices(tokens, probabilities)[0]\n",
        "        generated.append(current_token)\n",
        "\n",
        "    return \" \".join(generated)\n"
      ],
      "metadata": {
        "id": "nktsM2Pmuz4I"
      },
      "id": "nktsM2Pmuz4I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testando o ‚Äúmodelo‚Äù"
      ],
      "metadata": {
        "id": "k98LMHceu5in"
      },
      "id": "k98LMHceu5in"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Testes de gera√ß√£o\n",
        "# ============================================================\n",
        "\n",
        "for start in [\"o\", \"gato\", \"cachorro\"]:\n",
        "    print(f\"\\nCome√ßando com '{start}':\")\n",
        "    print(generate_text(start))\n"
      ],
      "metadata": {
        "id": "kTX4yPSAu6rR"
      },
      "id": "kTX4yPSAu6rR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### O que acabamos de construir?\n",
        "\n",
        "Este modelo extremamente simples:\n",
        "\n",
        "- n√£o entende linguagem\n",
        "- n√£o tem consci√™ncia\n",
        "- n√£o sabe o significado das palavras\n",
        "\n",
        "E ainda assim:\n",
        "- aprende padr√µes\n",
        "- respeita contexto local\n",
        "- gera texto plaus√≠vel\n",
        "\n",
        "Isso acontece porque ele aprendeu **probabilidades condicionais**.\n",
        "\n",
        "LLMs modernos fazem exatamente isso,\n",
        "mas com:\n",
        "- tokeniza√ß√£o muito mais sofisticada\n",
        "- contextos muito maiores\n",
        "- arquiteturas como Transformers\n",
        "- bilh√µes de par√¢metros\n",
        "\n",
        "O princ√≠pio fundamental, por√©m, √© o mesmo.\n"
      ],
      "metadata": {
        "id": "HWeQVHBhu8WR"
      },
      "id": "HWeQVHBhu8WR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conex√£o com LLMs reais\n",
        "\n",
        "O que voc√™ viu aqui √© uma vers√£o microsc√≥pica da ideia central dos LLMs.\n",
        "\n",
        "Nos pr√≥ximos cap√≠tulos, vamos:\n",
        "- melhorar a tokeniza√ß√£o\n",
        "- aumentar o contexto\n",
        "- substituir estat√≠stica simples por redes neurais\n",
        "- introduzir aten√ß√£o e Transformers\n",
        "\n",
        "Mas sempre mantendo a mesma pergunta no centro:\n",
        "\n",
        "> ‚ÄúDado tudo que veio antes, qual √© o pr√≥ximo token mais prov√°vel?‚Äù\n"
      ],
      "metadata": {
        "id": "bi0fQ3YOvFtV"
      },
      "id": "bi0fQ3YOvFtV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üßæ Gloss√°rio R√°pido\n",
        "\n",
        "**Token**  \n",
        "Unidade b√°sica de texto processada pelo modelo.\n",
        "\n",
        "**Previs√£o do pr√≥ximo token**  \n",
        "Tarefa central dos modelos de linguagem: estimar o token mais prov√°vel dado um contexto.\n",
        "\n",
        "**Probabilidade condicional**  \n",
        "Probabilidade de um evento acontecer dado que outro j√° ocorreu.\n",
        "\n",
        "**Modelo de linguagem**  \n",
        "Modelo que atribui probabilidades a sequ√™ncias de tokens.\n"
      ],
      "metadata": {
        "id": "Swsk96l_vK4S"
      },
      "id": "Swsk96l_vK4S"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}