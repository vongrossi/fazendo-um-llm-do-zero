# Cap√≠tulo 01 ‚Äî O que √© um LLM (de verdade)

Este cap√≠tulo estabelece o **modelo mental fundamental** para todo o restante da s√©rie.

Antes de falar sobre tokeniza√ß√£o, aten√ß√£o ou treinamento, precisamos responder com clareza:
**o que √© ‚Äî e o que n√£o √© ‚Äî um Large Language Model (LLM).**

Aqui, o foco n√£o √© hype nem atalhos.
√â entendimento conceitual s√≥lido.

---

## üéØ Objetivo do Cap√≠tulo

Ao final deste cap√≠tulo, voc√™ deve ser capaz de:

- explicar o que √© um LLM sem recorrer a frases vagas ou m√°gicas
- entender por que ‚Äúprever o pr√≥ximo token‚Äù √© o cora√ß√£o do modelo
- reconhecer por que LLMs geram tantos tipos diferentes de aplica√ß√µes
- compreender os est√°gios de constru√ß√£o e uso de um LLM
- ter uma vis√£o de alto n√≠vel da arquitetura Transformer
- entender o que diferencia um modelo GPT de outras arquiteturas
- saber o que s√£o tokens, modelos fundacionais, encoder e decoder

Este cap√≠tulo **n√£o implementa um LLM completo**.
Ele constr√≥i o **mapa conceitual** que torna os pr√≥ximos cap√≠tulos compreens√≠veis.

---

## üß≠ Como este cap√≠tulo est√° organizado

O conte√∫do √© apresentado em camadas, seguindo uma progress√£o l√≥gica:

1. **O que √© um LLM**
   - LLM como modelo probabil√≠stico
   - Previs√£o do pr√≥ximo token
   - Analogias do mundo real

2. **Aplica√ß√µes de LLMs**
   - Por que um √∫nico mecanismo gera tantos usos diferentes
   - Texto, c√≥digo, resumo, perguntas e respostas

3. **Est√°gios de constru√ß√£o e uso**
   - Dados
   - Pr√©-treinamento
   - Ajuste (fine-tuning)
   - Uso em aplica√ß√µes

4. **Introdu√ß√£o √† arquitetura Transformer**
   - Por que arquiteturas anteriores n√£o escalam bem
   - Aten√ß√£o como mecanismo central
   - Processamento em paralelo

5. **Um olhar mais pr√≥ximo do GPT**
   - GPT como Transformer do tipo *decoder-only*
   - Gera√ß√£o autoregressiva
   - Rela√ß√£o com modelos fundacionais

6. **Conceitos fundamentais**
   - Token
   - Modelo fundacional
   - Encoder
   - Decoder

Cada se√ß√£o √© acompanhada de **infogr√°ficos did√°ticos** e exemplos conceituais.

---

## üß™ Parte pr√°tica (notebook)

Este cap√≠tulo inclui um notebook execut√°vel no Google Colab com um exemplo pr√°tico que:

- demonstra a ideia de previs√£o do pr√≥ximo token
- mostra como comportamento ‚Äúinteligente‚Äù emerge de regras simples
- antecipa conceitos que ser√£o aprofundados nos pr√≥ximos cap√≠tulos

O objetivo do notebook **n√£o √© performance**, mas compreens√£o.

---

## üìä Infogr√°ficos

Este cap√≠tulo utiliza infogr√°ficos para refor√ßar visualmente conceitos como:

- LLM como preditor de tokens
- Ciclo de vida de um LLM
- Arquitetura Transformer em alto n√≠vel
- Diferen√ßa entre encoder e decoder
- Estrutura conceitual de um modelo GPT

As descri√ß√µes dos infogr√°ficos est√£o documentadas no arquivo:

- `infograficos/README.md`

---

## ‚ñ∂Ô∏è Como executar

1. Leia o di√°rio do cap√≠tulo:
   - `diario.md`

2. Execute o notebook:
   - `notebook.ipynb`

3. Ou abra diretamente no Google Colab:
   - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vongrossi/fazendo-um-llm-do-zero/blob/main/01-o-que-e-um-llm/notebook.ipynb)
   - ou veja mais detalhes em `links.md`

---

## üìò Refer√™ncias

Este cap√≠tulo √© baseado no Cap√≠tulo 1 do livro  
*Build a Large Language Model (From Scratch)*, de Sebastian Raschka,

e em materiais complementares do pr√≥prio autor, incluindo palestras introdut√≥rias sobre LLMs.

O conte√∫do foi adaptado para:
- uma abordagem did√°tica em portugu√™s
- execu√ß√£o pr√°tica no Google Colab
- foco em modelo mental, n√£o apenas implementa√ß√£o

---

## ‚ö†Ô∏è Observa√ß√£o importante

Se algum conceito parecer abstrato neste cap√≠tulo, isso √© esperado.

Os pr√≥ximos cap√≠tulos transformam cada ideia apresentada aqui em:
- c√≥digo
- experimentos
- visualiza√ß√µes
- implementa√ß√µes progressivas

Este cap√≠tulo √© o **mapa**.
Os pr√≥ximos s√£o o **territ√≥rio**.
